<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><docs>https://blogs.law.harvard.edu/tech/rss</docs><title>Blogs on Bryan Apperson</title><link>https://bryanapperson.com/blog/</link><description>Recent content in Blogs on Bryan Apperson</description><image><title>Blogs on Bryan Apperson</title><link>https://bryanapperson.com/blog/</link><url>https://source.unsplash.com/collection/983219/2000x1322</url></image><ttl>1440</ttl><generator>Hugo 0.81.0</generator><language>en-US</language><copyright>Copyright &amp;copy; Bryan Apperson. Licensed under CC-BY-ND-4.0. Hosted on Github.</copyright><lastBuildDate>Fri, 19 Mar 2021 02:46:39 UT</lastBuildDate><atom:link href="https://bryanapperson.com/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>A Gradual Journey to Typed Python</title><link>https://bryanapperson.com/blog/gradually-typed-python/</link><pubDate>Thu, 18 Mar 2021 22:14:00 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/gradually-typed-python/</guid><description>Python is much loved by developers for its ease of development, its low time to minimum viable product and, some might say, its dynamic typing. I&amp;rsquo;ve been using dynamically typed Python for the better part of a decade now, for those same reasons. Dynamically typed Python is rapid to develop in, features amorphous blobs of data and works great for writing code quickly.
Gradual typing and static analysis split the difference between the reduced velocity of a strongly typed language and the likewise reduced velocity of maintaining code where the input and output data types are unclear.</description><content:encoded>Python is much loved by developers for its ease of development, its low time to minimum viable product and, some might say, its dynamic typing. I&amp;amp;rsquo;ve been using dynamically typed Python for the better part of a decade now, for those same reasons. Dynamically typed Python is rapid to develop in, features amorphous blobs of data and works great for writing code quickly.
Gradual typing and static analysis split the difference between the reduced velocity of a strongly typed language and the likewise reduced velocity of maintaining code where the input and output data types are unclear. Gradually typed Python makes reasoning about larger, and older, codebases easier. When I started using types in Python, I was just sprinkling in type annotations here and there. The Python type hinting system can do much more.
Static Analysis of Gradually Typed Python Formatting tools like Black and isort, along with static analysis tools like Flake8 are great for keeping python code consistent in form and free of unused variables. But Python type hinting can do more. Mypy provides many of the compile time checks you&amp;amp;rsquo;d get in a strongly typed, compiled language like Rust, which are useful when developing and add no overhead at runtime.
Command Line Interfaces via Typed Python Click is one of my favorite command line interface libraries. Recently I stumbled upon Typer, Typer is an abstraction on top of Click which uses type annotations and doctrings to generate beautiful command line interfaces with much less boilerplate code. Using Typer, a command line interface is easy to build in just a few lines.
De/Serialization and Validation with Types Pydantic, especially when combined with mypy is a sure fire way to get lightning fast data models with low cost of development, and lower probability of bugs for serialization, deserizaliztion, and by nature, data validation.
Conclusion Using the type hinting system has made my code better, made me a better developer, and allowed the sort of semantics used in languages like Rust in Python. I think it can do the same for the reader, and is definitely worth using. I hope to put out some more blogs in the near future on my Python toolbox and maybe even a sample app putting it all together.</content:encoded></item><item><title>A Socratic Study of Euclid's Elements</title><link>https://bryanapperson.com/blog/socratic-study-of-euclid/</link><pubDate>Sun, 26 Apr 2020 20:58:00 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/socratic-study-of-euclid/</guid><description>Studying The Elements of Euclid reveals the foundations of mathematical rigour and the systems through the lens of which we see the world today. Though the end result of the concepts covered in many of the books of the Elements will be familiar to those who have completed high school maths; seeing Euclid&amp;rsquo;s system come together to weave the fabric of maths and understanding the proofs from first principles is another matter entirely.</description><content:encoded><![CDATA[Studying The Elements of Euclid reveals the foundations of mathematical rigour and the systems through the lens of which we see the world today. Though the end result of the concepts covered in many of the books of the Elements will be familiar to those who have completed high school maths; seeing Euclid&amp;rsquo;s system come together to weave the fabric of maths and understanding the proofs from first principles is another matter entirely. When I started my, what will likely be decades long, study of the great books a few months ago, Euclid was on the list. I started study of Euclid by myself, but was fortunate enough to be able to engage in a seminar based study of book 1 with Online Great Books.
Approaching rigorous, proof based mathematics from millennia past might seem like a daunting task to some. It actually could not be further from daunting; it is thrilling. When examined with wonder, discoveries abound. The study of spacial relation that is geometry is the most natural math for the human maid to apprehend, and is a unique type of gymnastics for the mind. Plato required mastery of geometry before entry the academy. It&amp;rsquo;s at the heart of the physical sciences, at the center of logic. This system remained unaltered for millennia, passing through the conflagration at Alexandria, and to this day still mostly still does.
The first step to a study of Euclid is to take a look over the proofs and draw them out on paper. Draw them by hand with nothing but a pencil, straight edge and dividers; use color! Use the previous proofs to produce the later ones, throw away your ruler and dive into the continuum without discrete measurement. The elegance is stunning; every proof is used, no thought is wasted. From a point, described as that which has no parts, Euclid&amp;rsquo;s system builds to geometries which can be used to describe the universe we inhabit, along with an entirely abstract continuum which exists only in the mind.
Euclid, like most things, is best understood with fellow travelers. While solitary study will yield understanding, the eye of the other is the best tool with which to refine our apprehension. Using the format of Socratic seminars to study Euclid allows for joint inquiry, and for me helped illuminate the substance of Euclidean geometries as the soil that allowed modernity to bloom. Once you&amp;rsquo;ve drawn out the proofs and thought about them, it is amazing to see the variety approaches and connections other folks in seminar bring to the table.
Through the dialectic of the seminar, the group, along with the individuals it is composed of, emerge with an understanding entirely other than anybody arrived with. The Socratic seminar is the key to getting a deeper understanding, broader context and better a calibration of how to approach the material. So to sum it up, pencil, paper, dividers, straightedge, fellow travelers; put all this in the format of a Socratic seminar, and you&amp;rsquo;ve got the bones of an elucidating study of The Elements.
]]></content:encoded></item><item><title>Creating a Zettelkasten with Hugo</title><link>https://bryanapperson.com/blog/creating-a-zettelkasten-with-hugo/</link><pubDate>Tue, 18 Feb 2020 01:42:00 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/creating-a-zettelkasten-with-hugo/</guid><description>Why do I need a Zettelkasten? The idea of a Zettelkasten struck me in a chat recently. I have been keeping physical journals, using a modified form of bullet journalling, for a while; I had been keeping less structured notes before that, and ruminating on how to digitize them. After learning about the concept of Zettelkasten, it seemed like the solution; and that led me down the rabbit hole of investigation.</description><content:encoded><![CDATA[Why do I need a Zettelkasten? The idea of a Zettelkasten struck me in a chat recently. I have been keeping physical journals, using a modified form of bullet journalling, for a while; I had been keeping less structured notes before that, and ruminating on how to digitize them. After learning about the concept of Zettelkasten, it seemed like the solution; and that led me down the rabbit hole of investigation. Because of that investigation, I am now building a Zettlekasten for myself. The idea of a way to grok all notes for eternity is an obvious boon to any knowledge worker. This system has less obvious, and much greater benefits beyond just digitizing notes.
What is a Zettelkasten? The Zettelkasten is a concept, essentially, of a graph of data interconnected by metadata, links and taxonomies. It was originally laid out by a German sociologist: If you wish to educate a partner in communication, it will be good to provide him with independence from the beginning. A slip box, which has been made according to the suggestions just given can exhibit great independence.
Communicating With Slip Boxes, by Niklas Luhmann  It was also explored in this physical format by Umberto Eco (1977) in How to Write a Thesis. This format involved drawers full of annotated index cards.
This loosely organized graph allows a Cartesian explosion of complexity. The core idea of the system is a second brain, one mirroring the structure of the human mind. This organic structure is how zufall, or randomness, eventually emerges, resulting in serendipitous discovery. The weighting of links produces more prominent and less prominent regions, much like search engine rankings. This starts to sound an awful lot like a preeminent description of the internet and search engines. There is, however, another aspect to consider:
Without variation in the given material of ideas, there are no possibilities of examining and selecting novelties. The real problem thus becomes therefore one of producing accidents with sufficiently enhanced probabilities for selection.
Niklas Luhmann, Communicating With Slip Boxes  The distillation of broader research into notes with metadata allows for your own personal internet, with a less hierarchical structure than say, a wiki. The balance between order and chaos is essential, Luhmann purports, to the extract synthesis of new ideas from a body of notes.
What Makes a Zettelkasten? Zettels A zettel is a set of data, along with related attachments and metadata. Every zettel, or note, is a node in the graph that becomes the Zettelkasten.
Arbitrary Internal Branching Any zettel must have the ability to branch and be a parent of other zettels
Possibility of Linking Any zettel must be able to link to any other zettel.
Index, or Register of Taxonomies and Tags For the zettels to be useful in the concept of the Zettelkasten, there must be an index or register that is searchable.
Inflow There must be a source of content. For me that is my paper notes and digital notes. Those notes must be collected; then they must be processed into zettels with metadata, and links to other zettels.
Outflow The Zettelkasten is accessed via the register to search for ideas while writing or producing other outputs. Optionally there is a feedback into the inflow during this process.
How Can We Make a Digital Zettelkasten? Thoughts on Design The physical design of the system as proposed by Luhmann is outdated. Physical space was a concern, but in the context of cheap hard drive space it is not. The issue ordering also becomes irrelevant. Though, the question of a flat structure to prevent usage bias, or obsession over categorization and order, has it&amp;rsquo;s merit. So digital is definitely the way to go.
That leads to the question of how best to do it. There are ready made solutions out there like The Archive, and that may be the solution for most. It does seem however to duplicate extant functionalities in hypertext and web browsers. All we really need is URIs, metadata, and a search system for that metadata to create a functioning Zettelkasten.
Enter: A Hugo Based Zettelkasten In From WordPress to Hugo, I migrated my blog to Hugo. This proved to be the extant solution I was looking for. I thought about an optimal design for a Zettelkasten; one that would be portable, open source, and stand the test of time. A solution that could check the boxes of arbitrary metadata, linking and search of those things. Hugo, with its use of markdown, standard URIs and static page generation is a great solution.
I stumbled upon the After Dark theme for hugo, by Josh Habdas. It already had many of the features I needed, such as search, citations, and rich linking between pages, or zettels implemented. So I switched my blog over to the aforementioned theme. I then created a private git submodule repository, and put that in the content/zettelkasten folder of my hugo setup. Voila, a public blog with a private Zettelkasten.
The only caveat on the theme is that it uses Fuse.js for search. I am not sure if it is the implementation in the theme, or the nature of the fuzzy search, but it isn&amp;rsquo;t always the best at finding things. I am considering tweaking the implementation or switching it out for Lunr, but those are future problems.
When the CI pipeline runs to build the blog, does not have permissions to pull the Zettelkasten, which is secured by private key. But when I run a local development server, I have both my Zettelkasten and my blog, an outflow, searchable at my fingertips.
What about portability? Here is the cool part, Hugo can run on android. So with git, I can just clone my blog on my phone and run a local web server. More to follow on that in another blog post.
What&amp;rsquo;s Next? I still have to approach getting all of my notes into zettels. Maybe I&amp;rsquo;ll have more to say after that. I&amp;rsquo;ll also likely add a technical step by step for setting up a Zettelkasten using Hugo and the After Dark theme, as time permits. Leave your thoughts in the comments below.
]]></content:encoded></item><item><title>Mounting RBD at Boot Under CentOS 7</title><link>https://bryanapperson.com/blog/mounting-rbd-at-boot-under-centos-7/</link><pubDate>Tue, 18 Feb 2020 00:17:32 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/mounting-rbd-at-boot-under-centos-7/</guid><description>This tutorial covers mounting an RBD image at boot under CentOS 7. Make sure to unmount the RBD you want to have mount at boot before following this tutorial. This tutorial requires a CentOS 7 client with a client or admin keyring from Ceph, and a working Ceph cluster. This tutorial also assumes you have already created the RBD image you want to be mounted at boot. Let&amp;rsquo;s begin!
Assumptions This tutorial assumes the node you are implementing this on has connectivity to a working ceph cluster and also assumes that kernel module RBD is enabled.</description><content:encoded><![CDATA[This tutorial covers mounting an RBD image at boot under CentOS 7. Make sure to unmount the RBD you want to have mount at boot before following this tutorial. This tutorial requires a CentOS 7 client with a client or admin keyring from Ceph, and a working Ceph cluster. This tutorial also assumes you have already created the RBD image you want to be mounted at boot. Let&amp;rsquo;s begin!
Assumptions This tutorial assumes the node you are implementing this on has connectivity to a working ceph cluster and also assumes that kernel module RBD is enabled. For the purposes of this tutorial I will place variables, the values specified here:
export poolname = your_pools_name export rbdimage = the_name_of_your_rbd_image export mountpoint = place_to_mount_the rbd Create A systemd service to map and mount automatically on boot / demand You will want to automatically load the kernel module, map the appropriate rbd storage to a local device and mount the ceph image. Here is a simple script for mounting and un-mounting RBD images create one at /usr/bin/mount-rbd-$poolname-$rbdimage for each of your RBD images:
#!/bin/bash # Image mount/unmount and pool are passed from the systems service as arguments # Determine if we are mounting or unmounting if [ &amp;#34;$1&amp;#34; == &amp;#34;m&amp;#34; ]; then modprobe rbd rbd map --pool $poolname $rbdimage --id admin --keyring /etc/ceph/ceph.client.admin.keyring mkdir -p $mountpoint mount /dev/rbd/$poolname/$rbdimage $mountpoint fi if [ &amp;#34;$1&amp;#34; == &amp;#34;u&amp;#34; ]; then umount $mountpoint rbd unmap /dev/rbd/$poolname/$rbdimage fi Create a new systemd service unit (/etc/systemd/system/mount-rbd-$poolname-$rbdimage.service) for each of your remote rbd images:
[Unit] Description=RADOS block device mapping for $rbdimage in pool $poolname&amp;#34; Conflicts=shutdown.target Wants=network-online.target After=NetworkManager-wait-online.service [Service] Type=oneshot RemainAfterExit=yes ExecStart=/usr/bin/mount-rbd-$poolname-$rbdimage m ExecStop=/usr/bin/mount-rbd-$poolname-$rbdimage u [Install] WantedBy=multi-user.target Make sure your target RBD is unmounted. Start the service and check whether /dev/rbd0 is created or not:
systemctl start mount-rbd-$poolname-$rbdimage
Mounting an RBD at Boot Under CentOS 7 is Easy! If everything seems to be fine, enable the service to start on boot:
systemctl enable mount-rbd-$poolname-$rbdimage
You now have a working RBD mount at boot time! I wil be following this up with a complete tutorial on the entire process of creating an RBD at some point in the future. Leave your thoughts in the comments below.
]]></content:encoded></item><item><title>Install HHVM, Nginx on Ubuntu 14.04 - Make WordPress Fly</title><link>https://bryanapperson.com/blog/install-hhvm-nginx-on-ubuntu-14-04-make-wordpress-fly/</link><pubDate>Tue, 18 Feb 2020 00:12:01 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/install-hhvm-nginx-on-ubuntu-14-04-make-wordpress-fly/</guid><description>Installing HHVM and Nginx on Ubuntu 14.04 is the next step in the &amp;ldquo;Make WordPress Fly&amp;rdquo; series. This tutorial assumes you have completed the prerequisites, read Part 1 and completed Part 2 of this guide. At this point you have a reasonably secure box with MariaDB installed and configured. In this (Part 3) of the &amp;ldquo;Make WordPress Fly&amp;rdquo; guide we will start out by preparing our system for Nginx. The first step is to reconnect to your VM via SSH.</description><content:encoded><![CDATA[Installing HHVM and Nginx on Ubuntu 14.04 is the next step in the &amp;ldquo;Make WordPress Fly&amp;rdquo; series. This tutorial assumes you have completed the prerequisites, read Part 1 and completed Part 2 of this guide. At this point you have a reasonably secure box with MariaDB installed and configured. In this (Part 3) of the &amp;ldquo;Make WordPress Fly&amp;rdquo; guide we will start out by preparing our system for Nginx. The first step is to reconnect to your VM via SSH.
ssh -p port user@you.rip.add.res
Installing Nginx on Ubuntu 14.04 After reconnecting we are going to install some prerequisites in this order to make sure HHVM plays nicely with Nginx and WordPress.
sudo apt-get update sudo apt-get install php5-gd libssh2-php After that process completes it is time to install Nginx. Installing Nginx on Ubuntu 14.04 is a very easy process. Ubuntu official repos come with a Nginx package but I prefer using launchpad repo maintained by Nginx team. We will also install the Naxsi WAF (Web Application Firewall) to provide some added security. You can choose not load Naxsi later as it slows down cached requests per second by around 3%. However, a full fledged WAF is worth a 3% requests per second hit.
sudo add-apt-repository ppa:nginx/stable sudo apt-get update sudo apt-get install nginx-naxsi If you prefer to use Nginx package in Ubuntu repo, you can simply run following command:
sudo apt-get install nginx-naxsi
That concludes the process of installing Nginx. We will configure it further in later parts of this tutorial series based on whether you use WordPress Multisite or a single install.
Installing HHVM on Ubuntu 14.04 Let&amp;rsquo;s move on to installing HHVM on Ubuntu 14.04. We&amp;rsquo;ll need to prepare the HHVM repositories. Using sudo or as root user it is recommended to run sudo apt-get update and sudo apt-get upgrade first, or you may receive errors. Then we are ready to add the repositories and install HHVM.
wget -O - http://dl.hhvm.com/conf/hhvm.gpg.key | sudo apt-key add - echo deb http://dl.hhvm.com/ubuntu trusty main | sudo tee /etc/apt/sources.list.d/hhvm.list sudo apt-get update sudo apt-get install hhvm Now that HHVM is installed there are a few simple configurations to apply. HHVM comes bundled with a script that makes setting it up with Ubuntu very easy. If you are already using Nginx with PHP-FPM, you&amp;rsquo;ll have to modify the configuration file to disable the use of PHP-FPM. This file is normally located at /etc/nginx/sites-available/default Look for the following section and make sure it&amp;rsquo;s all commented (by adding a # at the beginning of each line)
# pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ { # fastcgi_split_path_info ^(.&#43;\.php)(/.&#43;)$; # # NOTE: You should have &amp;quot;cgi.fix_pathinfo = 0;&amp;quot; in php.ini # # # With php5-cgi alone: # fastcgi_pass 127.0.0.1:9000; # # With php5-fpm: # fastcgi_pass unix:/var/run/php5-fpm.sock; # fastcgi_index index.php; # include fastcgi_params; #} After doing this, execute the following script:
/usr/share/hhvm/install_fastcgi.sh Executing this script configures Nginx to start using HHVM to process the PHP code. It&amp;rsquo;ll also restart the Nginx server so you don&amp;rsquo;t have to do anything else. Then you may want to tweak the max_upload_size of HHVM by editing /etc/hhvm/php.ini. Otherwise HHVM is now setup and working.
Verifying that HHVM is Working Correctly With Nginx and Ubuntu 14.04 It is important verify that HHVM is working with Nginx. You can verify this by creating a file in /usr/share/nginx/html called test.php. Paste this inside:
&amp;lt;?php echo defined(&#39;HHVM\_VERSION&#39;)?&#39;Using HHVM&#39;:&#39;Not using HHVM&#39;; ?&amp;gt; Visit http://you.rip.add.res/test.php to view the output. This will verify that HHVM is handling PHP. Now just make sure that HHVM and Nginx run by default at startup.
sudo update-rc.d nginx defaults sudo update-rc.d hhvm defaults You are ready to move on to the next part of this tutorial.
]]></content:encoded></item><item><title>A Trip to Acoma Pueblo</title><link>https://bryanapperson.com/blog/a-trip-to-acoma-pueblo/</link><pubDate>Tue, 18 Feb 2020 00:05:38 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/a-trip-to-acoma-pueblo/</guid><description>Back in November of 2013 I had the opportunity to visit the Navajo nation and the Hopi and Acoma Pueblos. The Acoma had a nice museum and guided tours of a mesa-top city know as &amp;ldquo;Sky City&amp;rdquo;. Acoma Pueblo is a Native American pueblo approximately sixty miles west of Albuquerque, New Mexico in the United States. Three villages make up Acoma Pueblo: Sky City (Old Acoma), Acomita, and McCartys. The Acoma Pueblo tribe is a federally recognized tribal entity.</description><content:encoded><![CDATA[Back in November of 2013 I had the opportunity to visit the Navajo nation and the Hopi and Acoma Pueblos. The Acoma had a nice museum and guided tours of a mesa-top city know as &amp;ldquo;Sky City&amp;rdquo;. Acoma Pueblo is a Native American pueblo approximately sixty miles west of Albuquerque, New Mexico in the United States. Three villages make up Acoma Pueblo: Sky City (Old Acoma), Acomita, and McCartys. The Acoma Pueblo tribe is a federally recognized tribal entity. The historical land of Acoma Pueblo totaled roughly 5 million acres; now only 10% of this land is in the hands of the community within the Acoma Indian Reservation. This picture was taken in Old Acoma.
My wife and I were able to snap some beautiful shots of the surrounding area. The mesas of Arizona and New Mexico are breathtaking places.
   The Acoma Pueblo    I have to say that the climb down the &amp;ldquo;stairs&amp;rdquo; with my daughter Alice on my back was a fun experience.
   The Acoma Pueblo Stairs Down    The Acoma pueblo is interesting in that it was destroyed by the Spaniards back in the 17th century via cannon fire. The lower levels of housing, seen in sandstone are what remains. When it was rebuilt under Spanish rule, the Spanish taught the pueblo people to use mud bricks, which are much more high maintenance and require yearly stucco.
]]></content:encoded></item><item><title>Ceph Raw Disk Performance Testing</title><link>https://bryanapperson.com/blog/ceph-raw-disk-performance-testing/</link><pubDate>Mon, 17 Feb 2020 23:27:54 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/ceph-raw-disk-performance-testing/</guid><description>Ceph raw disk performance testing is something you should not overlook when architecting a ceph cluster. When choosing media for use as a journal or OSD in a Ceph cluster, determining the raw IO characteristics of the disk when used in the same way ceph will use the disk is of tantamount importance before tens, hundreds or thousands of disks are purchased. The point of this article is to briefly discuss how ceph handles IO.</description><content:encoded>Ceph raw disk performance testing is something you should not overlook when architecting a ceph cluster. When choosing media for use as a journal or OSD in a Ceph cluster, determining the raw IO characteristics of the disk when used in the same way ceph will use the disk is of tantamount importance before tens, hundreds or thousands of disks are purchased. The point of this article is to briefly discuss how ceph handles IO. One important point is to estimate the deviation caused by ceph between RAW IOs from disk and ceph IOs.
TESTING &amp;amp;amp; GRAPHING WITH FIO For this article I assume you are aware of fio and feel some degree of comfort uing it. You will need FIO and GNUPlot installed to run these tests. I have developed an automation tool in my spare time for writing these tests. You can find it here: ceph-disk-test RBD can best be simulated by using a block size of 4M in your testing. However it is pertinent to test with smaller IOs like 64k or 4k for worst case. Below is an example test run with a Samsung Extreme USB stick to demonstrate how the results look using this automation. The automation produces a nice graphs.
Journal IO Journal IO in Ceph uses O_DIRECT and D_SYNC flags. Journals write with an IO Depth of 1 (1 IO at a time). However if you colocate multiple journals you should increase your IO depth to the number of journals you plan to colocate on the drive. Here is an example FIO test for testing journal performance on a disk:
ioengine=libaio invalidate=1 ramp_time=5 iodepth=1 runtime=300 time_based direct=1 sync=1 bs=4m size=10240m filename=/tmp/tmp.ohf4hXHp1F/test.file [seq-write] stonewall rw=write write_bw_log=/tmp/tmp.QpdVIjrlnI/150825-1653-seq-write-journal-4m-intel-dc3700-d1 write_lat_log=/tmp/tmp.QpdVIjrlnI/150825-1653-seq-write-journal-4m-intel-dc3700-d1 write_iops_log=/tmp/tmp.QpdVIjrlnI/150825-1653-seq-write-journal-4m-intel-dc3700-d1 write_iolog=/tmp/tmp.QpdVIjrlnI/150825-1653-seq-write-journal-4m-intel-dc3700-d1 [rand-write] stonewall rw=randwrite write_bw_log=/tmp/tmp.QpdVIjrlnI/150825-1653-rand-write-journal-4m-intel-dc3700-d1 write_lat_log=/tmp/tmp.QpdVIjrlnI/150825-1653-rand-write-journal-4m-intel-dc3700-d1 write_iops_log=/tmp/tmp.QpdVIjrlnI/150825-1653-rand-write-journal-4m-intel-dc3700-d1 write_iolog=/tmp/tmp.QpdVIjrlnI/150825-1653-rand-write-journal-4m-intel-dc3700-d1 OSD IO OSDs use buffered IO and thus you need to run performance tests of a size and duration that is greater then the amount of RAM in the test machine. Here is an example test file for an OSD:
ioengine=libaio invalidate=1 ramp_time=5 iodepth=32 runtime=120 time_based direct=0 bs=4m size=10240m filename=/tmp/tmp.taNceuCnCq/test.file [seq-write] stonewall rw=write write_bw_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-write-osd-4m-intel-dc3700 write_lat_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-write-osd-4m-intel-dc3700 write_iops_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-write-osd-4m-intel-dc3700 write_iolog=/tmp/tmp.IF0gdpokNE/150825-1218-seq-write-osd-4m-intel-dc3700 [rand-write] stonewall rw=randwrite write_bw_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-write-osd-4m-intel-dc3700 write_lat_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-write-osd-4m-intel-dc3700 write_iops_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-write-osd-4m-intel-dc3700 write_iolog=/tmp/tmp.IF0gdpokNE/150825-1218-rand-write-osd-4m-intel-dc3700 [seq-read] stonewall rw=read write_bw_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-read-osd-4m-intel-dc3700 write_lat_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-read-osd-4m-intel-dc3700 write_iops_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-read-osd-4m-intel-dc3700 write_iolog=/tmp/tmp.IF0gdpokNE/150825-1218-seq-read-osd-4m-intel-dc3700 [rand-read] stonewall rw=randread write_bw_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-read-osd-4m-intel-dc3700 write_lat_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-read-osd-4m-intel-dc3700 write_iops_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-read-osd-4m-intel-dc3700 write_iolog=/tmp/tmp.IF0gdpokNE/150825-1218-rand-read-osd-4m-intel-dc3700 References Research material used to structure these ceph raw disk performance tests:
Journal Testing OSD Testing Ceph IO, The Bad</content:encoded></item><item><title>Installing WordPress with Nginx on Ubuntu 14.04</title><link>https://bryanapperson.com/blog/installing-wordpress-with-nginx-on-ubuntu-14-04/</link><pubDate>Mon, 17 Feb 2020 23:10:08 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/installing-wordpress-with-nginx-on-ubuntu-14-04/</guid><description>Installing WordPress with Nginx on Ubuntu 14.04 is a fairly straightforward task. In this tutorial we will do over how to do it. This tutorial assumes you have completed the Getting Started with an Ubuntu VPS guide and have an Ubuntu 14.04 VPS (if not you can get one at vultr). It also assumes that you already have a LEMP stack setup (Linux, Nginx, MySQL, etcetera) or you are following the WordPress HHVM guide.</description><content:encoded><![CDATA[Installing WordPress with Nginx on Ubuntu 14.04 is a fairly straightforward task. In this tutorial we will do over how to do it. This tutorial assumes you have completed the Getting Started with an Ubuntu VPS guide and have an Ubuntu 14.04 VPS (if not you can get one at vultr). It also assumes that you already have a LEMP stack setup (Linux, Nginx, MySQL, etcetera) or you are following the WordPress HHVM guide. This tutorial assumes the use of Nginx as the web server, Fastcgi or HHVM for PHP and either MariaDB or MySQL for your MySQL server. The first step in this tutorial is to connect to your virtual machine via SSH. This tutorial assumes that you are using Linux as your operating system and have SSH installed. If you do not you can use a tool like Putty for SSH. In Linux you just need to run the following command:
ssh -p port user@you.rip.add.res After connecting to your instance via SSH it is time to begin the process of installing WordPress to work with Nginx. All of our tutorials for Nginx assume a &amp;ldquo;web root&amp;rdquo; of /var/www/html, make sure that your Nginx configuration points there and that the directory exists. If the directory does not exist create it using mkdir and chown it to www-data .
sudo mkdir /var/www/ sudo mkdir /var/www/html/ sudo chown -R www-data:www-data /var/www/html/ Creating a Database and User After you have confirmed that Nginx is using /var/www/html/ as your web root or setup another of your choice, it&amp;rsquo;s time to create a database for WordPress. Please make sure you have already setup MySQL or MariaDB prior to this step. Setting up the database is easy. Start by logging into an interactive session with the MySQL administrative account.
mysql -u root -p You will be prompted for the root password you setup during MySQL installation. Enter it and proceed to the interactive prompt. Next we are going to create a database for WordPress to use and store information in. The name of the database does not matter, but it should be memorable so that you can distinguish it as you add additional databases later on. To do this simply run this command:
CREATE DATABASE wordpress; Note the semi-colon (;) that ends the MySQL statement. Every MySQL statement must end with one, so check that if you are running into issues. Now that you have created a database, we need to create a user. You are going to use the same interactive interface you are in now to create a user. Use this command:
CREATE USER wordpressuser@localhost IDENTIFIED BY &#39;password&#39;; Make sure you replace &amp;lsquo;password&amp;rsquo; with the database password you want to use and &amp;lsquo;wordpressuser&amp;rsquo; with the name of the database user you want to create. After that is done you need to assign that user privileges to use the database we just created. Use this command:
GRANT ALL PRIVILEGES ON wordpress.\* TO wordpressuser@localhost; Make sure you replace the database name and username with the ones you created. Everything should now be configured correctly. We need to flush the privileges (save them to disk) so that our current instance of MySQL knows about the privilege changes we have made:
FLUSH PRIVILEGES; Now you can exit MySQL:
exit
At this point you are back at the shell command prompt and ready to continue.
Installing WordPress with Nginx on Ubuntu 14.04 The next step is to download the latest version of WordPress to the server. It is available on their website. We are going to use the wget command to copy it to our home directory. WordPress always keeps the latest stable version at the place we will use in this command.
cd ~ wget http://wordpress.org/latest.tar.gz The files which compose WordPress were downloaded as a compressed archive stored in a file called latest.tar.gz. We can extract the contents by typing:
tar xzvf latest.tar.gz This will extract a directory called wordpress containing all the files we need to set up WordPress. First however make sure that php5-gd and libssh2-php are installed. If they are not, run the command below. This will make sure you can work with images and install modules/plugins over SSH.
sudo apt-get update sudo apt-get install php5-gd libssh2-php Configuring WordPress with Nginx on Ubuntu 14.04 Now we are ready to configure WordPress and move it into the web root. Let&amp;rsquo;s move into the directory that we extracted WordPress to in the last section:
cd ~/wordpress Now we want to copy the sample configuration to take the place of the non-existent main configuration.
cp wp-config-sample.php wp-config.php
Next we need to make 3 small changes to wp-config.php using nano or your text editor of choice.
nano wp-config.php
The file is suitable for launching WordPress; it is just lacking the information to connect to the database we created a few minutes ago. The parameters we need to set are DB_NAME , DB_USER , and DB_PASSWORD . After you make the changes to that section of the file it should look something like this:
// ** MySQL settings - You can get this info from your web host ** // /** The name of the database for WordPress */ define(&#39;DB_NAME&#39;, &#39;wordpress&#39;); /** MySQL database username */ define(&#39;DB_USER&#39;, &#39;wordpressuser&#39;); /** MySQL database password */ define(&#39;DB_PASSWORD&#39;, &#39;password&#39;); For now you can ignore the rest of the site. If you are planning on deploying a multisite network add this line:
/* Multisite */ define( &amp;lsquo;WP_ALLOW_MULTISITE&amp;rsquo;, true );
Once you have made these changes you can save and close the file. Now it is time to copy the files to our web root (/var/www/html/ in this example). We can copy the files to this place by typing:
sudo rsync -avP ~/wordpress/ /var/www/html/ Now we need to move over to that folder to assign some permissions.
cd /var/www/html/ Then we are going to make sure that Nginx owns these files so that it can manipulate them.
sudo chown -R www-data:www-data /var/www/html/*
Before we move on, we should create a new directory for user uploads:
mkdir wp-content/uploads The new directory should have group writing set already, but the new directory isn&amp;rsquo;t assigned with www-data group ownership yet. Let&amp;rsquo;s fix that:
sudo chown -R :www-data /var/www/html/wp-content/uploads Now just make sure that your web server is configured to use /var/www/html/ as the webroot and you can visit yourdomain.com to set your site name and get started. You are also going to want to install Postfix so that WordPress can send emails. We will be writing a tutorial for that in the near future. Thanks for reading and leave your thoughts in the comments below.
]]></content:encoded></item><item><title>Hiking to Ciudad Perdida</title><link>https://bryanapperson.com/blog/hiking-to-ciudad-perdida-the-lost-city/</link><pubDate>Mon, 17 Feb 2020 21:39:44 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/hiking-to-ciudad-perdida-the-lost-city/</guid><description>The Trip On a Saturday evening in August of 2018 my other half and I and left the Upper East Side of Manhattan in a taxi bound for JFK. Excitement and apprehension were in the air as we were eager to leave, but about to commend our kids to my mother in law for a week. We had, and have not since, been away for so long from our children. After arriving, we were informed that we were extremely early.</description><content:encoded>The Trip On a Saturday evening in August of 2018 my other half and I and left the Upper East Side of Manhattan in a taxi bound for JFK. Excitement and apprehension were in the air as we were eager to leave, but about to commend our kids to my mother in law for a week. We had, and have not since, been away for so long from our children. After arriving, we were informed that we were extremely early. Stuck in the purgatory of the waiting area outside the terminal, we polished off of dinner in a nondescript diner, eagerly biding our time. After a period of time, we passed through security and boarded our red-eye to Santa Marta by way of Bogota.
Our Journey to the Lost City Arrival Having gathered a little bit of sleep on the plane, we were in Santa Marta, and it was a glorious radiant morning. Into a taxi bound for Hotel Tayromar we went. After a drive through this unfamiliar country and a conversation utilizing my rusty Spanish, we arrived. With the day free and our bags stowed at the hotel, fighting the need for sleep, we took to the streets. Cafe Ikaro was adjacent, thus it was the next logical stop for coffee and a bite to eat. We snapped some excellent photos following an afternoon rainstorm.
Downtown Santa Marta When we roused from an early evening nap, a steak and ceviche dinner followed. We had introductions and some time to get familiar with the group who would enter our lives for the next five days. After conversing with our guides and fellow travelers, we turned in, prepared for the journey ahead.
Day 1 - Santa Marta to Wiwa Camp At dawn our group hopped into two all wheel drive vehicles. The beginning of a one and a half hour winding ride up into the foothills of la Sierra Nevada de Santa Marta. Breakfast was served in a village by the name of Machete Pelao. One of our guides explained the namesake, people there invariably had to maintain a machete at the ready. The substantial food and better hospitality after the Colombian fashion set us up for a good day of walking.
Once breakfast had settled drove a bit further, lurching to a halt at the trail-head. The pavement ended. The dust from our rides driving away cemented the finality of the fact that we would devote the next five days to being afoot. The beginning of the day was invested walking on a dirt road that was mixed use, though the lone vehicles we observed were motorcycles. The road wound up farther into the mountains, through cattle pasture and coffee fields that had until perhaps a decade ago been under cultivation for cocaine and cannabis. The conversion to coffee, cattle and tourism represented a turn for the better. During the midday heat we stopped to have a break at a roadside refreshment stand and I observed the first local arachnid of the trip.
Say cheese Our path continued, traversing a few more miles in the sweltering heat of the exposed pasture. The relief was palpable when we reached the border of Wiwa and Kogi lands. This zone was under permacultural cultivation and the forest canopy was in tact. The perceived and effective temperature of the August sun in Colombia was much reduced. We started to pass creeks, rivers and springs unused for intensive agriculture. Our indigenous guide drank directly from the springs. Though our minds still felt a need to pass the water through chemical and mechanical purification, perhaps out of fear of the unknown. We included a few stops along the way to consume succulent fruits, which aided me immensely.
Towards the waning of the day, we began passing by Kogi settlements. Most of the tribe still lives in the traditional fashion. It was explained to us that the elect and the Mamos are the solitary ones burdened by interacting with the external world. Utilizing unnatural or composite materials to deliver opportunity to the community and export their ideas. We got a glimpse into a reflection what life in precolonial Colombia might have been like. At this point there were no more motorcycles or cars, all transit was powered by human or mule.
Kogi Village on the 1st day Some time later once the hinterlands of dusk were playing upon the sky, we struck Wiwa camp, our home for the night. There was a natural pool in the Buritaca River nearby, and we all experienced a much needed dip in the cool mountain waters. Once we had devoured a hearty dinner prepared by our trail chef discovered stories and myths regaled by our Wiwa guide and translated to us in English by our jovial Colombian guide. After listening to the tales and enjoying a cold beer, it was time to retire. At Wiwa Camp beds and hammocks were provided. However, my wife and had brought our own two person hammock, that would prove a sensible choice. We suspended it and slept like the dead.
Day 2 - Wiwa Camp to Paraiso The morning arrived with stiffness present. We had mistakenly packed more then needed, being used to unsupported hikes. We shook down our packs and took down our shelter. We took the opportunity to drop off many bits of unneeded gear in a lockup at this camp, to come back for it in a few days. Feeling heartened by a lighter load and a solid breakfast we embraced the day.
A spectacular waterfall about 60ft high There was an optional trip to a nearby waterfall which a subset of the group, us included, partook of. We all swam, which at least for me helped soothe the ache of my neophyte trail legs. Then our guide suggested standing under the waterfall, which I withstood the fury of, a backcountry massage. After some relaxation we left to reunite with the remnant who had remained at camp and resume our journey. On the way out we had the privilege of observing a traditional greeting ritual.
Traditional Wiwa greeting ceremony After that we hit the trail for a shorter day by mileage, but a harder one in terms of elevation change and terrain. After some time following the river, we had an opportunity to take a swim. The next leg of the journey was described to the group as two hours up, one hour down and two hours up. Given that, we all decided to stop and swim. Afterwards we resumed our journey and went uphill. Then we kept going uphill.
Once we got to the top of the first mountain, a few hours later, we stopped for some delicious Colombian green oranges and a rest. Once we were all sufficiently recovered our traversal of the Sierra resumed in earnest. Passing, and almost being run over by mules was constant enjoyment. We pushed to Teyuna Paraiso Camp and arrived in early afternoon, beating a rainstorm by a few minutes. After this small blessing we ate lunch, some of us swam again or soaked sore feet in the ever-colder Buritaca.
The Buritaca River, our stalwart guide I had lugged a deck of cards against humanity all the way through the jungle. It was a boon as it kept us busy when we were under the roof, dodging the foul weather. Playing with the locals and people from all around the world was extremely interesting, if a strain for our bilingual guide, who translated it all. After the game we had a scrumptious dinner prepared by our awesome camp chefs, once again.
Our Wiwa guide explained more about his culture and the ritual items such as the porporo, bag and coca leaves. He told us various stories of the Wiwa verbal tradition. Then our Colombian guide told us more about the contemporary history and rediscovery of Teyuna by looters in the nineteen seventies. Apparently the looters had seen the error of their ways and some of them are now involved in running the treks. After enjoying some peaceful time, we all turned in - ready to see the Lost City.
Day 3 - The Lost City At about four o&amp;amp;rsquo;clock, before the sun had even thought of rising, we awoke. Our culinary companions miraculously had breakfast served by four thirty. Afterwards, we hurriedly stowed our packs and departed for the Lost City, carrying only water bottles. Leaving so early made us the first group out of camp and the first to arrive at Ciudad Perdida. The sun was rising by the time we reached the crossing of the Buritaca.
The stairs were rediscovered quite by accident. You could pass by them on the riverbank without knowing they were there. Even now the are shrouded in the deep green of the forest. We crossed the swiftly flowing river on the upstream side of a rope to prevent us from taking an unwanted ride downstream. After getting shoes back on wet feet, we started to climb. All the way up one thousand two hundred steps, made with narrow stones twelve centuries ago.
The entrance to Teyuna, main axis When we reached the top, we made an offering of coca leaves to the ancestors guarding the city. Then, our guides told us all about the various features of the main axis, which is the central excavated portion. Foolishly I did not take notes or make journal entries, and my memories three quarters of a year later are wanting. One sharp thing in my memory is the map of all the waterways and trails in the Sierra, carved centuries ago. The fact that most of these sites and trails are so remote is intriguing. It seems likely to me that there is still much to be discovered about the precolonial civilization in la Sierra Nevada de Santa Marta.
A map of la Sierra Nevada de Santa Marta, made centuries ago After climbing a few hundred more steps, those of us with the energy climbed another set, and then another. From the rarefied heights we had a birds eye view of a mostly empty main axis. The scale is not done justice by the pictures. What is pictured here is perhaps five percent of the entire settlement. That settlement further extends under the jungle not as of yet excavated in all directions.
The main axis of Teyuna, the lone soldier gives an idea of scale This was where the nobility and priesthood lived for centuries, a city that would fill with peasantry from the countryside during rituals. Each generation building on the houses of the previous. All of the rings are foundations of these multi-generational houses and ceremonial buildings. The wood bodies of which are long since gone. Ever September the whole city still closes for these rituals. Our guide morbidly hinted that they used to involve human sacrifice.
Our group on the main axis of Ciudad Perdida After the main axis we made wishes on a ceremonial stone that had been there for ten or more centuries. Our group had the unique chance to see a newly uncovered area. A part of the city where craftspeople would produce refined goods. Some of the stone tooling was still present. This was where all the metallurgy that had attracted the treasure hunters in the seventies took place.
We had an exclusive look at the newly excavated workshop area We walked by the home of the Mamo, who is caretaker of the city. Passing through his gardens provided an image of what the houses on top of those foundations might have once looked like. Then it was time for the long climb down the steep and narrow stairway. Once down the stairway, after stopping at Paraiso to pick up our bags and eat, it was time for the long walk back to Wiwa Camp.
On the way back to Wiwa Camp there was a torrential downpour. The mountains turned into mudslides and the rivers swelled up like angry serpents. We were in a race against the weather. Sometimes it felt like the rivers would sweep you away during a crossing. Luckily we all made it in without any injury. Dinner and more stories about the locale and indigenous beliefs ensued. Then we went to sleep for what must have been a solid twelve hours.
Day 4 - Wiwa Camp to Vista Hermosa We left Wiwa Camp after a hearty breakfast and retraced our path back along the way we had come in day prior. Although the day was balmy, it is amazing how much I, at least, had adapted to the heat. The vistas were as stunning as they were on the way out. I enjoyed the way back more as I had my trail legs and was able to expend that surplus energy in admiration of the scenery. The trail was shepherding us back towards the outside world.
We stopped for a fruit break at Adán Camp. There were artifacts there that had been looted from the Lost City when it was initially discovered. This was fascinating to see, but definitely highlighted the exploitation which had occurred there. After eating and catching our wind, some of us stopped to swim. The rest of us continued uphill to Ricardito Camp.
On the trip up the hill, we encountered the first motorcycles and telephone signal in days. We were coming back to the world, and I was not completely certain that was something I desired. When we arrived, we enjoyed another fantastic lunch. Then we enjoyed an afternoon of sitting around and relaxing. That was something I was certainly ready for.
The views from Ricardito Camp, known locally as Vista Hermosa Once we had decompressed and bathed in the views, yet another extraordinary dinner came and went. Then, for the first time on the trip, we had a campfire. We sat around the fire and talked about the Wiwa creation myths. Later on we sang songs in native tongues, Spanish and English, reflecting on the journey so far. Slowly members of the group drifted off and then we turned in, ready for the final day of walking yet to come.
Day 5 - Vista Hermosa to Gotsezhi Village Rolling pasture on the way to Gotsezhi Village From Ricardito Camp we had a unique opportunity to visit Gotsezhi Village, which in an indigenous Wiwa community. Walking through the pastures on our way there we were out in the open under the tropical sun. Luckily we had an early start and reached our destination before the heat of the day. When we arrived at Gotsezhi Village we had the opportunity to enjoy more traditional cuisine prepared by the Wiwa people and shop for goods produced locally by the Wiwa.
Once we took that in and took another swim in another gorgeous swimming hole, it was time to return to Santa Marta. We all boarded our 4x4 livery for a two hour ride. The roads were some of the roughest I have ever been on. Unexpectedly the air conditioning in the vehicles felt unbearably cold. It is amazing how the body changes after adjusting to sleeping under the stars in the tropical heat for a week.
On return to Santa Marta, we all had some time to settle in, then went out on the town for a final dinner. The dinner was at a local restaurant and we were treated to the famous Colombian hospitality. Afterwards, a subset of the group went out dancing to experience the Santa Marta nightlife, us included. This was our last night in Colombia, and so there was reason to resist the strong urge to sleep.
Departure The next morning, being just two again, we bid farewell to the hotel and went out for breakfast at a cafe. Then we hopped in a taxi, feeling nostalgic for our time in the jungle and headed back to our own concrete jungle.</content:encoded></item><item><title>Ceph OSD Performance: Backends and Filesystems</title><link>https://bryanapperson.com/blog/ceph-osd-performance-backends-and-filesystems/</link><pubDate>Mon, 17 Feb 2020 21:24:48 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/ceph-osd-performance-backends-and-filesystems/</guid><description>Ceph OSD performance characteristics are one of the most important considerations when deploying a RADOS (Replicated Asynchronous Distributed Object Storage) cluster. Ceph is an open source project for scale out storage based on the CRUSH algorithm. An OSD is an &amp;ldquo;Object Storage Daemon&amp;rdquo;, which represents a journaling partition and a data storage partition in the Filestore backend implementation. An OSD is, in a broader sense where Ceph stores objects which hash to a specific placement group (PG).</description><content:encoded><![CDATA[Ceph OSD performance characteristics are one of the most important considerations when deploying a RADOS (Replicated Asynchronous Distributed Object Storage) cluster. Ceph is an open source project for scale out storage based on the CRUSH algorithm. An OSD is an &amp;ldquo;Object Storage Daemon&amp;rdquo;, which represents a journaling partition and a data storage partition in the Filestore backend implementation. An OSD is, in a broader sense where Ceph stores objects which hash to a specific placement group (PG). A placement group is a hash bucket in a general computer science sense. This article explores the performance characteristics and features of various Ceph OSD backends and filesystems. The content of this article was prepared for and presented at the April 12th, 2016 Ceph ATL meetup.
Ceph OSD Performance Optimal Ceph OSD performance can reduce the capital expense and operational expense of meeting deployment requirements for a Ceph storage cluster. There are many considerations and best practices when deploying a Ceph/RADOS cluster which can enhance performance and stability. Many of those, such as kernel optimizations, network stack optimizations, choice of hardware and Ceph tuning parameters are outside the scope of this article. For those interested in other performance enhancement vectors for Ceph deployments, some were covered at the Ceph ATL Kick-Off Meetup, and many can be found in the Red Hat/Supermicro Ceph reference architecture document. However, perhaps obviously, the interface to backing storage block devices is integral in determining the performance of a RADOS cluster. The most widely used deployment is with the OSD filestore backend and the XFS filesystem. There are interesting developments in Ceph Jewel, namely the bluestore backend which may change that.
Ceph OSD Backends As of the Ceph Jewel release there will be multiple backends which can be used for Ceph OSDs. This article covers filestore, bluestore and memstore.
Filestore At present filestore is the de-facto backend for production Ceph clusters. With the filestore backend, Ceph writes objects as files on top of a POSIX filesystem such as XFS, BTRFS or EXT4. With the filestore backend a OSD is composed of an un-formatted journal partition and an OSD data partition.
One of the largest drawbacks with the OSD filestore backend is the fact that all data is written twice, through the journal and then to the backing data partition. This essentially cuts the write performance of an OSD with co-located journal and data in twain. This has resulted in many deployments using dedicated solid state block devices split up into multiple partitions for journals. Many deployments use a 4:1 or 5:1 ratio for journals to SSD an disk. This requires the use of additional drive bays and increases the cost to performance ratio significantly.
Filestore is the only backend bench-marked in this article.
Filesystems The Ceph filestore OSD backend supports XFS, BTRFS and EXT4 filesystems. The documentation presently recommends XFS for use in production, and BTRFS for testing and development environments. Below is a comparison of Ceph OSD performance for these three filesystems. But before going into Ceph OSD performance, a feature comparison is useful.
XFS XFS is used at present in many production Ceph deployments. XFS was developed for Silicon Graphics, and is a mature and stable filesystem. The filestore/XFS combination is well tested, stable and great for use today. There are some tuning parameters which can be used during filesystem creation and mounting as an OSD data partition which can be used to improve Ceph OSD performance. These parameters are included in the Ceph configuration on the github page for these tests.
BTRFS BTRFS is a copy-on-write filesystem. It supports file creation timestamps and checksums that verify metadata integrity, so it can detect bad copies of data and fix them with good copies. BTRFS very interestingly supports transparent LZO and GZIP compression among and other features. While the performance of compression on BTRFS will not be covered in this article, the use in a large scale storage cluster is obvious. The BTRFS community also aims to provide fsck, deduplication, and data encryption support. BTRFS is not recommended at this time for production use with Ceph, however according to BTRFS developers, it is no longer experimental.
EXT4 EXT4 is a solid, battle tested filesystem. However, with a maximum size of 16TB, it is not exactly future proof (considering that Samsung has already released a 16TB drive). It is production ready for use as a Ceph filestore filesystem.
Bluestore Bluestore is set to release for experimental use in Jewel. The benefits of Bluestore are a direct to block OSD, without filesystem overhead or the need for a &amp;ldquo;double-write&amp;rdquo; penalty (associated with the filestore journal). Bluestore utilizes RocksDB, which stores object metadata, a write ahead log, Ceph omap data and allocator metadata. Bluestore can have 2-3 partitions per, one for RocksDB, one for RocksDB WAL and one for OSD data (un-formatted - direct to block). Due to time constraints on the presentation, and the lack of a Ceph Jewel build for Fedora 23. I may follow this up with a comparison of the best performing filestore backend on RHEL7 and Ceph bluestore. RocksDB and RocksDB WAL can be placed on the same partition. For the tests below both RocksDB and OSD data were colocated on the same physical disk for an apples-to-apples comparison with a co-located filestore backend. A great in depth explanation of the OSD bluestore backend is available here.
MEMSTORE Memstore is an available backend for Ceph. However, it should never be used in production due to the obvious volatility of memory. Steps for enabling memstore can be found here. Due to the fact that memstore is not a serious backend for production use, no performance tests were run with it.
Ceph OSD PERFORMANCE TEST METHOD This section covers how the performance tests in this article were executed, the environment and overall method. Since this article is strictly about OSD backends and filesystems, all tests were executed on a single machine to eliminate network related variance. Journal partitions (for the filestore backend) were co-located on the same physical disks as the data partitions.
ENVIRONMENT All performance tests in this article were performed using Ceph Hammer. The specifications for the test machine are as follows:
 CPU: Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz RAM: 32GB DDR4 2133 MHz OSDs: 5 x Seagate ST2000DM001 2TB OS: Fedora 23 OS Drive: 2 x Samsung 850 Pro SSD (BTRFS RAID1)  The OSD Performance Test Rig[/caption] The test environment was my personal desktop system. The OSD hard drives are consumer grade Seagate drives. Block device/drive type has a huge impact on the performance of a Ceph storage cluster, these 7200 RPM SATA III drives were used for all tests in this article. To more on how to test the raw performance characteristics of a physical drive for use as a OSD journal, data or co-located journal/data device see this github repo. This test cluster was a single monitor, single node &amp;ldquo;cluster&amp;rdquo;. The SATA controller was on-board ASM1062 and nothing fancy like an LSI-2308. There was a PCIe &amp;ldquo;cache tier&amp;rdquo; present with 2 M2 form factor SSDs as OSDs, although those were unused in these tests. The test machine was running kernel 4.3.3-300.
CONFIGURATIONS The Ceph configurations filesystem tested are available on this github repository. All tests were performed with 3 replica storage pools.
Tools The build-in rados bench command was used for all performance metrics in this article.
Ceph OSD Performance Test Results Note: Take the random read speeds with a grain of salt. Even with running echo 3 &amp;gt; /proc/sys/vm/drop_caches in between benchmarks, randomly read objects may have already been stored in memory.
* XFS: * 4 MB * Write: * IOPS: 19.17 * BW: 76.516 MB/s * Latency: 0.835348s * Read: * IOPS: 118.38 * BW: 473.466MB/s * Latency: 0.134731s * 4 KB * Write: * IOPS: 203.124 * BW: 0.790MB/s * Latency: 0.0789896s * Read: * IOPS: 209.33 * BW: 0.812MB/s * Latency: 0.076963s Conclusion The Filestore/XFS deployment scenario may be the stable way to go for production Ceph clusters at the present. BTRFS/Filestore may be the most feature rich. However, with the development of Bluestore this may change in the future.
]]></content:encoded></item><item><title>A Study of the Great Books</title><link>https://bryanapperson.com/blog/a-study-of-the-great-books/</link><pubDate>Mon, 17 Feb 2020 21:21:18 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/a-study-of-the-great-books/</guid><description>In August 2019 I became interested in reading the great books of Western Civilization. This interest started whilst considering, at length, the public school system in the United States. Both in consideration of my mixed experiences with it, and in planning for my children’s educations. I have always been an avid reader. However, I haven’t read many of, or studied in great depth, the foundational works of the West. In my research I stumbled upon Online Great Books via the Art of Manliness Podcast.</description><content:encoded>In August 2019 I became interested in reading the great books of Western Civilization. This interest started whilst considering, at length, the public school system in the United States. Both in consideration of my mixed experiences with it, and in planning for my children’s educations. I have always been an avid reader. However, I haven’t read many of, or studied in great depth, the foundational works of the West. In my research I stumbled upon Online Great Books via the Art of Manliness Podcast.
This started me on a journey of reading deeply into the western canon as outlined in How to Read a Book by Mortimer Adler. For the time being, I am using Online great books, for a community to discuss these works with. This series contains my musings on the works as I read them and my eventualy contain important resources and study methods.
My Musings on the Great Books</content:encoded></item><item><title>A SimpleBiped with an RTOS on an Arduino Mini</title><link>https://bryanapperson.com/blog/a-simplebiped-with-an-rtos-on-an-arduino-mini/</link><pubDate>Mon, 17 Feb 2020 21:08:08 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/a-simplebiped-with-an-rtos-on-an-arduino-mini/</guid><description>About eight years back, I wrote up a program for and designed a bipedal robot (I coined it the SimpleBiped) based on the Arduino Micro board. This article is an outline of what I did, mainly for reference if I revisit this later, but hopefully its useful to you as well. I designed a bipedal robot wh That code looks something like this:ich uses ultrasonic sensors for object detection, and some basic logic for navigation as well as some primitives for servo control.</description><content:encoded>About eight years back, I wrote up a program for and designed a bipedal robot (I coined it the SimpleBiped) based on the Arduino Micro board. This article is an outline of what I did, mainly for reference if I revisit this later, but hopefully its useful to you as well. I designed a bipedal robot wh That code looks something like this:ich uses ultrasonic sensors for object detection, and some basic logic for navigation as well as some primitives for servo control. The robot had 5 servos as defined in the code below. Here is a SimpleBiped hardware spreadsheet on what I used for the build. The body parts were found on Thingiverse, modified in blender and printed with an old thing-o-matic.
That printer was awesome at the time and definitely served it’s purpose. It unfortunately no longer works (going to make some iteration of the Prusa eventually). I don’t have pictures of the robot and my last iteration is in storage in NY right now (I am in Georgia). I will post the blender parts up some time on my Thingiverse (all that is there now is the enclosure for the Arduino micro I integrated into the body) and link them here. Once (and if) I get started on this I’ll post circuit diagrams as well. At first the robot would walk, stop, scan for objects - then make a choice and continue walking.
The code can be found on github. I did try to add an RTOS. Unfortunately I never got this working smoothly and the robot would jitter too much while walking due to the interrupts. Mutexs and Semaphores are tough to get right, I’d love to revisit this project at a later time (once I have a working 3D printer for body components again). Feel free to use this code (but not the name SimpleBiped) in your projects under a GNU/GPL liscence, or (and I would appreciate it), tell me where I went wrong with my mutexs!
If and when I do revisit this project I will definitely post further updates here.</content:encoded></item><item><title>Setup OpenVPN on Ubuntu the Easy Way</title><link>https://bryanapperson.com/blog/setup-openvpn-on-ubuntu-the-easy-way/</link><pubDate>Mon, 17 Feb 2020 21:03:31 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/setup-openvpn-on-ubuntu-the-easy-way/</guid><description>The best way to setup OpenVPN on Ubuntu, like many other things, is to script it. This way it&amp;rsquo;s easier to create uniform deployment across larger networks. So, this is how you setup OpenVPN on Ubuntu the easy way - this neat little script makes installing OpenVPN on an Ubuntu VPS simple:
Go to your home directory:
cd ~ Then create a file by running this command:
cat &amp;gt; openvpn.sh #!</description><content:encoded><![CDATA[The best way to setup OpenVPN on Ubuntu, like many other things, is to script it. This way it&amp;rsquo;s easier to create uniform deployment across larger networks. So, this is how you setup OpenVPN on Ubuntu the easy way - this neat little script makes installing OpenVPN on an Ubuntu VPS simple:
Go to your home directory:
cd ~ Then create a file by running this command:
cat &amp;gt; openvpn.sh #!/usr/bin/env bash # # Functions ok() { echo -e &amp;#39;\e[32m&amp;#39;$1&amp;#39;\e[m&amp;#39;; } die() { echo -e &amp;#39;\e[1;31m&amp;#39;$1&amp;#39;\e[m&amp;#39;; exit 1; } # Sanity check if [[ $(id -g) != &amp;#34;0&amp;#34; ]] ; then die &amp;#34;❯❯❯ Script must be run as root.&amp;#34; fi if [[ ! -e /dev/net/tun ]] ; then die &amp;#34;❯❯❯ TUN/TAP device is not available.&amp;#34; fi dpkg -l openvpn &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 if [[ $? -eq 0 ]]; then die &amp;#34;❯❯❯ OpenVPN is already installed.&amp;#34; fi # Install openvpn ok &amp;#34;❯❯❯ apt-get update&amp;#34; apt-get update -q &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 ok &amp;#34;❯❯❯ apt-get install openvpn curl openssl&amp;#34; apt-get install -qy openvpn curl &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 # IP Address SERVER_IP=$(curl ipv4.icanhazip.com) if [[ -z &amp;#34;${SERVER_IP}&amp;#34; ]]; then SERVER_IP=$(ip a | awk -F&amp;#34;[ /]&#43;&amp;#34; &amp;#39;/global/ &amp;amp;&amp;amp; !/127.0/ {print $3; exit}&amp;#39;) fi # Generate CA Config ok &amp;#34;❯❯❯ Generating CA Config&amp;#34; openssl dhparam -out /etc/openvpn/dh.pem 2048 &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 openssl genrsa -out /etc/openvpn/ca-key.pem 2048 &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 chmod 600 /etc/openvpn/ca-key.pem openssl req -new -key /etc/openvpn/ca-key.pem -out /etc/openvpn/ca-csr.pem -subj /CN=OpenVPN-CA/ &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 openssl x509 -req -in /etc/openvpn/ca-csr.pem -out /etc/openvpn/ca.pem -signkey /etc/openvpn/ca-key.pem -days 365 &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 echo 01 &amp;gt; /etc/openvpn/ca.srl # Generate Server Config ok &amp;#34;❯❯❯ Generating Server Config&amp;#34; openssl genrsa -out /etc/openvpn/server-key.pem 2048 &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 chmod 600 /etc/openvpn/server-key.pem openssl req -new -key /etc/openvpn/server-key.pem -out /etc/openvpn/server-csr.pem -subj /CN=OpenVPN/ &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 openssl x509 -req -in /etc/openvpn/server-csr.pem -out /etc/openvpn/server-cert.pem -CA /etc/openvpn/ca.pem -CAkey /etc/openvpn/ca-key.pem -days 365 &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 cat &amp;gt; /etc/openvpn/udp1194.conf &amp;lt; /dev/null 2&amp;gt;&amp;amp;1 chmod 600 /etc/openvpn/client-key.pem openssl req -new -key /etc/openvpn/client-key.pem -out /etc/openvpn/client-csr.pem -subj /CN=OpenVPN-Client/ &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 openssl x509 -req -in /etc/openvpn/client-csr.pem -out /etc/openvpn/client-cert.pem -CA /etc/openvpn/ca.pem -CAkey /etc/openvpn/ca-key.pem -days 36525 &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 cat &amp;gt; /etc/openvpn/client.ovpn &amp;lt; $(cat /etc/openvpn/client-key.pem) $(cat /etc/openvpn/client-cert.pem) $(cat /etc/openvpn/ca.pem) EOF # Iptables if [[ ! -f /proc/user_beancounters ]]; then N_INT = $(ip a |awk -v sip=&amp;#34;$SERVER_IP&amp;#34; &amp;#39;$0 ~ sip { print $7}&amp;#39;) iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o $N_INT -j MASQUERADE else iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -j SNAT --to-source $SERVER_IP fi iptables-save &amp;gt; /etc/iptables.conf cat &amp;gt; /etc/network/if-up.d/iptables &amp;lt; /proc/sys/net/ipv4/ip_forward # Restart Service ok &amp;#34;❯❯❯ service openvpn restart&amp;#34; service openvpn restart &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 ok &amp;#34;❯❯❯ Your client config is available at /etc/openvpn/client.ovpn&amp;#34; ok &amp;#34;❯❯❯ All done!&amp;#34;&amp;lt;/pre&amp;gt; Press CTRL&#43;D to save. Then:
chmod 755 openvpn.sh This simple script will got OpenVPN installed and working on your VM or box easily. OpenVPN is a great way to connect to a work network, remain private, and encrypt your endpoint.
In just a few seconds you are all set, the script will automatically install OpenVPN and all the necessary dependencies, configure, and add a new user. Then just connect via SFTP and download the files to connect. Place them in the OpenVPN config directory on Windows or setup the values to match on a linux desktop.
OpenVPN is a very secure tunnel and I highly recommend it. I get near native speed running OpenVPN on a 512MB RAM Ubuntu 14.04 VM.
]]></content:encoded></item><item><title>The Definitive Guide: Ceph Cluster on Raspberry Pi</title><link>https://bryanapperson.com/blog/the-definitive-guide-ceph-cluster-on-raspberry-pi/</link><pubDate>Mon, 17 Feb 2020 20:58:50 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/the-definitive-guide-ceph-cluster-on-raspberry-pi/</guid><description>A Ceph cluster on Raspberry Pi is an awesome way to create a RADOS home storage solution (NAS) that is highly redundant and low power usage. It&amp;rsquo;s also a low cost way to get into Ceph, which may or may not be the future of storage (software defined storage definitely is as a whole). Ceph on ARM is an interesting idea in and of itself. I built one of these as a development environment (playground) for home.</description><content:encoded><![CDATA[A Ceph cluster on Raspberry Pi is an awesome way to create a RADOS home storage solution (NAS) that is highly redundant and low power usage. It&amp;rsquo;s also a low cost way to get into Ceph, which may or may not be the future of storage (software defined storage definitely is as a whole). Ceph on ARM is an interesting idea in and of itself. I built one of these as a development environment (playground) for home. It can be done on a relatively small budget. Since this was a spur of the moment idea, I purchased everything locally. I opted for the Raspberry Pi 2 B (for the 4 cores and 1GB of RAM). I&amp;rsquo;d really recommend going with the Pi 2 B, so you have one core and 256MB RAM for each USB port (potential OSD). In this guide I will outline the parts, software I used and some options that you can use for achieving better performance. This guide assumes you have access to a Linux PC with an SD card reader. It also assumes you have a working knowledge of Linux in general and a passing familiarity with Ceph.
Parts Although I will explain many options in this guide, this is the minimum you will need to get a cluster up and running, this list assumes 3 Pi nodes.
 3 x 3ft Cat6 Cables 3 x Raspberry Pi 2 B 3 x Raspberry Pi 2 B Case 3 x 2 Amp Micro USB Power Supply 3 empty ports on a gigabit router 3 x Class 10 MicroSD (16GB or more) for OS drive 3-12 x USB 2.0 Flash Drives (at least 32GB, better drive for better performance)  I used 3 x 64GB flash drives, 3 x 32GB MicroSD and existing ports on my router. My cost came in at about $250. You can add to this list based on what you add to your setup throughout the guide, but this is pretty much the minimum for a fully functional Ceph cluster.
Operating System Raspbian. The testing repository for Raspbian has the many packages of Ceph 0.80.9 and dependencies pre-compiled. Everything you&amp;rsquo;ll need for this tutorial and is the &amp;ldquo;de facto&amp;rdquo; OS of choice for flexibility on Raspberry Pi. You can download the Raspbian image here: Raspbian Download. Once you have the image, you&amp;rsquo;ll want to put it on an SD card. For this application I recommend using at least a 16GB MicroSD card (Class 10 preferably - OS drive speed matters for Ceph monitor processes). To transfer the image on Linux, you can use DD. run the lsblk command to display your devices once you&amp;rsquo;ve inserted the card into your card reader. Then you can use dd to transfer the image to your SD. The command below assumes the image name is raspbian-wheezy.img and that it lives in your present working directory. The above command also assumes that your SD card is located at /dev/mmcblk0 adjust these accordingly and make sure that your SD card doesn&amp;rsquo;t contain anything important and is empty.
sudo dd bs=4M if=raspbian-wheezy.img of=/dev/mmcblk0
This command will take a few minutes to complete. Once it does run sync to flush all cache to disk and make sure it is safe to remove the device. You&amp;rsquo;ll then boot up into Raspbian, re-size the image to the full size of your MicroSD, set a memorable password, overclock if you want. Once this is done there are a few modifications to make. We&amp;rsquo;ll get into this in the installation section below. I don&amp;rsquo;t recommend using too large of a MicroSD as later in this tutorial we will image the whole OS from our first MicroSD for deployment to our other Pi nodes.
Hardware Limitations The first limitation to consider is overall storage space. Ceph OSD processes require roughly 1MB of RAM per GB of storage. Since we are co-locating monitor processes the effective storage limitation is 512GB per Pi 2 B (4 x 128GB sticks) RAW (before Ceph replication or erasure coding overhead). Network speed is also a factor as discussed later in document. You will hit network speed limitations before you hit the speed limitations of the Pi 2 B&amp;rsquo;s single USB 2.0 bus (480Mbit).
Network In this setup I used empty ports on my router. I run a local DNS server on my home router and use static assignments for local DNS. You may want to consider just using a flat 5 or 8 port (depending on number of nodes you plan to have) gigabit switch for the cluster network and WiPi modules for the public (connected to your router via WiFi). The nice thing about using a flat layer 2 switch is that if all the Pi nodes are in the same subnet, you don&amp;rsquo;t have to worry about a gateway and it also keeps the cost down (compared to using router ports) while reducing the network overhead (for Ceph replication) on your home network. Using a dedicated switch for the cluster network will also increase your cluster performance, especially considering the 100Mbit limitations of the Pi 2 B&amp;rsquo;s network port. By using a BGN Dongle for Pi and a dedicated switch for the cluster network, you will get a speedier cluster. This will use one of your 4 USB ports and thus, you will get one less OSD per Pi. Keep in mind, depending on if you use replication or erasure coding private traffic can be 1-X times greater then client IO (X being 3 in a standard replication profile) if that matters for your application. Of course this is all optional and for additional &amp;ldquo;clustery goodness&amp;rdquo;. It really depends on budget, usage - etcetera.
Object Storage Daemons In this guide, I co-located OSD journals on the OSD drives. For better performance, you can use a faster USB like the SanDisk Extreme 3.0 (keep in mind that you&amp;rsquo;ll be limited by the 60MB/s speed of USB 2.0). Using a dedicated (faster) journal drive will yield much better performance. But you don&amp;rsquo;t really need to worry about it unless you are using multiple networks as outlined above. If you are not, 4 decent USB sticks will saturate your 100Mbit NIC per node. There is a lot more to learn about Ceph architecture that I cover in this article and I highly recommend you do so here.
OSD Filesystem XFS is the default in Ceph Firefly. I prefer BTRFS as an OSD filesystem for multi-fold reasons and I use it in this tutorial.
Installation Assuming you have setup your network and operating system - have 3 nodes and the hardware you want to use - we can begin. The first thing to do is wire up power and network as you see fit. After that, you&amp;rsquo;ll want to run through the initial raspi-config on what will become your admin node. Then it&amp;rsquo;s time to make some changes. Once your admin node is booted and configured, you have to edit /etc/apt/sources.list . Raspbian Wheezy has archaic versions of Ceph in the main repository, but the latest firefly version in the testing repository. Before we delve into this, I find it useful to install some basic tools and requirements. Connect via SSH or directly to terminal and issue this command from the Pi:
sudo apt-get install vim screen htop iotop btrfs-tools lsb-release gdisk
From this point forward we will assume you are connecting to your Pi nodes via SSH. You&amp;rsquo;ve just installed BTRFS-tools, vim (better then vi) and some performance diagnostics tools I like. Now that we have vim it&amp;rsquo;s time to edit our sources:
vi /etc/apt/sources.list
You&amp;rsquo;ll see the contents of your sources file. Which will look like this:
deb http://mirrordirector.raspbian.org/raspbian/ wheezy main contrib non-free rpi
Uncomment line below then &amp;lsquo;apt-get update&amp;rsquo; to enable &amp;lsquo;apt-get source&amp;rsquo; #deb-src http://archive.raspbian.org/raspbian/ wheezy main contrib non-free rpi
Modify it to look like this:
deb http://mirrordirector.raspbian.org/raspbian/ testing main contrib non-free rpi
Uncomment line below then &amp;lsquo;apt-get update&amp;rsquo; to enable &amp;lsquo;apt-get source&amp;rsquo; #deb-src http://archive.raspbian.org/raspbian/ testing main contrib non-free rpi
We&amp;rsquo;ve replaced wheezy with testing .Once this is done, then issue this command:
sudo apt-get update
Once this process has completed is time to start getting the OS ready for Ceph. Everything we do in this section up to the point of imaging the OS is needed for nodes that will run Ceph. First we will create a ceph user and give it password-less sudo access. To do so issue these commands:
ssh user@ceph-server sudo useradd -d /home/ceph -m ceph sudo passwd ceph Set the password to a memorable one as it will be used on all of your nodes in this guide. Now we need to give the ceph user sudo access
echo &amp;#34;ceph ALL = (root) NOPASSWD:ALL&amp;#34; | sudo tee /etc/sudoers.d/ceph sudo chmod 0440 /etc/sudoers.d/ceph We&amp;rsquo;ll be using ceph-deploy later and it&amp;rsquo;s best to have a defult user to login as all the time. Issue this command:
mkdir -p ~/.ssh/ Then create this file using vi:
vi ~/.ssh/config I assume 3 nodes in this tutorial and a naming convention of piY, where Y is the node number starting from 1.
Host pi1 Hostname pi1 User ceph Host pi2 Hostname pi2 User ceph Host pi3 Hostname pi3 User ceph Save the file and exit. As far as hostnames, you can use whatever you want of course. As I mentioned, I run local DNS and DHCP with static assignments. If you do not, you&amp;rsquo;ll need to edit /etc/hosts so that your nodes can resolve each-other. You can do this after the OS image, as each node will have a different IP. Now it&amp;rsquo;s time to install the ceph-deploy tool. Raspbian wget can be strange with HTTPS so we will ignore the certificate (do so at your own peril):
wget --no-check-certificate -q -O- &amp;#39;https://ceph.com/git/?p=ceph.git;a=blob\_plain;f=keys/release.asc&amp;#39; | sudo apt-key add - echo deb http://ceph.com/debian-firefly/ wheezy main | sudo tee /etc/apt/sources.list.d/ceph.list Now that we&amp;rsquo;ve added the Ceph repository, we can install ceph-deploy:
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install ceph-deploy ceph ceph-common
Since we are installing ceph from the Raspbian repositories, we need to change the default behavior of ceph-deploy:
sudo vi /usr/share/pyshared/ceph_deploy/hosts/debian/install.py
Change
def install(distro, version_kind, version, adjust_repos): codename = distro.codename machine = distro.machine_type To
def install(distro, version_kind, version, adjust_repos): adjust_repos = False codename = distro.codename machine = distro.machine_type This will prevent ceph-deploy from altering repos as the Ceph armhf (Rasberry Pi&amp;rsquo;s processor type) repos are mostly empty. Finally, we should revert the contents of /etc/apt/sources.list :
sudo vi /etc/apt/sources.list
You&amp;rsquo;ll see the contents of your sources file. Which will look like this:
deb http://mirrordirector.raspbian.org/raspbian/ testing main contrib non-free rpi # Uncomment line below then &#39;apt-get update&#39; to enable &#39;apt-get source&#39; #deb-src http://archive.raspbian.org/raspbian/ testing main contrib non-free rpi Modify it to look like this:
deb http://mirrordirector.raspbian.org/raspbian/ wheezy main contrib non-free rpi # Uncomment line below then &#39;apt-get update&#39; to enable &#39;apt-get source&#39; #deb-src http://archive.raspbian.org/raspbian/ wheezy main contrib non-free rpi We&amp;rsquo;ve replaced testing with wheezy .Once this is done, then issue this command:
sudo apt-get update Kernel Tweaks We are also going to tweak some kernel parameters for better stability. To do so we will edit /etc/sysctl.conf .
vi /etc/sysctl.conf
At the bottom of the file, change add the following lines:
vm.swappiness=1 vm.min_free_kbytes = 32768 kernel.pid_max = 32768 Imaging the OS Now we have a good baseline for deploying ceph to our other Pi nodes. It&amp;rsquo;s time to stop our admin node and image the drive (MicroSD). Issue:
sudo halt
Then unplug power to your Pi node and remove the MicroSD. Insert the microSD in your SD adapter, then the SD adapter into your Linux PC. You&amp;rsquo;ll need at least as much free drive space on your PC as the size of the MicroSD card.Where /dev/mmcblk0 is your SD card and pi-ceph.img is your image destination, run:
sudo dd if=/dev/mmcblk0 of=ceph-pi.img bs=4M
This can take a vary long time depending on the size of your SD and you can compress it with gzip or xz for long term storage (empty space compresses really well it turns out). Once the command returns, run sync to flush the cache to disk and make sure you can remove the MicroSD
Imaging Your Nodes OS Drives Now that you have a good baseline image on your PC, you are ready to crank out &amp;ldquo;Ceph-Pi&amp;rdquo; nodes - without redoing all of the above. To do so, insert a fresh MicroSD into your adapter and then PC. Then assuming ceph-pi.img is your OS image and /dev/mmcblk0 is your MicroSD card run:
sudo dd if=ceph-pi.img of=/dev/mmcblk0 bs=4M
Repeat this for a many nodes as you intend to deploy.
Create a Ceph Cluster on Raspberry Pi Insert your ceph-pi MicroSD cards into your Pi nodes and power them all on. You&amp;rsquo;ve made it this far, now it&amp;rsquo;s time to get &amp;ldquo;cephy&amp;rdquo;. Deploying with ceph-deploy is a breeze. First we need to SSH to our admin node, make sure you have setup IPs, network and /etc/hosts on all Pi nodes if you are not using local DNS and DHCP with static assignments. We need to generate and distribute an SSH key for password-less authentication between nodes. To do so run (leave the password blank):
ssh-keygen Generating public/private key pair. Enter file in which to save the key (/ceph-client/.ssh/id\_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /ceph-client/.ssh/id\_rsa. Your public key has been saved in /ceph-client/.ssh/id\_rsa.pub. Now copy the key to all nodes (assuming 3 with the naming convention from above): ssh-copy-id ceph@pi1 ssh-copy-id ceph@pi2 ssh-copy-id ceph@pi3 You will be prompted for the password you created for the ceph user each time to establish initial authentication. Once that is done and you are connected to your admin node (1st node in the cluster) as the pi user you&amp;rsquo;ll want to create an admin node directory:
mkdir -p ~/ceph-pi-cluster cd ~/ceph-pi-cluster Creating an initial Ceph Configuration We are going to create an initial Ceph configuration assuming all 3 pi nodes as monitors. If you have more, keep in mind - you always want an odd number of monitors to avoid a split-brain scenario. To to this run:
ceph-deploy new pi1 pi2 pi3
Now there are some special tweaks that should be made for best stability and performance within the hardware limitations of the Raspberry Pi 2 B. To apply these changes we&amp;rsquo;ll need to edit the ceph.conf here on the admin node before it is distributed. To do so:
vi ~/ceph-pi-cluster/ceph.conf
After the existing lines add:
# Disable in-memory logs debug_lockdep = 0/0 debug_context = 0/0 debug_crush = 0/0 debug_buffer = 0/0 debug_timer = 0/0 debug_filer = 0/0 debug_objecter = 0/0 debug_rados = 0/0 debug_rbd = 0/0 debug_journaler = 0/0 debug_objectcatcher = 0/0 debug_client = 0/0 debug_osd = 0/0 debug_optracker = 0/0 debug_objclass = 0/0 debug_filestore = 0/0 debug_journal = 0/0 debug_ms = 0/0 debug_monc = 0/0 debug_tp = 0/0 debug_auth = 0/0 debug_finisher = 0/0 debug_heartbeatmap = 0/0 debug_perfcounter = 0/0 debug_asok = 0/0 debug_throttle = 0/0 debug_mon = 0/0 debug_paxos = 0/0 debug_rgw = 0/0 osd heartbeat grace = 8 [mon] mon compact on start = true mon osd down out subtree_limit = host [osd] # Filesystem Optimizations osd journal size = 1024 # Performance tuning max open files = 327680 osd op threads = 2 filestore op threads = 2 #Capacity Tuning osd backfill full ratio = 0.95 mon osd nearfull ratio = 0.90 mon osd full ratio = 0.95 # Recovery tuning osd recovery max active = 1 osd recovery max single start = 1 osd max backfills = 1 osd recovery op priority = 1 # Optimize Filestore Merge and Split filestore merge threshold = 40 filestore split multiple = 8 Creating Initial Monitors Now we can deploy our spiffy ceph.conf, create our initial monitor daemons, deploy our authentication keyring and chmod it as needed. We will be deploying to all 3 nodes for the purposes of this guide:
ceph-deploy mon create-initial ceph-deploy admin pi1 pi2 pi3 for i in pi1 pi2 pi3;do ssh $i chmod 644 /etc/ceph/ceph.client.admin.keyring;done Creating OSDs (Object Storage Daemons) Ready to create some storage? I know I am. Insert your USB keys of choice into your Pi USB ports. For the purposes of this guide I will be deploying 1 OSD (USB key) per Pi node. I will also be using the BTRFS filesystem and co-locating the journals on the OSDs with a default journal size of 1GB (assuming 2 * 40MB/s throughput max and a default filestor max sync interval of 5). This value is hard coded into our ceph-pi config above. The formula is:
osd journal size = {2 * (expected throughput * filestore max sync interval)}
So let&amp;rsquo;s deploy our OSDs. Once our USBs are plugged in, use lsblk to display the device locations. To make sure our drives are clean and have a GPT partition table, use the gdisk command for each OSD on each node. Assuming /dev/sda as our OSD: gdisk /dev/sda Create a new partition table, write it to disk and exit. Do this for each OSD on each node. You can craft a bash for loop if you are feeling &amp;ldquo;bashy&amp;rdquo; or programmatic. Once all OSD drives have a fresh partition table you can use ceph-deploy to create your OSDs (using BTRFS for this guide) where pi1 is our present node and /dev/sda is the OSD we are creating:
ceph-deploy osd create --fs-type btrfs pi1:/dev/sda
Repeat this for all OSD drives on all nodes (or write a for loop). Once you&amp;rsquo;ve created at least 3 you are ready to move on.
Checking Cluster Health Congratulations! You should have a working Ceph-Pi cluster. Trust, but verify. Get the health status of your cluster using this command:
ceph -s
and for a less verbose output
ceph health
What to do now? Use your storage cluster! Create an RBD, mount it - export NFS or CIFS. There is a lot of reading out there. Now you know how to deploy a Ceph cluster on Raspberry Pi.
]]></content:encoded></item><item><title>Configure Nginx/HHVM for WP - Making WordPress Fly</title><link>https://bryanapperson.com/blog/configure-nginx-hhvm-for-wp-making-wordpress-fly/</link><pubDate>Mon, 17 Feb 2020 20:40:54 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/configure-nginx-hhvm-for-wp-making-wordpress-fly/</guid><description>The next step in the tutorial &amp;ldquo;Making WordPress Fly&amp;rdquo; is to configure Nginx/HHVM and install WordPress. This step has two options, configuring for single site (this article) or configuring for multisite. This tutorial will assume that you have completed the prerequisites and read the introduction (part one). It will also that you have completed both parts two and three. We will also assume that you have an Ubuntu VPS. If you don&amp;rsquo;t, you can get one at Vultr.</description><content:encoded><![CDATA[The next step in the tutorial &amp;ldquo;Making WordPress Fly&amp;rdquo; is to configure Nginx/HHVM and install WordPress. This step has two options, configuring for single site (this article) or configuring for multisite. This tutorial will assume that you have completed the prerequisites and read the introduction (part one). It will also that you have completed both parts two and three. We will also assume that you have an Ubuntu VPS. If you don&amp;rsquo;t, you can get one at Vultr. At this point you should have your VPS secured. You should also have MariaDB, Nginx, and HHVM installed. The first step in this section is to reconnect to your VM via SSH.
ssh -p port user@you.rip.add.res
After connecting to your instance, we are going to create a location to install WordPress. We are going to use /var/www/html however you can use a different directory like /var/www/html/domain-com/ if you choose to. Be aware that you will have to update the configurations to match if you choose a different directory structure. Creating the new directory is straightforward.
sudo mkdir /var/www/html/
We need to install WordPress to /var/www/html . The guide for that can be found here. Once you have completed that, you are ready to move on to the next step. Configuring Nginx for Multisite WordPress with W3 Total cache is the most involved part of the &amp;ldquo;Making WordPress Fly&amp;rdquo; series. At this point we are going to assume that you have been following the series since the beginning. We are also going to assume you already have an Ubuntu VPS. So far we have secured an Ubuntu VPS, setup MariaDB, installed HHVM and installed Nginx. Now we are ready for part 4a, to configure Nginx to work with WordPress Multisite and W3 Total Cache. Part 4b is the alternative if you only need a single site WordPress install on your HHVM enhanced, WordPress optimized VPS. Choosing a multisite install definitely makes sense as you can use this VPS to host more then a few sites, even if the get significant traffic.
That&amp;rsquo;s all for now.
]]></content:encoded></item><item><title>MariaDB 10.1 Setup for Ubuntu 14.04 - Make WordPress Fly</title><link>https://bryanapperson.com/blog/mariadb-10-1-setup-for-ubuntu-14-04-make-wordpress-fly/</link><pubDate>Mon, 17 Feb 2020 20:35:53 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/mariadb-10-1-setup-for-ubuntu-14-04-make-wordpress-fly/</guid><description>In this tutorial we will cover optimal MariaDB 10.1 setup for Ubuntu 14.04 on a VM with 2-4GB of RAM. This is part 2 of the &amp;ldquo;Make WordPress Fly&amp;rdquo; tutorial. You can find part 1 here. Part 1 covered the benefits of using HHVM, MariaDB, Nginx and Ubuntu 14.04 to run a WordPress website. In this section we&amp;rsquo;ll be digging in to MariaDB and the optimal configurations for it. This tutorial assumes you have a VM with at least 512MB of RAM, 1 Xeon Core, 10 GB HDD and Vanilla Ubuntu 14.</description><content:encoded><![CDATA[In this tutorial we will cover optimal MariaDB 10.1 setup for Ubuntu 14.04 on a VM with 2-4GB of RAM. This is part 2 of the &amp;ldquo;Make WordPress Fly&amp;rdquo; tutorial. You can find part 1 here. Part 1 covered the benefits of using HHVM, MariaDB, Nginx and Ubuntu 14.04 to run a WordPress website. In this section we&amp;rsquo;ll be digging in to MariaDB and the optimal configurations for it. This tutorial assumes you have a VM with at least 512MB of RAM, 1 Xeon Core, 10 GB HDD and Vanilla Ubuntu 14.04 installed and ideally secured. So, assuming you have your Ubuntu VPS all setup, we will proceed with the fairly straightforward process of installing MariaDB on Ubuntu 14.04. We are specifically going to be deploying MariaDB 10.1 which as discussed in part 1 has significant performance benefits over even the newest versions on MySQL. First, connect to your VM via SSH.
ssh -p port user@you.rip.add.res
Then we&amp;rsquo;ll add the MariaDB 10.1 repository and install the prerequisites.
sudo apt-get install software-properties-common sudo apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xcbcb082a1bb943db sudo add-apt-repository &#39;deb http://ftp.osuosl.org/pub/mariadb/repo/10.1/ubuntu trusty main&#39; Once the key is imported and the repository added we will install MariaDB.
sudo apt-get update sudo apt-get install mariadb-server During that process you will be prompted to create a root password for MariaDB. Make sure that you store it in a safe place. Consider using KeePass (or a similar utility) for test passwords, it creates strong passwords you can review later and encrypts them with a master key. Now that MariaDB is installed we need to make sure it runs on startup.
sudo update-rc.d mysql defaults Then, run the sudo mysql_secure_installation. This will guide you through some procedures that will remove some defaults which are dangerous to use in a production environment.
Next we will want to check that everything looks good in the my.cnf file.
nano /etc/mysql/my.cnf It looks like this, yours should be similar, it may be a bit different as MariaDB does some system based configuration on installation.
socket	= /var/run/mysqld/mysqld.sock nice	= 0 [mysqld] # # * Basic Settings # user	= mysql pid-file	= /var/run/mysqld/mysqld.pid socket	= /var/run/mysqld/mysqld.sock port	= 3306 basedir	= /usr datadir	= /var/lib/mysql tmpdir	= /tmp lc_messages_dir	= /usr/share/mysql lc_messages	= en_US skip-external-locking # # Instead of skip-networking the default is now to listen only on # localhost which is more compatible and is not less secure. bind-address	= 127.0.0.1 # # * Fine Tuning # max_connections	= 100 connect_timeout	= 5 wait_timeout	= 600 max_allowed_packet	= 16M thread_cache_size = 128 sort_buffer_size	= 4M bulk_insert_buffer_size	= 16M tmp_table_size	= 32M max_heap_table_size	= 32M # # * MyISAM # # This replaces the startup script and checks MyISAM tables if needed # the first time they are touched. On error, make copy and try a repair. myisam_recover = BACKUP key_buffer_size	= 128M #open-files-limit	= 2000 table_open_cache	= 400 myisam_sort_buffer_size	= 512M concurrent_insert	= 2 read_buffer_size	= 2M read_rnd_buffer_size	= 1M # # * Query Cache Configuration # # Cache only tiny result sets, so we can fit more in the query cache. query_cache_limit	= 128K query_cache_size	= 64M # for more write intensive setups, set to DEMAND or OFF #query_cache_type	= DEMAND # # * Logging and Replication # # Both location gets rotated by the cronjob. # Be aware that this log type is a performance killer. # As of 5.1 you can enable the log at runtime! #general_log_file = /var/log/mysql/mysql.log #general_log = 1 # # Error logging goes to syslog due to /etc/mysql/conf.d/mysqld_safe_syslog.cnf. # # we do want to know about network errors and such log_warnings	= 2 # # Enable the slow query log to see queries with especially long duration #slow_query_log[={0|1}] slow_query_log_file	= /var/log/mysql/mariadb-slow.log long_query_time = 10 #log_slow_rate_limit	= 1000 log_slow_verbosity	= query_plan #log-queries-not-using-indexes #log_slow_admin_statements # # The following can be used as easy to replay backup logs or for replication. # note: if you are setting up a replication slave, see README.Debian about # other settings you may need to change. #server-id	= 1 #report_host	= master1 #auto_increment_increment = 2 #auto_increment_offset	= 1 log_bin	= /var/log/mysql/mariadb-bin log_bin_index	= /var/log/mysql/mariadb-bin.index # not fab for performance, but safer #sync_binlog	= 1 expire_logs_days	= 10 max_binlog_size = 100M # slaves #relay_log	= /var/log/mysql/relay-bin #relay_log_index	= /var/log/mysql/relay-bin.index #relay_log_info_file	= /var/log/mysql/relay-bin.info #log_slave_updates #read_only # # If applications support it, this stricter sql_mode prevents some # mistakes like inserting invalid dates etc. #sql_mode	= NO_ENGINE_SUBSTITUTION,TRADITIONAL # # * InnoDB # # InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/. # Read the manual for more InnoDB related options. There are many! default_storage_engine	= InnoDB # you can&amp;#39;t just change log file size, requires special procedure #innodb_log_file_size	= 50M innodb_buffer_pool_size	= 256M innodb_log_buffer_size	= 8M innodb_file_per_table	= 1 innodb_open_files	= 400 innodb_io_capacity	= 400 innodb_flush_method	= O_DIRECT # # * Security Features # # Read the manual, too, if you want chroot! # chroot = /var/lib/mysql/ # # For generating SSL certificates I recommend the OpenSSL GUI &amp;#34;tinyca&amp;#34;. # # ssl-ca=/etc/mysql/cacert.pem # ssl-cert=/etc/mysql/server-cert.pem # ssl-key=/etc/mysql/server-key.pem [mysqldump] quick quote-names max_allowed_packet	= 16M [mysql] #no-auto-rehash	# faster start of mysql but no tab completition [isamchk] key_buffer	= 16M # # * IMPORTANT: Additional settings that can override those from this file! # The files must end with &amp;#39;.cnf&amp;#39;, otherwise they&amp;#39;ll be ignored. # !includedir /etc/mysql/conf.d/&amp;lt;/pre&amp;gt; Performance can be tweaked a bit once we&amp;rsquo;ve had the WordPress site up and running for 24-48 hours by using mysqltuner.pl. For good measure restart the service.
sudo service mysql restart
This concludes part 2 of the guide &amp;ldquo;MariaDB Setup for Ubuntu 14.04 - Make WordPress Fly&amp;rdquo;. As the rest of the guide is released links will be posted here and on all of the articles in the tutorial.
]]></content:encoded></item><item><title>HHVM, MariaDB and Nginx Make WordPress Fly - Intro</title><link>https://bryanapperson.com/blog/hhvm-mariadb-and-nginx-make-wordpress-fly-intro/</link><pubDate>Mon, 17 Feb 2020 20:17:30 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/hhvm-mariadb-and-nginx-make-wordpress-fly-intro/</guid><description>HHVM, MariaDB and Nginx Make WordPress fly (seriously). This site is running on what may the fastest possible software stack for WordPress. That stack is HHVM, MariaDB 10.1, Nginx and Ubuntu 14.04. As you are browsing this site you may notice that it is graphically intensive. It also leverages many CPU hungry plugins that would make it take 6-10 seconds to load on even good shared hosting. With this aforementioned software stack pages up to 5MB on this site still load in under a second, end-user pipe permitting.</description><content:encoded><![CDATA[HHVM, MariaDB and Nginx Make WordPress fly (seriously). This site is running on what may the fastest possible software stack for WordPress. That stack is HHVM, MariaDB 10.1, Nginx and Ubuntu 14.04. As you are browsing this site you may notice that it is graphically intensive. It also leverages many CPU hungry plugins that would make it take 6-10 seconds to load on even good shared hosting. With this aforementioned software stack pages up to 5MB on this site still load in under a second, end-user pipe permitting. This is all happening on a VPS with 2 x 2.26Ghz cores and 2GB of RAM. Not only that but this stack can serve over 1000 2MB WordPress pages per second without losing stability:
AB BenchMark [user@host ~]# ab -c 50 -n 5000 http://bryanapperson.com/ This is ApacheBench, Version 2.3 &amp;lt;$Revision: 655654 $&amp;gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking bryanapperson.com (be patient) Completed 500 requests Completed 1000 requests Completed 1500 requests Completed 2000 requests Completed 2500 requests Completed 3000 requests Completed 3500 requests Completed 4000 requests Completed 4500 requests Completed 5000 requests Finished 5000 requests Server Software: nginx Server Hostname: bryanapperson.com Server Port: 80 Document Path: / Document Length: 16138 bytes Concurrency Level: 50 Time taken for tests: 3.916 seconds Complete requests: 5000 Failed requests: 0 Write errors: 0 Total transferred: 83046606 bytes HTML transferred: 80706138 bytes Requests per second: 1276.68 \[#/sec\](mean) Time per request: 39.164 \[ms\](mean) Time per request: 0.783 \[ms\] (mean, across all concurrent requests) Transfer rate: 20707.77 \[Kbytes/sec\] received Connection Times (ms) min mean\[&#43;/-sd\] median max Connect: 1 1 0.2 1 5 Processing: 12 38 8.1 37 88 Waiting: 11 37 8.1 36 87 Total: 14 39 8.1 38 89 Percentage of the requests served within a certain time (ms) 50% 38 66% 41 75% 43 80% 44 90% 49 95% 54 98% 60 99% 66 100% 89 (longest request) Why HHVM for WordPress? So you may be asking yourself, is that really possible? Yes, HHVM and WordPress work very well together. If you asked me a few days ago I might have said no. But after playing around with HHVM, also known as &amp;ldquo;Hip Hop for PHP&amp;rdquo;, it is. HHVM is Facebook&amp;rsquo;s production PHP server which has now gone open source. At this point it still has a few compatibility issues. Especially with the usual culprits like Ioncube. However it works very well with WordPress 3.9&#43;. When combined with Nginx, MariaDB and Ubuntu &amp;ldquo;Trust Tahr&amp;rdquo; you get a pretty unbeatable platform for WordPress. Serving 200 request per second even on un-cached and heavy pages where PHP-FPM can only achieve 18 requests per second on a VM with the same resources (rendering the same un-cached pages).
MariaDB 10.1 MariaDB provides a solid database back-end and can easily be scaled out into a Galera Cluster for larger deployments. MariaDB 10.1 outperforms MySQL 5.7.4 by a significant margin, that is why it was chosen for this stack and it proved itself in implementation. MariaDB would perform better on SSD if available, but the above results were achieved on RAID10 7200RPM SATAIII with an LSI Megaraid BBU controller (512MB Cache).
Nginx Nginx can be somewhat less intuitive to configure then Apache. However it is a beast for serving static files especially per resource usage when configured correctly. Which is mostly what it does in this stack as all PHP processing is done by HHVM. Nginx really shines in serving static files to many user concurrently with the configuration we&amp;rsquo;ll outline in the coming articles.
Ubuntu 14.04 &amp;ldquo;Trusty Tahr&amp;rdquo; Choosing Ubuntu 14.04 for this deployment made sense, because it is LTS (5 years of support) and apt-get makes it almost trivial to get all of this setup. Not to mention that Ubuntu is a stable OS (although I usually prefer CentOS/RHEL). Nginx is built into the native repos for Ubuntu 14.04 and having maintained repos for both HHVM and MariaDB with Ubuntu 14.04 makes this stack easy to update later on. If you need an Ubuntu VPS you can get one for here. Before you get started with this you will probably want to secure your Ubuntu VPS.
WordPress 3.9.2 This series of articles will show you how to set all of this up and make it work with both a WordPress multi-site network, and a single WordPress site. I used the multi-site network with WP MU Domain Mapper and Nginx helper for ease of moving my multiple blogs and family/friends WordPress sites on to one platform. We will also be leveraging W3 Total Cache and APC (which is built in to HHVM) for Opcode caching.
Concluding the Introduction This setup is so efficient you wouldn&amp;rsquo;t need to scale out past a single VM instance unless you were in the Alexa top 10000, so we won&amp;rsquo;t handle that in this series. In articles to follow I will layout how to build this stack and use it for Lightning fast WordPress hosting on a shoestring budget. You&amp;rsquo;ll be able to handle 50,000 page loads an hour or more on a 2GB RAM Xen VM. I will update this article with links to the upcoming tutorials.
]]></content:encoded></item><item><title>Getting Started with an Ubuntu VPS Running 14.04</title><link>https://bryanapperson.com/blog/getting-started-with-an-ubuntu-vps-running-14-04/</link><pubDate>Mon, 17 Feb 2020 19:27:00 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/getting-started-with-an-ubuntu-vps-running-14-04/</guid><description>When you get a new Ubuntu VPS or server there are a few things you want to make sure are taken care of right off the bat. This will optimize the security and usability of your server, providing a reliable foundation for subsequent alterations. If you need an Ubuntu VPS you can get one for .
Step One Logging into your new VPS is the first step. You&amp;rsquo;ll need to know the public IP address of the server to begin.</description><content:encoded><![CDATA[When you get a new Ubuntu VPS or server there are a few things you want to make sure are taken care of right off the bat. This will optimize the security and usability of your server, providing a reliable foundation for subsequent alterations. If you need an Ubuntu VPS you can get one for .
Step One Logging into your new VPS is the first step. You&amp;rsquo;ll need to know the public IP address of the server to begin. You&amp;rsquo;ll also need to know the password for the &amp;ldquo;root&amp;rdquo; user. Once you know those two things you are ready to login. The public IP address is e-mailed to you and you can view the credentials you chose in your client area if the VPS is hosted with Bitronic Technologies (my company). In Linux the root user an administrator with sweeping privileges. Because the root user has the power to irreversibly damage a system you shouldn&amp;rsquo;t use root for daily use. It is best to login as a user with limited privileges and then to &amp;ldquo;sudo&amp;rdquo; root when you need to in the case of Ubuntu 14.04. In this tutorial we will cover how to login as root, create an alternate user for day-to-day use, disable root login and how to login as an alternate (non-root) user, then escalate to root. As mentioned, logging in is the first step to getting started with your Ubuntu 14.04 VPS. Initially the only account is the root account. If you are using Linux connect to the server using the &amp;ldquo;ssh&amp;rdquo; command at terminal. If you are using windows consider using an SSH program like putty. For purposes of this article we will assume you are running Linux on your PC, however all of the same steps will apply either way except for initial login. If you are using Windows you will need to open putty, enter the root@pub.lic.ipa.ddr (the dotted quad public IP of your VPS) then login and enter the password. On linux just run this command in terminal:
ssh root@pub.lic.ipa.ddr
Of course entering your public IP address in place of those letters. You will then most likely see a warning in your terminal that looks like this:
established. ECDSA key fingerprint is 79:95:46:1a:ab:37:11:8e:86:54:36:38:bb:3c:fa:c0. Are you sure you want to continue connecting (yes/no)? Your computer is letting you know that it does not recognize your remote server. This is expected behaviour because it is the first time this connection is being made. However if you see this message in the future (on the same computer) it can be a sign that your server has been compromised (assuming you haven&amp;rsquo;t reinstalled the operating system). It is safe to type &amp;ldquo;yes&amp;rdquo; and accept the connection. Then you will need to enter the password for the root account.
Step Two You may wish to change the root password to something more memorable. Make sure to use a strong password as root login is the &amp;ldquo;key to the kingdom&amp;rdquo; so to speak. A good password can be a sentence like &amp;ldquo;IlikedtheFordBroncoin1996!&amp;rdquo; or a random string. But it should ideally contain uppercase/lowercase letters, numbers and at least 1 special character. While logged in as root it is easy to change the password by typing:
passwd
You will be prompted to enter and then confirm your new root password. You will not see any characters display on the screen while you are typing (this is an intentional security measure).
Step Three The third step in preparing and securing your new Ubuntu 14.04 VPS is to add a new user. This is going to be the account you&amp;rsquo;ll use to login after the end of the tutorial, so make sure the username is memorable. For the purposes of this tutorial we are going to make the username &amp;ldquo;demo&amp;rdquo;, but you should select a more meaningful username. So to add the user just run this command:
adduser demo
You will then be prompted with a few questions, starting with the password you want to use for the account. It is recommended that you choose a different password then the one you used for the root account as an added security measure. So go ahead and fill out the password, then optionally provide the other details requested. If you don&amp;rsquo;t want to fill those out you can just hit enter and Ubuntu will skip those fields.
Step Four In step four we will provide the new user we just created in step three with the ability to escalate to root. This allows you to gain root access without having to allow remote root login, or logout and log back in as root if you do choose to leave root login over SSH enable (not recommended). In Ubuntu we use the command &amp;ldquo;sudo&amp;rdquo; to escalate to root. This allows a normal user to run a command that requires root privileges. You can also use the &amp;ldquo;sudo -i&amp;rdquo; command to assume the identity of root until you use the command &amp;ldquo;exit&amp;rdquo;. To add the new user we just created to the list of users who can use &amp;ldquo;sudo&amp;rdquo; to attain root, we need to use a command called &amp;ldquo;visudo&amp;rdquo;. This command will open a configuration file. Go ahead and type:
visudo
Then scroll down until you find a section that looks similar to this:
# User privilege specification root ALL=(ALL:ALL) ALL This may look complex to you but that line is non-consequential and we don&amp;rsquo;t need to alter it. Just add another line below it that follows the format, replacing &amp;ldquo;demo&amp;rdquo; with the username you created in step three. The result should look like this:
# User privilege specification root ALL=(ALL:ALL) ALL demo ALL=(ALL:ALL) ALL After you have made the changes, press CTRL-X to exit visudo. You will have to type &amp;ldquo;Y&amp;rdquo; to save the file and &amp;ldquo;ENTER&amp;rdquo; to confirm the location (you can use the provided value).
Step Five (Optional) This step is optional but is highly recommended for optimal security going forward. You can secure your server a bit more by disabling root login over SSH entirely and changing SSH to a non default port. The process is fairly straightforward. Get started by opening the sshd configuration file with a text editor as root. You can use any text editor installed on your VPS but for this tutorial we will assume you use &amp;ldquo;nano&amp;rdquo;. Nano is a fairly easy to use command line text editor for Linux. So go ahead and type this command:
nano /etc/ssh/sshd_config
Step Five-A (Optional) The first thing to do is to change the port that SSHD listens on. Find the line that looks like this:
Port 22
Change this number to something between 1025 and 65536, make sure it does not conflict with the port of another program you plan on running. This is helpful in preventing unauthorized users from trying to break into the system over SSH. If you change from the default port it adds an extra step of sniffing for them to find it and you can even add a firewall rule to block IPs for port scanning of use a tool like ConfigServer Firewall to do so. If you do change this value make sure you remember it or write it down. You will need to know this port to reconnect to your server. For the purposes of this guide we will change the port to 3333. Remember you will need to tell your ssh client to connect to port 3333 (or the port you choose) in the future. So change the line to look like this (or the port you choose):
Port 3333
Save the file using &amp;ldquo;CTRL-O&amp;rdquo;, &amp;ldquo;Y&amp;rdquo; then exit &amp;ldquo;CTRL-X&amp;rdquo;. Now we are going to enable Ubuntu&amp;rsquo;s UFW (Uncomplicated Firewall). Enter this command:
sudo ufw enable
Then we are going to make sure that port 3333 or whatever port you chose is open. Run the command:
sudo ufw allow 3333
Replace 3333 with whatever port you decided to use then run:
sudo service ufw restart sudo service ssh restart Now you can reconnect via the port you chose in the future.
Step Five-B (Optional) The next thing we can do for added security is to restrict remote root login over SSH. Run the command:
nano /etc/ssh/sshd_config
Then find the line that looks like this:
PermitRootLogin yes
Change this to:
PermitRootLogin no
This is a much more secure setting as it discourages the direct brute force or the root password. This is especially effective if you use a different password for your &amp;ldquo;day-to-day&amp;rdquo; user then you do for root. If you run:
sudo service ssh restart
You will no longer be able to login as root, only as the alternate user you created.
Step Five-C (Optional) If you wanted to go one step further you could explicitly specify which users can connect to your server via SSH. This might not be needed now, but can become useful as you add additional users. Any user not on the list you are about to configure will not be able to login over SSH at all. You should use caution when configuring this as you can lock your self out of your server entirely. We are going to need to open the SSHD configuration again:
nano /etc/ssh/sshd_config
You&amp;rsquo;ll have to add this line yourself as it does not exist by default. Make sure to replace &amp;ldquo;demo&amp;rdquo; with the username you created in step 3.
AllowUsers demo
When you have added the line and triple checked that the syntax and username are correct, save and close the file using &amp;ldquo;CTRL-O&amp;rdquo;, &amp;ldquo;Y&amp;rdquo; and then &amp;ldquo;CTRL-X&amp;rdquo; as we covered earlier. Then go ahead and restart SSHD again:
sudo service ssh restart
Before you log out of the server, test your configuration by opening another connection using the user, password and port that you have created in this guide. If you can&amp;rsquo;t connect go back and correct your errors using the original connection (which you should not close until you are sure that your settings work). If you followed this guide the command you would use is:
ssh -p 3333 demo@ser.ver.ipa.ddr
Replacing the port and user with the ones you chose. Then entering the password for the user you created in step 3. Remember from now on you will have to use the sudo command to run commands that need root:
sudo your_command
You can now close both connections by typing exit in each window.
What&amp;rsquo;s Next? You now have a fairly secure server, however you can continue further securing you server by installing fail2ban or a similar utility to help prevent brute force and port scanning attacks. Outside of that you are ready to install LAMP, LEMP or whatever other programs you want to run on your VPS! Leave me your thoughts in the comments below and check out my other Ubuntu VPS articles.
]]></content:encoded></item><item><title>From WordPress to Hugo</title><link>https://bryanapperson.com/blog/from-wordpress-to-hugo/</link><pubDate>Mon, 17 Feb 2020 00:00:00 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/from-wordpress-to-hugo/</guid><description>Hugo static blogs are awesome Migrating to Hugo It is 2019, enter the brave new world of serverless computing and static blogging. After years of using WordPress, and a few years without any updates this blog is in need of a refresh. When I originally created this blog I had servers (a whole web hosting company in fact). Now I just want to be able to create content and have a lightning fast website that is easy to maintain.</description><content:encoded><![CDATA[    Hugo static blogs are awesome    Migrating to Hugo It is 2019, enter the brave new world of serverless computing and static blogging. After years of using WordPress, and a few years without any updates this blog is in need of a refresh. When I originally created this blog I had servers (a whole web hosting company in fact). Now I just want to be able to create content and have a lightning fast website that is easy to maintain.
Earlier today I was doing some research, came across Hugo and decided to rebuild this blog using it and host it using github pages, at least initially. This site will likely be a work in progress for some time, however at least the text of the old content is migrated now. As I do this migration I will put out a series of articles on the how and the why of the migration.
Why Hugo and the Coder theme? Hugo is lightning fast at rendering pages and simple to use, plus the workflow with github pages is straightforward. The coder theme was chosed for now because I like the aesthetics.
Migrating from Wordpress I wanted a pretty fresh start, so I just used WordPress to export XML and then found a script here. Then I modified it for python 3 as follows:
#!/usr/bin/env python3 &amp;#34;&amp;#34;&amp;#34; This script is used to convert a WordPress XML dump to Hugo-formatted posts. NOTE: The WP post data is kept as-is (probably HTML). It is not converted to Markdown. This is to reduce the amount of &amp;#34;fixing&amp;#34; one has to do after the data is converted (e.g. line endings, links, etc). This is generally not an issue since Markdown allows HTML. The post Metadata is converted to TOML. The posts are written as: &amp;lt;year&amp;gt;/&amp;lt;title&amp;gt;.md where &amp;lt;year&amp;gt; is the year the post was written, and &amp;lt;title&amp;gt; is the WP title with all non-word characters replaced with &amp;#34;-&amp;#34;, and converted to lower case. &amp;#34;&amp;#34;&amp;#34; # Imports ###################################################################### import os import re import maya import time import calendar import xml.etree.ElementTree as ET from distutils.version import LooseVersion # Metadata ##################################################################### __author__ = &amp;#34;Timothy McFadden&amp;#34; __creationDate__ = &amp;#34;07/24/2015&amp;#34; __license__ = &amp;#34;MIT&amp;#34; __version__ = &amp;#34;1.0.0dev&amp;#34; # Globals ###################################################################### DEBUG = False KNOWN_WP_VERSION = LooseVersion(&amp;#34;4.2&amp;#34;) def hugo_format(data): result = [&amp;#34;&#43;&#43;&#43;&amp;#34;] for heading in [&amp;#34;title&amp;#34;, &amp;#34;date&amp;#34;, &amp;#34;type&amp;#34;]: result.append(&amp;#39;%s= &amp;#34;%s&amp;#34;&amp;#39; % (heading, data[heading])) result.append(&amp;#34;tags = %s&amp;#34; % str(data[&amp;#34;tags&amp;#34;])) result.append(&amp;#34;&#43;&#43;&#43;&amp;#34;) result.append(&amp;#34;&amp;#34;) result.append(data[&amp;#34;body&amp;#34;]) return &amp;#34;\n&amp;#34;.join(result) def wp_version_check(channel): match = re.search(&amp;#34;\?v=([\d\.]&#43;)&amp;#34;, channel.find(&amp;#34;generator&amp;#34;).text) if not match: print(&amp;#34;WARNING: Could not find WP version in your XML.&amp;#34;) print(&amp;#34;...This script may not work&amp;#34;) raw_input(&amp;#34;...press Enter to continue: &amp;#34;) else: wp_version = LooseVersion(match.group(1)) if wp_version &amp;lt; KNOWN_WP_VERSION: print(&amp;#34;WARNING: WP version in your XML (%s) is less than known good version (%s)!&amp;#34; % (wp_version, KNOWN_WP_VERSION)) print(&amp;#34;...This script may not work&amp;#34;) raw_input(&amp;#34;...press Enter to continue: &amp;#34;) def convert_wp_xml(xml_path): tree = ET.parse(xml_path) # FYI: xml.etree doesn&amp;#39;t support reading the namespaces, and I don&amp;#39;t feel # like requiring lxml. nsmap = { &amp;#34;excerpt&amp;#34;: &amp;#34;http://wordpress.org/export/1.2/excerpt/&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;http://purl.org/rss/1.0/modules/content/&amp;#34;, &amp;#34;wfw&amp;#34;: &amp;#34;http://wellformedweb.org/CommentAPI/&amp;#34;, &amp;#34;dc&amp;#34;: &amp;#34;http://purl.org/dc/elements/1.1/&amp;#34;, &amp;#34;wp&amp;#34;: &amp;#34;http://wordpress.org/export/1.2/&amp;#34;, } channel = tree.find(&amp;#34;channel&amp;#34;) wp_version_check(channel) for item in channel.findall(&amp;#34;item&amp;#34;): data = { &amp;#34;tags&amp;#34;: [], &amp;#34;title&amp;#34;: (item.find(&amp;#34;title&amp;#34;).text).strip(&amp;#39;\n&amp;#39;), &amp;#34;date&amp;#34;: None, &amp;#34;body&amp;#34;: None, &amp;#34;fpath&amp;#34;: None, &amp;#34;type&amp;#34;: &amp;#34;post&amp;#34; } scraped_time = item.find(&amp;#34;pubDate&amp;#34;).text datetime = maya.parse(scraped_time).datetime(to_timezone=&amp;#39;US/Eastern&amp;#39;, naive=True) data[&amp;#34;date&amp;#34;] = datetime data[&amp;#34;tags&amp;#34;] = [x.attrib[&amp;#34;nicename&amp;#34;] for x in item.findall(&amp;#34;category&amp;#34;)] data[&amp;#34;body&amp;#34;] = item.find(&amp;#34;content:encoded&amp;#34;, nsmap).text fname = re.sub(&amp;#34;\W&#43;&amp;#34;, &amp;#34;-&amp;#34;, data[&amp;#34;title&amp;#34;]) fname = re.sub(&amp;#34;(-&#43;)$&amp;#34;, &amp;#34;&amp;#34;, fname) fname = fname[1:] data[&amp;#34;fname&amp;#34;] = &amp;#34;{}.md&amp;#34;.format(fname.lower()) data[&amp;#34;fdir&amp;#34;] = os.path.abspath(os.path.join(&amp;#34;.&amp;#34;, str(datetime.year))) data[&amp;#34;fpath&amp;#34;] = os.path.join(data[&amp;#34;fdir&amp;#34;], data[&amp;#34;fname&amp;#34;]) hugo_text = hugo_format(data) if not os.path.isdir(data[&amp;#34;fdir&amp;#34;]): os.makedirs(data[&amp;#34;fdir&amp;#34;]) with open(data[&amp;#34;fpath&amp;#34;], &amp;#34;wb&amp;#34;) as fh: fh.write(hugo_text.encode(&amp;#39;UTF-8&amp;#39;)) print(&amp;#34;Created: {}/{}&amp;#34;.format(datetime.year, data[&amp;#34;fname&amp;#34;])) if __name__ == &amp;#39;__main__&amp;#39;: import sys if len(sys.argv) == 1: print(&amp;#34;Usage: python wp_to_hugo.py &amp;lt;wordpress XML file&amp;gt;&amp;#34;) sys.exit(1) convert_wp_xml(sys.argv[1]) I still need to manually sift through and fix images and curate the older posts have now updated all of the older posts.
Github Pages I chose github pages because I already have a github account and the workflow seemed good. Plus there is an easy way I plan to automate publishing master using travis-ci in the near future have automated publishing. I followed the Hugo docs, using the gh-pages branch based workflow. The repo for this blog now lives here.
Travis CI I adapted this guide on Travis CI and Hugo to automate the build and publish of my blog to github pages. It is pretty straightforward and I highly recommend it.
Here is the .travis.yml that I ended up with after optimizing a bit for faster builds of Hugo:
# Credit to:#https://axdlog.com/2018/using-hugo-and-travis-ci-to-deploy-blog-to-github-pages-automatically/# https://docs.travis-ci.com/user/deployment/pages/# https://docs.travis-ci.com/user/reference/xenial/# https://docs.travis-ci.com/user/languages/go/# https://docs.travis-ci.com/user/customizing-the-build/cache:directories:- $HOME/.cache/go-build- $HOME/gopath/pkg/moddist:xeniallanguage:gogo:- 1.12.x# Only clone the most recent commit.git:depth:1# before_install# install - install any dependencies requiredinstall:- go get github.com/gohugoio/hugobefore_script:- rm -rf public 2&amp;gt; /dev/null# script - run the build scriptscript:- hugodeploy:provider:pagesskip-cleanup:truegithub-token:$GITHUB_TOKEN# Set in travis-ci.org dashboard, marked secureemail:$GITHUB_EMAILname:$GITHUB_USERNAMEverbose:truelocal-dir:publicfqdn:bryanapperson.comon:branch:master# branch contains Hugo generator codeConclusion Overall the initial learning curve, setup, customization and migration took about 8 hours. I would say that is pretty good. There will definitely be more posts to follow on my journey with blogging using Hugo.
]]></content:encoded></item><item><title>Astable Multivibrator from Discrete Transistors</title><link>https://bryanapperson.com/blog/astable-multivibrator-from-discrete-transistors/</link><pubDate>Sun, 24 Jan 2016 00:00:00 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/astable-multivibrator-from-discrete-transistors/</guid><description>This blog post is about making an astable multivibrator from discrete transistors. This weekend I embarked on making a home-brew computer from discrete transistors. To test circuits like a JK flip-flop or SRAM, a clock is needed. In the spirit of keeping with an all discrete transistor computer, I used an astable multivibrator composed of two RTL NAND gates. I wanted to start with a very low frequency, so I targeted 10 seconds high, 10 low for the clock.</description><content:encoded>This blog post is about making an astable multivibrator from discrete transistors. This weekend I embarked on making a home-brew computer from discrete transistors. To test circuits like a JK flip-flop or SRAM, a clock is needed. In the spirit of keeping with an all discrete transistor computer, I used an astable multivibrator composed of two RTL NAND gates. I wanted to start with a very low frequency, so I targeted 10 seconds high, 10 low for the clock. Changing the size of the capacitor and resistor outside the NAND gates will adjust the frequency. The formula for the frequency of the clock works out to be something like this: $:f=\frac{1}{t}=\frac{1}{2RC}=\frac{1}{2 \times 4700\Omega \times 2200uF}=20.68s$.
Designing the Astable Multivibrator I am new to low-level electronics like this, but I had used Arduino in the past and the designers of that micro-controller advocate Fritzing. I used Fritzing to design the schematic, breadboard and PCB. The first step was to design the schematic. I used and adaptation of a circuit I found here.
The next step was to breadboard the circuit. The cover image of this blog post shows the real life version, however I was able to breadboard it in Fritzing to validate the circuit.
After that I went ahead and breadboarded/tested the circuit. Everything worked as expected after making blue smoke out of an LED that was shorted.
The final step for having a drop in circuit for a test clock was to design a PCB. I made considerations for being able to change R8 and C1 to change the frequency to something like .5hz for use in testing other components as I go down the homebrew computer road. That was convenient because I was able to fabricate the PCB from the program directly. I ordered a fabrication of the PCB from Fritzing Fab.
In about two weeks the PCB will be here from Germany and the parts will go on the board. The Fritzing file for this circuit can be downloaded here. Hopefully this article is helpful to anybody looking to make a clock circuit from discrete transistors using resistor-transistor logic NAND gates. Let me know your thoughts in the comments.</content:encoded></item><item><title>Copy Ceph Pool Objects to Another Pool</title><link>https://bryanapperson.com/blog/copy-ceph-pool-objects-to-another-pool/</link><pubDate>Tue, 17 Nov 2015 23:06:18 UT</pubDate><dc:creator>Bryan Apperson</dc:creator><guid>https://bryanapperson.com/blog/copy-ceph-pool-objects-to-another-pool/</guid><description>Sometimes it is necessary to copy Ceph pool objects from one Ceph pool to another - such as when changing CRUSH/erasure rule sets on an expanding cluster. There is a built-in command in RADOS for doing this. However the command in question, rados cppool , has some limitations. It only seems to work with replicated target pools. Thus it cannot copy Ceph pool objects from a erasure pool to a replicated pool, or between erasure pools.</description><content:encoded><![CDATA[Sometimes it is necessary to copy Ceph pool objects from one Ceph pool to another - such as when changing CRUSH/erasure rule sets on an expanding cluster. There is a built-in command in RADOS for doing this. However the command in question, rados cppool , has some limitations. It only seems to work with replicated target pools. Thus it cannot copy Ceph pool objects from a erasure pool to a replicated pool, or between erasure pools. So to offer a utility for copying the contents of an erasure coded pool to another erasure pool, this evening I wrote up a function in my python-rados-utils repository. To use the python-rados-utils package, you first have to build and install it. At this point the repository only works with RHEL/CentOS/Fedora due to the RPM based build system. You can however look through the code for usage on other platforms. It&amp;rsquo;s pretty easy to get python-rados-utils up and running. Building from python-rados-utils from source:
git clone git@github.com:bryanapperson/python-rados-utils.git cd python-rados-utils rpmbuild -ba python-rados-utils.spec Installing python-rados-utils: The rpm from the build we just did will be output in ~/rpmbuild/RPMS/noarch/. Install the rpm using: rpm -Uvh &amp;lt;path-to-rpm&amp;gt; Once the python-rados-utils package is installed, using it to copy all objects from one Ceph pool to another is very straight-forward. In your favorite text editor, open up a file called copy_objects.py . In this file place:
# Optionally you can pass in the keyring and ceph.conf # locations as strings. thiscluster = common_utils.Cluster() # Replace these empty stings with your source and target pool names source = &amp;#39;&amp;#39; target = &amp;#39;&amp;#39; thiscluster.copy_pool(source, target) NOTE: Updated versions of the above snippet can be found here. This script is single threaded at the moment and synchronous. I may add asynchronous and multi-threading functionalities to speed up Ceph pool copy in the near future. This code comes with no warranty of any kind and the code is licensed under GPLv2. Test in your own environment, but for me this worked well to copy all objects in one pool to another. Please leave your thoughts in the comments below and commit back any cool stuff to the python-rados-utils repository.
]]></content:encoded></item></channel></rss>