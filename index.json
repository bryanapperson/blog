[{"categories":null,"contents":"Decentralized finance is changing the world of lending, borrowing, swapping assets, investing, insurance, and more. This is all happening in a trustless fashion via smart contracts, primarily on the Ethereum blockchain. When I realized this, perhaps a bit late, in January of 2021; I became borderline obsessed. For the first time ever, regular people can act as their own banks, lending out money, swapping assets directly with counterparties, providing or purchasing insurance coverage and other derivatives. That\u0026rsquo;s only the beginning. Art markets are emerging, corporate governance is happening on chain, real estate and auto markets are likely soon to follow. This is the internet in 1997. Fortunes are being made and destroyed daily in a wild west where emergent developments outpace the understanding of anybody without an ear to the ground 24/7.\nBitcoin is just an asset store, but DeFi has the potential to eat most of traditional centralized finance and insurance. What is stopping it, for now, as the title of this article belies, is network congestion caused by limiting architecture and frontrunning. Those problems are being solved, and are the only thing which, in my opinion, put a hamper on the parabolic growth of DeFi between August of 2020 and February of 2021.\nThe Characters of Decentralized Finance DeFi on Ethereum is composed of miners, searchers, builders, users and speculators. Miners validate the transactions and blocks which comprise the blockchain. Searchers act like DevOps for DeFi protocols providing arbitrage to automated market makers, liquidations to lending and borrowing protocols, as well as price oracle updates. Builders create the protocols, often riffing on other protocols and using them as building blocks for novel financial instruments. Users use protocols for many purposes outlined above. Speculators buy and hold various assets to speculate on future value.\nI approached DeFi, at first, as a user. I was intrigued by the idea of lending stablecoins out for APYs one thousand times greater than those offered by centralized finance institutions. What are stablecoins you ask? Stablecoins are crypto assets designed to have a stable value, sometimes with a hard or soft peg to a fiat currency, or physical commodity like gold. Then I became interesting in speculating on governance tokens. I acquired art on chain, bought insurance for the Compound protocol where I was lending out Dai, a soft pegged USD stablecoin, for interest. I ran some 465 transactions over the course of a few months, with gas fees totalling enough to buy a decent used car. What I figured out, primarily, is that DeFi is going places, and that users, builders, searchers and miners are creating a whole new ecosystem that is definitely worth getting involved with. Also, that price speculation and swinging in and out of crypto assets is for the birds.\nLearning about DeFi The best way to learn about DeFi is to join the community Discord chats, ask questions. Get a hardware wallet, learn how to use it. Setup Metamask, play around on a test network, so you don\u0026rsquo;t spend a used car in gas. This trustless world of DeFi is just getting started, and it\u0026rsquo;s a playground for developers. Builders and searchers are in demand, and the opportunity there for the technically inclined is, I believe, leviathan.\nMy Favorite Ethereum DeFi Protocols Uniswap, Compound, Nexus Mutual, Chainlink, The Graph; these are the projects which intrigue me most at present. Layer 2, the solution to the expense holding back the growth of DeFi, is expanding. Layer 2 with rapid and inexpensive connectivity between L2 protocols and better fiat on and off ramps will take DeFi mainstream in the next 2 years. Self custodial debit cards like Monolith will almost entirely obviate the need for bank accounts once combined with fiat payment systems like Venmo and Zelle.\nWhat Are We Waiting For? What are you going to build? Why aren\u0026rsquo;t you earning 35% on your USD pegged savings? These are the questions which become immediately apparent when you start to dig into DeFi. For now, the answer might be transaction fees, but it won\u0026rsquo;t be forever.\n","summary":"Decentralized finance is changing the world of lending, borrowing, swapping assets, investing, insurance, and more. This is all happening in a trustless fashion via smart contracts, primarily on the Ethereum blockchain. When I realized this, perhaps a bit late, in January of 2021; I became borderline obsessed. For the first time ever, regular people can act as their own banks, lending out money, swapping assets directly with counterparties, providing or purchasing insurance coverage and other derivatives.","tags":["ethereum","defi","finance","bankless"],"title":"DeFi on Ethereum, a Used Car in Gas Later","url":"/blog/defi-on-ethereum/"},{"categories":null,"contents":"Python is much loved by developers for its ease of development, its low time to minimum viable product and, some might say, its dynamic typing. I\u0026rsquo;ve been using dynamically typed Python for the better part of a decade now, for those same reasons. Dynamically typed Python is rapid to develop in, features amorphous blobs of data and works great for writing code quickly.\nGradual typing and static analysis split the difference between the reduced velocity of a strongly typed language and the likewise reduced velocity of maintaining code where the input and output data types are unclear. Gradually typed Python makes reasoning about larger, and older, codebases easier. When I started using types in Python, I was just sprinkling in type annotations here and there. The Python type hinting system can do much more.\nStatic Analysis of Gradually Typed Python Formatting tools like Black and isort, along with static analysis tools like Flake8 are great for keeping python code consistent in form and free of unused variables. But Python type hinting can do more. Mypy provides many of the compile time checks you\u0026rsquo;d get in a strongly typed, compiled language like Rust, which are useful when developing and add no overhead at runtime.\nCommand Line Interfaces via Typed Python Click is one of my favorite command line interface libraries. Recently I stumbled upon Typer, Typer is an abstraction on top of Click which uses type annotations and doctrings to generate beautiful command line interfaces with much less boilerplate code. Using Typer, a command line interface is easy to build in just a few lines.\nDe/Serialization and Validation with Types Pydantic, especially when combined with mypy is a sure fire way to get lightning fast data models with low cost of development, and lower probability of bugs for serialization, deserizaliztion, and by nature, data validation.\nConclusion Using the type hinting system has made my code better, made me a better developer, and allowed the sort of semantics used in languages like Rust in Python. I think it can do the same for the reader, and is definitely worth using. I hope to put out some more blogs in the near future on my Python toolbox and maybe even a sample app putting it all together.\n","summary":"Python is much loved by developers for its ease of development, its low time to minimum viable product and, some might say, its dynamic typing. I\u0026rsquo;ve been using dynamically typed Python for the better part of a decade now, for those same reasons. Dynamically typed Python is rapid to develop in, features amorphous blobs of data and works great for writing code quickly.\nGradual typing and static analysis split the difference between the reduced velocity of a strongly typed language and the likewise reduced velocity of maintaining code where the input and output data types are unclear.","tags":["order","mathematics","python"],"title":"A Gradual Journey to Typed Python","url":"/blog/gradually-typed-python/"},{"categories":null,"contents":"Studying The Elements of Euclid reveals the foundations of mathematical rigour and the systems through the lens of which we see the world today. Though the end result of the concepts covered in many of the books of the Elements will be familiar to those who have completed high school maths; seeing Euclid\u0026rsquo;s system come together to weave the fabric of maths and understanding the proofs from first principles is another matter entirely. When I started my, what will likely be decades long, study of the great books a few months ago, Euclid was on the list. I started study of Euclid by myself, but was fortunate enough to be able to engage in a seminar based study of book 1 with Online Great Books.\nApproaching rigorous, proof based mathematics from millennia past might seem like a daunting task to some. It actually could not be further from daunting; it is thrilling. When examined with wonder, discoveries abound. The study of spacial relation that is geometry is the most natural math for the human maid to apprehend, and is a unique type of gymnastics for the mind. Plato required mastery of geometry before entry the academy. It\u0026rsquo;s at the heart of the physical sciences, at the center of logic. This system remained unaltered for millennia, passing through the conflagration at Alexandria, and to this day still mostly still does.\nThe first step to a study of Euclid is to take a look over the proofs and draw them out on paper. Draw them by hand with nothing but a pencil, straight edge and dividers; use color! Use the previous proofs to produce the later ones, throw away your ruler and dive into the continuum without discrete measurement. The elegance is stunning; every proof is used, no thought is wasted. From a point, described as that which has no parts, Euclid\u0026rsquo;s system builds to geometries which can be used to describe the universe we inhabit, along with an entirely abstract continuum which exists only in the mind.\nEuclid, like most things, is best understood with fellow travelers. While solitary study will yield understanding, the eye of the other is the best tool with which to refine our apprehension. Using the format of Socratic seminars to study Euclid allows for joint inquiry, and for me helped illuminate the substance of Euclidean geometries as the soil that allowed modernity to bloom. Once you\u0026rsquo;ve drawn out the proofs and thought about them, it is amazing to see the variety approaches and connections other folks in seminar bring to the table.\nThrough the dialectic of the seminar, the group, along with the individuals it is composed of, emerge with an understanding entirely other than anybody arrived with. The Socratic seminar is the key to getting a deeper understanding, broader context and better a calibration of how to approach the material. So to sum it up, pencil, paper, dividers, straightedge, fellow travelers; put all this in the format of a Socratic seminar, and you\u0026rsquo;ve got the bones of an elucidating study of The Elements.\n","summary":"Studying The Elements of Euclid reveals the foundations of mathematical rigour and the systems through the lens of which we see the world today. Though the end result of the concepts covered in many of the books of the Elements will be familiar to those who have completed high school maths; seeing Euclid\u0026rsquo;s system come together to weave the fabric of maths and understanding the proofs from first principles is another matter entirely.","tags":["mind","space","order","mathematics"],"title":"A Socratic Study of Euclid's Elements","url":"/blog/socratic-study-of-euclid/"},{"categories":null,"contents":"As the first play of The Orestia opens, the Trojan war is ending. The watchman, straggling the Atreidae\u0026rsquo;s roof dogwise, sees the beacon signaling victory in Ilium. Agamemnon returns home with Cassandra, a prophetess and daughter of Priam, as a concubine. He has been gone for ten years, with the human sacrifice of his daughter Iphigenia as his farewell. Once home he is greeted by the duplicitous Clytaemestra. She conspires with her lover Aegisthus to kill Agamemnon and seize the reins of power. The plot thickens knowing, from what the chorus has told the audience, that Agamemnon\u0026rsquo;s father served Aegisthus' father, his brother, his own children for dinner. This play is the quintessential family blood feud.\nThe Watchman  I speak for those who understand, but if they fail, I have forgotten everything.\nWatchman, Agamemnon L36  The Watchman, weary from his long service upon spotting the beacon speaks from atop the house. His location creates the illusion of the house of Atreus, or the nobility, speaking directly to the Chorus. He bemoans the time he has been waiting and subtly hints at the misrule of \u0026ldquo;a lady\u0026rsquo;s male strength of heart\u0026rdquo; and because of it he feels moved to \u0026ldquo;weep again the pity of this house no longer, as once, administered in the grand way\u0026rdquo;. This subtlety indicates his dependence on the rulers. His monologue indicates that he is on the side of the people against the aristocracy, should they have success, in an attempt to straddle the good graces of both sides.\nThe Chorus  From high good fortune in the blood blossoms the quenchless agony.\nAntistrophe C, Agamemnon L755  The Chorus, who represent the people, play a passive role in Agamemnon. The represent the common man. They tell the audience the background story of the House of Atreus and it\u0026rsquo;s curse. They tell of the sacrifice of Iphigenia, Agamemnon\u0026rsquo;s daughter, at the outset of the Trojan war. They hint at a mistrust of Clytaemestra in the beginning. After the murder of Agamemnon they examine Clytaemestra and Aegisthus, the guilt of whom is apparent. They fault Aegisthus for the cowardly act of letting his lover kill his enemy in surprise, rather then facing him directly. Throughout the Chorus acts as an interrogative force and the conscience of the audience. Though the chorus sees the injustices, they are not empowered to act and allow the tyrants to \u0026ldquo;soil justice, while the power is [theirs]\u0026rdquo;.\nClytaemestra  By my child\u0026#39;s Justice driven to fulfillment, by her Wrath and Fury, to whom I sacrificed this man\nClytaemestra, Agamemnon L 1430    Clytaemestra is unfaithful, but can we fault her? Her husband murdered his own child for luck in a meaningless war. His nature and the brutal legacy of his father made it easy to recruit enemies against him. If he had not sacrificed their child in pursuit of a frivolous war, might she have remained true? Was she wrong to plot his demise? What suffering should a person accept from a spouse? Her revenge for her child may have been justified, but lead to the deposition of her son Orestes, long absent, and the utter betrayal of her household. Her primary crime in the eyes of the ancients may have been hinted at in his speech about the \u0026ldquo;lady\u0026rsquo;s male strength of heart\u0026rdquo; in crossing traditional gender role boundaries. She is neither Helen, nor Penelope, but does that make her wrong?\nThe Herald The Herald announces the return of the pythian, or Apollo blessed, king. This is an appeal so that Apollo may show mercy on the Argives, now that the war has ended. An interchange occurs with the Chorus, where the Chorus Leader makes it apparent that the people have been oppressed under Clytaemestra\u0026rsquo;s rule. The interchange ends with the good news that the war in fact has ended. Clytaemestra\u0026rsquo;s tyranny and lies are seen to possibly be at an end. Unfortunately this hope remains unfulfilled. The herald further recalls the ruin of the Argive fleet, cementing the possibility of a coup.\nAgamemnon  In few men is it part of nature to respect a friend\u0026#39;s prosperity withou begruding him, as envy\u0026#39;s wicket poison settling to the heart piles up the pain in one sick with unhappiness, who, staggered under sufferings that are all this own, winces again to the vision of a neighbor\u0026#39;s bliss.\nAgamemnon, Agamemnon L 830  Agamemnon returns home victorious, though he suspects that the body politic may have turned gangrenous. The people inform him that, though his crusade was ill recieved, in victory the war for Helen has become palatable. On his arrival he trusts his home to be secure in the hands of Clytaemestra. She twists him into committing pride and idolatry of self by entering the palace walking on a red tapestry, literature\u0026rsquo;s first red carpet. Ironically his role in his namesake play is limited and instead he is more of a backdrop character, killed soon after arrival by his wife.\nCassandra  And drugged to double fury on the wine of men\u0026#39;s blood shed there lurks forever here a drunken rout of ingrown vengeful spirits never to be cast forth.\nCassandra, Agamemnon L 1180  Cassandra, the daughter of Priam, arrives in Argos as Agamemnon\u0026rsquo;s concubine. She is a prophetess, or oracle of Apollo. She prophecies of the death of Agamemnon at the hands of his wife, of her own death, and of the revenge which would befall Clytaemestra. She speaks truly of the vicious cycle of death which is the curse of the House of Atreus. Through her the audience learns of the coming retribution from the one who would kill his own mother. Shortly after revealing this, she dies at the hands of Clytaemestra without a threnoad.\nAegisthus Aegisthus is legitimately grieved; why not seize the opportunity which presents itself? Revenge, a throne, even a queen are up for grabs. He has all the motive and no reason not to indulge it. Once he gains the throne he quickly shows himself to be the tyrant Antinous would have become. While this may be a tragic outcome for Agamemnon, presented by the playwright as the returning hero, I cannot say that Aegisthus' actions were unjustifed. Just or unjust, the perpetuate the cycle of violence which plagues the house of Atreus, and the people suffer for it.\nTakeaways The House of Atreus the archtypical family fued. There is more to it than that though. It symbolizes the need for an impartial system of justice and the corruption that inherently occurs in powerful families. The common man sees the injustice here, but is not able to stop it. The cycle of violence continues, draining the land of it\u0026rsquo;s lifeblood. Where will it stop, what levels of depravity will be reached in it\u0026rsquo;s escalation?\n","summary":"As the first play of The Orestia opens, the Trojan war is ending. The watchman, straggling the Atreidae\u0026rsquo;s roof dogwise, sees the beacon signaling victory in Ilium. Agamemnon returns home with Cassandra, a prophetess and daughter of Priam, as a concubine. He has been gone for ten years, with the human sacrifice of his daughter Iphigenia as his farewell. Once home he is greeted by the duplicitous Clytaemestra. She conspires with her lover Aegisthus to kill Agamemnon and seize the reins of power.","tags":["aeschylus","orestia","justice","aristocracy","family","tyranny"],"title":"Agamemnon","url":"/musings/agamemnon/"},{"categories":null,"contents":"The scene opens; Prometheus, the fore thinking one, has defied Zeus in giving various knowledge to mankind, thus incurring his wrath. Hephaestus is nailing Prometheus to a craggy mountain face reluctantly, spurred on by Might and Violence. The immediate parallels with the fall of Man present themselves in the modern context. The imagery is inverted, with the rebel suffering at the hands of a tyrannical ruler of the universe for helping mankind. Prometheus is a fire bringer, one who illuminates. He claims to have liberated man from ignorance. Is man really better off with reason, conciousness and knowledge?\nWhether or not man is better off for what Prometheus has done, Prometheus is blessed with the gift of foresight. He knew beforehand the volume of his suffering and how it would come to an end. He still made the choice to ease the suffering of mankind, at his own expense. Prometheus has no fear of death, and has taken the foresight of death away from mankind; thus freeing them to explore knowledge, spurred by the ephemeral nature of mortal existence.\nZeus is presented in the dialectic of the characters as a young tyrant, eager to make an example of those who would defy him. This is what leads to the overzealous punishment of Prometheus, doomed to have his liver eaten daily and never die. This viewpoint of Zeus as arbitrary, capricious and tyrannical is also present in other works of this time period. Aeschylus seems to be solidly on the side of Prometheus in his writing. While Prometheus' actions are seen as a sin by Zeus, they are not in the general story. In furtherance of this, we see the suffering of Io, who is the matrilineal ancestor of Heracles. Prometheus tells her how to end her torture at the hands of Zeus, for his own long term benefit.\nCan Might exist without Violence? Does Zeus need to act the tyrant to maintain power? Does Might exist without Force behind it in our world today? Was what Prometheus did a Sin? In some ways we are left with more questions then answers, a lot to chew on.\nPrometheus Bound is the central play and remainder of a trilogy written by Aeschylus about the being who created mankind. Although the other plays are largely lost to us, we learn that he was bound for thirty thousand years in the surviving fragments of The Firecarrier. We also know that he was later freed by Heracles in Prometheus Unbound. Zeus allows this in exchange for Prometheus revealing the prophecy of Zeus' possible downfall in lying with Thetis, whereby a son stronger than the father would be born. The redirection of this threat leads to the birth of Achilles.\nWhere to Read or Watch Prometheus Bound I read through the University of Chicago Press edition, which I did not find to be a supremely readable edition, it is also available for free on Project Gutenberg. Reading the play and thinking about how you would direct it, or seeing it live is likely the best way to grasp it fully. In lieu of that, here is the best online production of the play I could find:\n  ","summary":"The scene opens; Prometheus, the fore thinking one, has defied Zeus in giving various knowledge to mankind, thus incurring his wrath. Hephaestus is nailing Prometheus to a craggy mountain face reluctantly, spurred on by Might and Violence. The immediate parallels with the fall of Man present themselves in the modern context. The imagery is inverted, with the rebel suffering at the hands of a tyrannical ruler of the universe for helping mankind.","tags":["aeschylus","justice","sin","prophecy"],"title":"Prometheus Bound","url":"/musings/prometheus-bound/"},{"categories":null,"contents":"The Odyssey is the story of family, love, justice, virtue, and vice. The tale of how they are woven into civilization. On a basic level, it is about a man\u0026rsquo;s journey home. More deeply, it is about the travails of an abandoned family; it\u0026rsquo;s about the unlawfulness of swaggering masculinity; it\u0026rsquo;s about the forbearance of a faithful wife. An epic of many tales about a man of many twists and turns.\nOdysseus Odysseus, like all, is a flawed man. The man of strife is the inverse of hot Achilles, in his serene, calculated wisdom. He leaves his family to fight a war. Ten years on, with the war won and plunder shared, he doesn\u0026rsquo;t exactly rush home. Depending on which of the two stories the man of convolutions tells is true, he was either sold into slavery or had ten years of supernatural journeys. The former paints him as the better man, though foolish with his endless raiding. The later portrays him as the unfaithful husband, an absentee father, in no hurry to return home. Eventually, with the help of the Phaeacians, a lawful people, he returns home. There he restores the law and regains marital unity.\nPenelope Penelope is the anti-Clytaemestra, standing by her family against suitors rather than betraying them; she is everything Helen is not. She is the ideal wife, as seen by Agamemnon in Hades, loyal, long-suffering, wise, holding the walls against the dissolution of hearth and home. She is faithful and sacrifices endlessly to defend Telemachus, spending twenty years of her prime with a missing husband.\nTelemachus Telemachus provides the prototypical coming of age story. From a boy, he grows into a lethal force and devises a plan to seize the reins of manhood. He becomes a lawful and righteous man, even without the guidance of his father. This is a contrast to the suitors, who, despite the presence of fathers, grow into swaggering fools.\nAntinous Antinous is the opposite of Telemachus in lawfulness, and the inverse of Odysseus' high intellect, in low cunning. He is the epitome of the spoiled brat, grown into a violent and entitled man, a man who is to eventually be on the receiving end of a red wedding, old testament justice.\nEumaeus Eumaeus is the exemplar of loyalty and lawfulness in the common man, the antithesis of Melanthius. his honorable nature raises him above the suitors, high in station, who debase themselves by flouting justice.\nTakeaways This tale addresses vice and virtue in all walks of life; a juxtaposition of right and wrong. The suitors and Telemachus in nobility, Eumaeus and Melanthius as common men, Odysseus as the beggar and Arnaeus as the poor.\nThis tale of tales, about the man of twists and turns, highlights the marital bed and unity of the family; it weaves the fabric of civilization from its fibers. It illuminates home as the most important place to be. It is a cautionary tale for those who would tarry away from home in search of wealth and glory, one that tells a story of returning home to find it no longer there. It elucidates true wisdom as fidelity, hospitality, and a swift return to those you love.\nThoughts on the Robert Fagles Translation I read through the Robert Fagles translation and found it enjoyable, more so than his Iliad. The reading was rich in imagery and, although I don\u0026rsquo;t know ancient Greek yet to judge against Homer, it definitely had beautiful poetry. This read almost like a modern novel. There were few or no awkwardly rendered portions. It was something you could sit down and really fall into.\n","summary":"The Odyssey is the story of family, love, justice, virtue, and vice. The tale of how they are woven into civilization. On a basic level, it is about a man\u0026rsquo;s journey home. More deeply, it is about the travails of an abandoned family; it\u0026rsquo;s about the unlawfulness of swaggering masculinity; it\u0026rsquo;s about the forbearance of a faithful wife. An epic of many tales about a man of many twists and turns.","tags":["homer","justice","family","law","love","virtue and vice"],"title":"The Odyssey","url":"/musings/the-odyssey/"},{"categories":null,"contents":"The Iliad is a brutal war epic, perhaps the war epic. I started reading it back in August 2019. During the reading, I attended two seminars to discuss the book, and completed it in November. It was a hard read for me. That may have been because of the constant, grinding, unreal level of violence portrayed. Though this was my first reading of The Iliad, I will hazard a summary to crystalize my understanding. The Iliad, in my reading, is a book about pride, rage, humility, the nature of men, and the horror of war, at least. The work illustrates amply the folly of rage and pride, as embodied in the characters it portrays.\nPride is a factor in various plots and schemes throughout. For example, Apollo, angered at the pride of Agamemnon, uses that pride against him to sow dissent with Achilles. This causes the death of many. In turn, Achilles own pride causes the death of his best friend, Patroclus. The moral I take from this is that humility, as shown by Priam in this story, can avoid unneeded violence. Hector, as I see it a hero of the story, is killed due to his sense of duty. This, combined with pride, in thinking he can face Achilles alone. Ultimately he dies in vain. Pride and vanity go hand in hand, a recurring theme.\nOne part of the book, which was brought to my attention in seminar, is the parrallel narratives of the gods and men, with two seperate conflicts taking place. One conflict of the gods, the other of men, who are used as pawns of the gods. The gods, as portrayed by Homer, are as flawed as humans, or moreso. It is the plots, vendettas and favoritism of the gods which are shown to drive the war forward. It is, of course, easier to blame the gods, rather than our own flawed nature.\nThere are sundry other topics treated in the story, such as the malleability of fate in the eyes of the ancient Greeks. This work, like many of the greats, is something you could come back to again, and again. I think you\u0026rsquo;d keep finding more depth. In fact, since reading, I see echoes of this story in every war movie. The men here are half comic book hero, half real and flawed human.\n   Muse, sing the rage of Peleus\u0026#39; son Achilles - Book One, The Iliad    About the Translation The long journey of reading this work started with a short walk, down to a local bookstore. The store, named Logos, seemed a fitting place to purchase my paperback copy of the Robert Fagles translation. I\u0026rsquo;d be interested to reread this in the Pope translation. Though I found the Fagles translation enjoyable, I have heard that Pope is worth a read. To that end I might also give the Pope edition a listen on Audible.\n","summary":"The Iliad is a brutal war epic, perhaps the war epic. I started reading it back in August 2019. During the reading, I attended two seminars to discuss the book, and completed it in November. It was a hard read for me. That may have been because of the constant, grinding, unreal level of violence portrayed. Though this was my first reading of The Iliad, I will hazard a summary to crystalize my understanding.","tags":["war","death","other","duty","iliad","epic","poetry"],"title":"The Iliad","url":"/musings/the-iliad/"},{"categories":null,"contents":"Why do I need a Zettelkasten? The idea of a Zettelkasten struck me in a chat recently. I have been keeping physical journals, using a modified form of bullet journalling, for a while; I had been keeping less structured notes before that, and ruminating on how to digitize them. After learning about the concept of Zettelkasten, it seemed like the solution; and that led me down the rabbit hole of investigation. Because of that investigation, I am now building a Zettlekasten for myself. The idea of a way to grok all notes for eternity is an obvious boon to any knowledge worker. This system has less obvious, and much greater benefits beyond just digitizing notes.\nWhat is a Zettelkasten? The Zettelkasten is a concept, essentially, of a graph of data interconnected by metadata, links and taxonomies. It was originally laid out by a German sociologist: If you wish to educate a partner in communication, it will be good to provide him with independence from the beginning. A slip box, which has been made according to the suggestions just given can exhibit great independence.\nCommunicating With Slip Boxes, by Niklas Luhmann  It was also explored in this physical format by Umberto Eco (1977) in How to Write a Thesis. This format involved drawers full of annotated index cards.\nThis loosely organized graph allows a Cartesian explosion of complexity. The core idea of the system is a second brain, one mirroring the structure of the human mind. This organic structure is how zufall, or randomness, eventually emerges, resulting in serendipitous discovery. The weighting of links produces more prominent and less prominent regions, much like search engine rankings. This starts to sound an awful lot like a preeminent description of the internet and search engines. There is, however, another aspect to consider:\nWithout variation in the given material of ideas, there are no possibilities of examining and selecting novelties. The real problem thus becomes therefore one of producing accidents with sufficiently enhanced probabilities for selection.\nNiklas Luhmann, Communicating With Slip Boxes  The distillation of broader research into notes with metadata allows for your own personal internet, with a less hierarchical structure than say, a wiki. The balance between order and chaos is essential, Luhmann purports, to the extract synthesis of new ideas from a body of notes.\nWhat Makes a Zettelkasten? Zettels A zettel is a set of data, along with related attachments and metadata. Every zettel, or note, is a node in the graph that becomes the Zettelkasten.\nArbitrary Internal Branching Any zettel must have the ability to branch and be a parent of other zettels\nPossibility of Linking Any zettel must be able to link to any other zettel.\nIndex, or Register of Taxonomies and Tags For the zettels to be useful in the concept of the Zettelkasten, there must be an index or register that is searchable.\nInflow There must be a source of content. For me that is my paper notes and digital notes. Those notes must be collected; then they must be processed into zettels with metadata, and links to other zettels.\nOutflow The Zettelkasten is accessed via the register to search for ideas while writing or producing other outputs. Optionally there is a feedback into the inflow during this process.\nHow Can We Make a Digital Zettelkasten? Thoughts on Design The physical design of the system as proposed by Luhmann is outdated. Physical space was a concern, but in the context of cheap hard drive space it is not. The issue ordering also becomes irrelevant. Though, the question of a flat structure to prevent usage bias, or obsession over categorization and order, has it\u0026rsquo;s merit. So digital is definitely the way to go.\nThat leads to the question of how best to do it. There are ready made solutions out there like The Archive, and that may be the solution for most. It does seem however to duplicate extant functionalities in hypertext and web browsers. All we really need is URIs, metadata, and a search system for that metadata to create a functioning Zettelkasten.\nEnter: A Hugo Based Zettelkasten In From WordPress to Hugo, I migrated my blog to Hugo. This proved to be the extant solution I was looking for. I thought about an optimal design for a Zettelkasten; one that would be portable, open source, and stand the test of time. A solution that could check the boxes of arbitrary metadata, linking and search of those things. Hugo, with its use of markdown, standard URIs and static page generation is a great solution.\nI stumbled upon the After Dark theme for hugo, by Josh Habdas. It already had many of the features I needed, such as search, citations, and rich linking between pages, or zettels implemented. So I switched my blog over to the aforementioned theme. I then created a private git submodule repository, and put that in the content/zettelkasten folder of my hugo setup. Voila, a public blog with a private Zettelkasten.\nThe only caveat on the theme is that it uses Fuse.js for search. I am not sure if it is the implementation in the theme, or the nature of the fuzzy search, but it isn\u0026rsquo;t always the best at finding things. I am considering tweaking the implementation or switching it out for Lunr, but those are future problems.\nWhen the CI pipeline runs to build the blog, does not have permissions to pull the Zettelkasten, which is secured by private key. But when I run a local development server, I have both my Zettelkasten and my blog, an outflow, searchable at my fingertips.\nWhat about portability? Here is the cool part, Hugo can run on android. So with git, I can just clone my blog on my phone and run a local web server. More to follow on that in another blog post.\nWhat\u0026rsquo;s Next? I still have to approach getting all of my notes into zettels. Maybe I\u0026rsquo;ll have more to say after that. I\u0026rsquo;ll also likely add a technical step by step for setting up a Zettelkasten using Hugo and the After Dark theme, as time permits. Leave your thoughts in the comments below.\n","summary":"Why do I need a Zettelkasten? The idea of a Zettelkasten struck me in a chat recently. I have been keeping physical journals, using a modified form of bullet journalling, for a while; I had been keeping less structured notes before that, and ruminating on how to digitize them. After learning about the concept of Zettelkasten, it seemed like the solution; and that led me down the rabbit hole of investigation.","tags":["mind","relation","zettelkasten","information","order","chaos"],"title":"Creating a Zettelkasten with Hugo","url":"/blog/creating-a-zettelkasten-with-hugo/"},{"categories":null,"contents":"This tutorial covers mounting an RBD image at boot under CentOS 7. Make sure to unmount the RBD you want to have mount at boot before following this tutorial. This tutorial requires a CentOS 7 client with a client or admin keyring from Ceph, and a working Ceph cluster. This tutorial also assumes you have already created the RBD image you want to be mounted at boot. Let\u0026rsquo;s begin!\nAssumptions This tutorial assumes the node you are implementing this on has connectivity to a working ceph cluster and also assumes that kernel module RBD is enabled. For the purposes of this tutorial I will place variables, the values specified here:\nexport poolname = your_pools_name export rbdimage = the_name_of_your_rbd_image export mountpoint = place_to_mount_the rbd Create A systemd service to map and mount automatically on boot / demand You will want to automatically load the kernel module, map the appropriate rbd storage to a local device and mount the ceph image. Here is a simple script for mounting and un-mounting RBD images create one at /usr/bin/mount-rbd-$poolname-$rbdimage for each of your RBD images:\n#!/bin/bash # Image mount/unmount and pool are passed from the systems service as arguments # Determine if we are mounting or unmounting if [ \u0026#34;$1\u0026#34; == \u0026#34;m\u0026#34; ]; then modprobe rbd rbd map --pool $poolname $rbdimage --id admin --keyring /etc/ceph/ceph.client.admin.keyring mkdir -p $mountpoint mount /dev/rbd/$poolname/$rbdimage $mountpoint fi if [ \u0026#34;$1\u0026#34; == \u0026#34;u\u0026#34; ]; then umount $mountpoint rbd unmap /dev/rbd/$poolname/$rbdimage fi Create a new systemd service unit (/etc/systemd/system/mount-rbd-$poolname-$rbdimage.service) for each of your remote rbd images:\n[Unit] Description=RADOS block device mapping for $rbdimage in pool $poolname\u0026#34; Conflicts=shutdown.target Wants=network-online.target After=NetworkManager-wait-online.service [Service] Type=oneshot RemainAfterExit=yes ExecStart=/usr/bin/mount-rbd-$poolname-$rbdimage m ExecStop=/usr/bin/mount-rbd-$poolname-$rbdimage u [Install] WantedBy=multi-user.target Make sure your target RBD is unmounted. Start the service and check whether /dev/rbd0 is created or not:\nsystemctl start mount-rbd-$poolname-$rbdimage\nMounting an RBD at Boot Under CentOS 7 is Easy! If everything seems to be fine, enable the service to start on boot:\nsystemctl enable mount-rbd-$poolname-$rbdimage\nYou now have a working RBD mount at boot time! I wil be following this up with a complete tutorial on the entire process of creating an RBD at some point in the future. Leave your thoughts in the comments below.\n","summary":"This tutorial covers mounting an RBD image at boot under CentOS 7. Make sure to unmount the RBD you want to have mount at boot before following this tutorial. This tutorial requires a CentOS 7 client with a client or admin keyring from Ceph, and a working Ceph cluster. This tutorial also assumes you have already created the RBD image you want to be mounted at boot. Let\u0026rsquo;s begin!\nAssumptions This tutorial assumes the node you are implementing this on has connectivity to a working ceph cluster and also assumes that kernel module RBD is enabled.","tags":["centos","ceph","rbd","rhel","systemd"],"title":"Mounting RBD at Boot Under CentOS 7","url":"/blog/mounting-rbd-at-boot-under-centos-7/"},{"categories":null,"contents":"Installing HHVM and Nginx on Ubuntu 14.04 is the next step in the \u0026ldquo;Make WordPress Fly\u0026rdquo; series. This tutorial assumes you have completed the prerequisites, read Part 1 and completed Part 2 of this guide. At this point you have a reasonably secure box with MariaDB installed and configured. In this (Part 3) of the \u0026ldquo;Make WordPress Fly\u0026rdquo; guide we will start out by preparing our system for Nginx. The first step is to reconnect to your VM via SSH.\nssh -p port user@you.rip.add.res\nInstalling Nginx on Ubuntu 14.04 After reconnecting we are going to install some prerequisites in this order to make sure HHVM plays nicely with Nginx and WordPress.\nsudo apt-get update sudo apt-get install php5-gd libssh2-php After that process completes it is time to install Nginx. Installing Nginx on Ubuntu 14.04 is a very easy process. Ubuntu official repos come with a Nginx package but I prefer using launchpad repo maintained by Nginx team. We will also install the Naxsi WAF (Web Application Firewall) to provide some added security. You can choose not load Naxsi later as it slows down cached requests per second by around 3%. However, a full fledged WAF is worth a 3% requests per second hit.\nsudo add-apt-repository ppa:nginx/stable sudo apt-get update sudo apt-get install nginx-naxsi If you prefer to use Nginx package in Ubuntu repo, you can simply run following command:\nsudo apt-get install nginx-naxsi\nThat concludes the process of installing Nginx. We will configure it further in later parts of this tutorial series based on whether you use WordPress Multisite or a single install.\nInstalling HHVM on Ubuntu 14.04 Let\u0026rsquo;s move on to installing HHVM on Ubuntu 14.04. We\u0026rsquo;ll need to prepare the HHVM repositories. Using sudo or as root user it is recommended to run sudo apt-get update and sudo apt-get upgrade first, or you may receive errors. Then we are ready to add the repositories and install HHVM.\nwget -O - http://dl.hhvm.com/conf/hhvm.gpg.key | sudo apt-key add - echo deb http://dl.hhvm.com/ubuntu trusty main | sudo tee /etc/apt/sources.list.d/hhvm.list sudo apt-get update sudo apt-get install hhvm Now that HHVM is installed there are a few simple configurations to apply. HHVM comes bundled with a script that makes setting it up with Ubuntu very easy. If you are already using Nginx with PHP-FPM, you\u0026rsquo;ll have to modify the configuration file to disable the use of PHP-FPM. This file is normally located at /etc/nginx/sites-available/default Look for the following section and make sure it\u0026rsquo;s all commented (by adding a # at the beginning of each line)\n# pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # fastcgi_split_path_info ^(.+\\.php)(/.+)$; # # NOTE: You should have \u0026quot;cgi.fix_pathinfo = 0;\u0026quot; in php.ini # # # With php5-cgi alone: # fastcgi_pass 127.0.0.1:9000; # # With php5-fpm: # fastcgi_pass unix:/var/run/php5-fpm.sock; # fastcgi_index index.php; # include fastcgi_params; #} After doing this, execute the following script:\n/usr/share/hhvm/install_fastcgi.sh Executing this script configures Nginx to start using HHVM to process the PHP code. It\u0026rsquo;ll also restart the Nginx server so you don\u0026rsquo;t have to do anything else. Then you may want to tweak the max_upload_size of HHVM by editing /etc/hhvm/php.ini. Otherwise HHVM is now setup and working.\nVerifying that HHVM is Working Correctly With Nginx and Ubuntu 14.04 It is important verify that HHVM is working with Nginx. You can verify this by creating a file in /usr/share/nginx/html called test.php. Paste this inside:\n\u0026lt;?php echo defined('HHVM\\_VERSION')?'Using HHVM':'Not using HHVM'; ?\u0026gt; Visit http://you.rip.add.res/test.php to view the output. This will verify that HHVM is handling PHP. Now just make sure that HHVM and Nginx run by default at startup.\nsudo update-rc.d nginx defaults sudo update-rc.d hhvm defaults You are ready to move on to the next part of this tutorial.\n","summary":"Installing HHVM and Nginx on Ubuntu 14.04 is the next step in the \u0026ldquo;Make WordPress Fly\u0026rdquo; series. This tutorial assumes you have completed the prerequisites, read Part 1 and completed Part 2 of this guide. At this point you have a reasonably secure box with MariaDB installed and configured. In this (Part 3) of the \u0026ldquo;Make WordPress Fly\u0026rdquo; guide we will start out by preparing our system for Nginx. The first step is to reconnect to your VM via SSH.","tags":["hhvm","nginx","ubuntu"],"title":"Install HHVM, Nginx on Ubuntu 14.04 - Make WordPress Fly","url":"/blog/install-hhvm-nginx-on-ubuntu-14-04-make-wordpress-fly/"},{"categories":null,"contents":"Back in November of 2013 I had the opportunity to visit the Navajo nation and the Hopi and Acoma Pueblos. The Acoma had a nice museum and guided tours of a mesa-top city know as \u0026ldquo;Sky City\u0026rdquo;. Acoma Pueblo is a Native American pueblo approximately sixty miles west of Albuquerque, New Mexico in the United States. Three villages make up Acoma Pueblo: Sky City (Old Acoma), Acomita, and McCartys. The Acoma Pueblo tribe is a federally recognized tribal entity. The historical land of Acoma Pueblo totaled roughly 5 million acres; now only 10% of this land is in the hands of the community within the Acoma Indian Reservation. This picture was taken in Old Acoma.\nMy wife and I were able to snap some beautiful shots of the surrounding area. The mesas of Arizona and New Mexico are breathtaking places.\n   The Acoma Pueblo    I have to say that the climb down the \u0026ldquo;stairs\u0026rdquo; with my daughter Alice on my back was a fun experience.\n   The Acoma Pueblo Stairs Down    The Acoma pueblo is interesting in that it was destroyed by the Spaniards back in the 17th century via cannon fire. The lower levels of housing, seen in sandstone are what remains. When it was rebuilt under Spanish rule, the Spanish taught the pueblo people to use mud bricks, which are much more high maintenance and require yearly stucco.\n","summary":"Back in November of 2013 I had the opportunity to visit the Navajo nation and the Hopi and Acoma Pueblos. The Acoma had a nice museum and guided tours of a mesa-top city know as \u0026ldquo;Sky City\u0026rdquo;. Acoma Pueblo is a Native American pueblo approximately sixty miles west of Albuquerque, New Mexico in the United States. Three villages make up Acoma Pueblo: Sky City (Old Acoma), Acomita, and McCartys. The Acoma Pueblo tribe is a federally recognized tribal entity.","tags":["acoma","new mexico","pueblo","nature"],"title":"A Trip to Acoma Pueblo","url":"/blog/a-trip-to-acoma-pueblo/"},{"categories":null,"contents":"Ceph raw disk performance testing is something you should not overlook when architecting a ceph cluster. When choosing media for use as a journal or OSD in a Ceph cluster, determining the raw IO characteristics of the disk when used in the same way ceph will use the disk is of tantamount importance before tens, hundreds or thousands of disks are purchased. The point of this article is to briefly discuss how ceph handles IO. One important point is to estimate the deviation caused by ceph between RAW IOs from disk and ceph IOs.\nTESTING \u0026amp; GRAPHING WITH FIO For this article I assume you are aware of fio and feel some degree of comfort uing it. You will need FIO and GNUPlot installed to run these tests. I have developed an automation tool in my spare time for writing these tests. You can find it here: ceph-disk-test RBD can best be simulated by using a block size of 4M in your testing. However it is pertinent to test with smaller IOs like 64k or 4k for worst case. Below is an example test run with a Samsung Extreme USB stick to demonstrate how the results look using this automation. The automation produces a nice graphs.\nJournal IO Journal IO in Ceph uses O_DIRECT and D_SYNC flags. Journals write with an IO Depth of 1 (1 IO at a time). However if you colocate multiple journals you should increase your IO depth to the number of journals you plan to colocate on the drive. Here is an example FIO test for testing journal performance on a disk:\nioengine=libaio invalidate=1 ramp_time=5 iodepth=1 runtime=300 time_based direct=1 sync=1 bs=4m size=10240m filename=/tmp/tmp.ohf4hXHp1F/test.file [seq-write] stonewall rw=write write_bw_log=/tmp/tmp.QpdVIjrlnI/150825-1653-seq-write-journal-4m-intel-dc3700-d1 write_lat_log=/tmp/tmp.QpdVIjrlnI/150825-1653-seq-write-journal-4m-intel-dc3700-d1 write_iops_log=/tmp/tmp.QpdVIjrlnI/150825-1653-seq-write-journal-4m-intel-dc3700-d1 write_iolog=/tmp/tmp.QpdVIjrlnI/150825-1653-seq-write-journal-4m-intel-dc3700-d1 [rand-write] stonewall rw=randwrite write_bw_log=/tmp/tmp.QpdVIjrlnI/150825-1653-rand-write-journal-4m-intel-dc3700-d1 write_lat_log=/tmp/tmp.QpdVIjrlnI/150825-1653-rand-write-journal-4m-intel-dc3700-d1 write_iops_log=/tmp/tmp.QpdVIjrlnI/150825-1653-rand-write-journal-4m-intel-dc3700-d1 write_iolog=/tmp/tmp.QpdVIjrlnI/150825-1653-rand-write-journal-4m-intel-dc3700-d1 OSD IO OSDs use buffered IO and thus you need to run performance tests of a size and duration that is greater then the amount of RAM in the test machine. Here is an example test file for an OSD:\nioengine=libaio invalidate=1 ramp_time=5 iodepth=32 runtime=120 time_based direct=0 bs=4m size=10240m filename=/tmp/tmp.taNceuCnCq/test.file [seq-write] stonewall rw=write write_bw_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-write-osd-4m-intel-dc3700 write_lat_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-write-osd-4m-intel-dc3700 write_iops_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-write-osd-4m-intel-dc3700 write_iolog=/tmp/tmp.IF0gdpokNE/150825-1218-seq-write-osd-4m-intel-dc3700 [rand-write] stonewall rw=randwrite write_bw_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-write-osd-4m-intel-dc3700 write_lat_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-write-osd-4m-intel-dc3700 write_iops_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-write-osd-4m-intel-dc3700 write_iolog=/tmp/tmp.IF0gdpokNE/150825-1218-rand-write-osd-4m-intel-dc3700 [seq-read] stonewall rw=read write_bw_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-read-osd-4m-intel-dc3700 write_lat_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-read-osd-4m-intel-dc3700 write_iops_log=/tmp/tmp.IF0gdpokNE/150825-1218-seq-read-osd-4m-intel-dc3700 write_iolog=/tmp/tmp.IF0gdpokNE/150825-1218-seq-read-osd-4m-intel-dc3700 [rand-read] stonewall rw=randread write_bw_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-read-osd-4m-intel-dc3700 write_lat_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-read-osd-4m-intel-dc3700 write_iops_log=/tmp/tmp.IF0gdpokNE/150825-1218-rand-read-osd-4m-intel-dc3700 write_iolog=/tmp/tmp.IF0gdpokNE/150825-1218-rand-read-osd-4m-intel-dc3700 References Research material used to structure these ceph raw disk performance tests:\n Journal Testing OSD Testing Ceph IO, The Bad\n","summary":"Ceph raw disk performance testing is something you should not overlook when architecting a ceph cluster. When choosing media for use as a journal or OSD in a Ceph cluster, determining the raw IO characteristics of the disk when used in the same way ceph will use the disk is of tantamount importance before tens, hundreds or thousands of disks are purchased. The point of this article is to briefly discuss how ceph handles IO.","tags":["ceph","storage","performance","osd","fio"],"title":"Ceph Raw Disk Performance Testing","url":"/blog/ceph-raw-disk-performance-testing/"},{"categories":null,"contents":"Installing WordPress with Nginx on Ubuntu 14.04 is a fairly straightforward task. In this tutorial we will do over how to do it. This tutorial assumes you have completed the Getting Started with an Ubuntu VPS guide and have an Ubuntu 14.04 VPS (if not you can get one at vultr). It also assumes that you already have a LEMP stack setup (Linux, Nginx, MySQL, etcetera) or you are following the WordPress HHVM guide. This tutorial assumes the use of Nginx as the web server, Fastcgi or HHVM for PHP and either MariaDB or MySQL for your MySQL server. The first step in this tutorial is to connect to your virtual machine via SSH. This tutorial assumes that you are using Linux as your operating system and have SSH installed. If you do not you can use a tool like Putty for SSH. In Linux you just need to run the following command:\nssh -p port user@you.rip.add.res After connecting to your instance via SSH it is time to begin the process of installing WordPress to work with Nginx. All of our tutorials for Nginx assume a \u0026ldquo;web root\u0026rdquo; of /var/www/html, make sure that your Nginx configuration points there and that the directory exists. If the directory does not exist create it using mkdir and chown it to www-data .\nsudo mkdir /var/www/ sudo mkdir /var/www/html/ sudo chown -R www-data:www-data /var/www/html/ Creating a Database and User After you have confirmed that Nginx is using /var/www/html/ as your web root or setup another of your choice, it\u0026rsquo;s time to create a database for WordPress. Please make sure you have already setup MySQL or MariaDB prior to this step. Setting up the database is easy. Start by logging into an interactive session with the MySQL administrative account.\nmysql -u root -p You will be prompted for the root password you setup during MySQL installation. Enter it and proceed to the interactive prompt. Next we are going to create a database for WordPress to use and store information in. The name of the database does not matter, but it should be memorable so that you can distinguish it as you add additional databases later on. To do this simply run this command:\nCREATE DATABASE wordpress; Note the semi-colon (;) that ends the MySQL statement. Every MySQL statement must end with one, so check that if you are running into issues. Now that you have created a database, we need to create a user. You are going to use the same interactive interface you are in now to create a user. Use this command:\nCREATE USER wordpressuser@localhost IDENTIFIED BY 'password'; Make sure you replace \u0026lsquo;password\u0026rsquo; with the database password you want to use and \u0026lsquo;wordpressuser\u0026rsquo; with the name of the database user you want to create. After that is done you need to assign that user privileges to use the database we just created. Use this command:\nGRANT ALL PRIVILEGES ON wordpress.\\* TO wordpressuser@localhost; Make sure you replace the database name and username with the ones you created. Everything should now be configured correctly. We need to flush the privileges (save them to disk) so that our current instance of MySQL knows about the privilege changes we have made:\nFLUSH PRIVILEGES; Now you can exit MySQL:\nexit\nAt this point you are back at the shell command prompt and ready to continue.\nInstalling WordPress with Nginx on Ubuntu 14.04 The next step is to download the latest version of WordPress to the server. It is available on their website. We are going to use the wget command to copy it to our home directory. WordPress always keeps the latest stable version at the place we will use in this command.\ncd ~ wget http://wordpress.org/latest.tar.gz The files which compose WordPress were downloaded as a compressed archive stored in a file called latest.tar.gz. We can extract the contents by typing:\ntar xzvf latest.tar.gz This will extract a directory called wordpress containing all the files we need to set up WordPress. First however make sure that php5-gd and libssh2-php are installed. If they are not, run the command below. This will make sure you can work with images and install modules/plugins over SSH.\nsudo apt-get update sudo apt-get install php5-gd libssh2-php Configuring WordPress with Nginx on Ubuntu 14.04 Now we are ready to configure WordPress and move it into the web root. Let\u0026rsquo;s move into the directory that we extracted WordPress to in the last section:\ncd ~/wordpress Now we want to copy the sample configuration to take the place of the non-existent main configuration.\ncp wp-config-sample.php wp-config.php\nNext we need to make 3 small changes to wp-config.php using nano or your text editor of choice.\nnano wp-config.php\nThe file is suitable for launching WordPress; it is just lacking the information to connect to the database we created a few minutes ago. The parameters we need to set are DB_NAME , DB_USER , and DB_PASSWORD . After you make the changes to that section of the file it should look something like this:\n// ** MySQL settings - You can get this info from your web host ** // /** The name of the database for WordPress */ define('DB_NAME', 'wordpress'); /** MySQL database username */ define('DB_USER', 'wordpressuser'); /** MySQL database password */ define('DB_PASSWORD', 'password'); For now you can ignore the rest of the site. If you are planning on deploying a multisite network add this line:\n/* Multisite */ define( \u0026lsquo;WP_ALLOW_MULTISITE\u0026rsquo;, true );\nOnce you have made these changes you can save and close the file. Now it is time to copy the files to our web root (/var/www/html/ in this example). We can copy the files to this place by typing:\nsudo rsync -avP ~/wordpress/ /var/www/html/ Now we need to move over to that folder to assign some permissions.\ncd /var/www/html/ Then we are going to make sure that Nginx owns these files so that it can manipulate them.\nsudo chown -R www-data:www-data /var/www/html/*\nBefore we move on, we should create a new directory for user uploads:\nmkdir wp-content/uploads The new directory should have group writing set already, but the new directory isn\u0026rsquo;t assigned with www-data group ownership yet. Let\u0026rsquo;s fix that:\nsudo chown -R :www-data /var/www/html/wp-content/uploads Now just make sure that your web server is configured to use /var/www/html/ as the webroot and you can visit yourdomain.com to set your site name and get started. You are also going to want to install Postfix so that WordPress can send emails. We will be writing a tutorial for that in the near future. Thanks for reading and leave your thoughts in the comments below.\n","summary":"Installing WordPress with Nginx on Ubuntu 14.04 is a fairly straightforward task. In this tutorial we will do over how to do it. This tutorial assumes you have completed the Getting Started with an Ubuntu VPS guide and have an Ubuntu 14.04 VPS (if not you can get one at vultr). It also assumes that you already have a LEMP stack setup (Linux, Nginx, MySQL, etcetera) or you are following the WordPress HHVM guide.","tags":["nginx","ubuntu","wordpress"],"title":"Installing WordPress with Nginx on Ubuntu 14.04","url":"/blog/installing-wordpress-with-nginx-on-ubuntu-14-04/"},{"categories":null,"contents":"The Trip On a Saturday evening in August of 2018 my other half and I and left the Upper East Side of Manhattan in a taxi bound for JFK. Excitement and apprehension were in the air as we were eager to leave, but about to commend our kids to my mother in law for a week. We had, and have not since, been away for so long from our children. After arriving, we were informed that we were extremely early. Stuck in the purgatory of the waiting area outside the terminal, we polished off of dinner in a nondescript diner, eagerly biding our time. After a period of time, we passed through security and boarded our red-eye to Santa Marta by way of Bogota.\n   Our Journey to the Lost City    Arrival Having gathered a little bit of sleep on the plane, we were in Santa Marta, and it was a glorious radiant morning. Into a taxi bound for Hotel Tayromar we went. After a drive through this unfamiliar country and a conversation utilizing my rusty Spanish, we arrived. With the day free and our bags stowed at the hotel, fighting the need for sleep, we took to the streets. Cafe Ikaro was adjacent, thus it was the next logical stop for coffee and a bite to eat. We snapped some excellent photos following an afternoon rainstorm.\n   Downtown Santa Marta    When we roused from an early evening nap, a steak and ceviche dinner followed. We had introductions and some time to get familiar with the group who would enter our lives for the next five days. After conversing with our guides and fellow travelers, we turned in, prepared for the journey ahead.\nDay 1 - Santa Marta to Wiwa Camp At dawn our group hopped into two all wheel drive vehicles. The beginning of a one and a half hour winding ride up into the foothills of la Sierra Nevada de Santa Marta. Breakfast was served in a village by the name of Machete Pelao. One of our guides explained the namesake, people there invariably had to maintain a machete at the ready. The substantial food and better hospitality after the Colombian fashion set us up for a good day of walking.\nOnce breakfast had settled drove a bit further, lurching to a halt at the trail-head. The pavement ended. The dust from our rides driving away cemented the finality of the fact that we would devote the next five days to being afoot. The beginning of the day was invested walking on a dirt road that was mixed use, though the lone vehicles we observed were motorcycles. The road wound up farther into the mountains, through cattle pasture and coffee fields that had until perhaps a decade ago been under cultivation for cocaine and cannabis. The conversion to coffee, cattle and tourism represented a turn for the better. During the midday heat we stopped to have a break at a roadside refreshment stand and I observed the first local arachnid of the trip.\n   Say cheese    Our path continued, traversing a few more miles in the sweltering heat of the exposed pasture. The relief was palpable when we reached the border of Wiwa and Kogi lands. This zone was under permacultural cultivation and the forest canopy was in tact. The perceived and effective temperature of the August sun in Colombia was much reduced. We started to pass creeks, rivers and springs unused for intensive agriculture. Our indigenous guide drank directly from the springs. Though our minds still felt a need to pass the water through chemical and mechanical purification, perhaps out of fear of the unknown. We included a few stops along the way to consume succulent fruits, which aided me immensely.\nTowards the waning of the day, we began passing by Kogi settlements. Most of the tribe still lives in the traditional fashion. It was explained to us that the elect and the Mamos are the solitary ones burdened by interacting with the external world. Utilizing unnatural or composite materials to deliver opportunity to the community and export their ideas. We got a glimpse into a reflection what life in precolonial Colombia might have been like. At this point there were no more motorcycles or cars, all transit was powered by human or mule.\n   Kogi Village on the 1st day    Some time later once the hinterlands of dusk were playing upon the sky, we struck Wiwa camp, our home for the night. There was a natural pool in the Buritaca River nearby, and we all experienced a much needed dip in the cool mountain waters. Once we had devoured a hearty dinner prepared by our trail chef discovered stories and myths regaled by our Wiwa guide and translated to us in English by our jovial Colombian guide. After listening to the tales and enjoying a cold beer, it was time to retire. At Wiwa Camp beds and hammocks were provided. However, my wife and had brought our own two person hammock, that would prove a sensible choice. We suspended it and slept like the dead.\nDay 2 - Wiwa Camp to Paraiso The morning arrived with stiffness present. We had mistakenly packed more then needed, being used to unsupported hikes. We shook down our packs and took down our shelter. We took the opportunity to drop off many bits of unneeded gear in a lockup at this camp, to come back for it in a few days. Feeling heartened by a lighter load and a solid breakfast we embraced the day.\n   A spectacular waterfall about 60ft high    There was an optional trip to a nearby waterfall which a subset of the group, us included, partook of. We all swam, which at least for me helped soothe the ache of my neophyte trail legs. Then our guide suggested standing under the waterfall, which I withstood the fury of, a backcountry massage. After some relaxation we left to reunite with the remnant who had remained at camp and resume our journey. On the way out we had the privilege of observing a traditional greeting ritual.\n   Traditional Wiwa greeting ceremony    After that we hit the trail for a shorter day by mileage, but a harder one in terms of elevation change and terrain. After some time following the river, we had an opportunity to take a swim. The next leg of the journey was described to the group as two hours up, one hour down and two hours up. Given that, we all decided to stop and swim. Afterwards we resumed our journey and went uphill. Then we kept going uphill.\nOnce we got to the top of the first mountain, a few hours later, we stopped for some delicious Colombian green oranges and a rest. Once we were all sufficiently recovered our traversal of the Sierra resumed in earnest. Passing, and almost being run over by mules was constant enjoyment. We pushed to Teyuna Paraiso Camp and arrived in early afternoon, beating a rainstorm by a few minutes. After this small blessing we ate lunch, some of us swam again or soaked sore feet in the ever-colder Buritaca.\n   The Buritaca River, our stalwart guide    I had lugged a deck of cards against humanity all the way through the jungle. It was a boon as it kept us busy when we were under the roof, dodging the foul weather. Playing with the locals and people from all around the world was extremely interesting, if a strain for our bilingual guide, who translated it all. After the game we had a scrumptious dinner prepared by our awesome camp chefs, once again.\nOur Wiwa guide explained more about his culture and the ritual items such as the porporo, bag and coca leaves. He told us various stories of the Wiwa verbal tradition. Then our Colombian guide told us more about the contemporary history and rediscovery of Teyuna by looters in the nineteen seventies. Apparently the looters had seen the error of their ways and some of them are now involved in running the treks. After enjoying some peaceful time, we all turned in - ready to see the Lost City.\nDay 3 - The Lost City At about four o\u0026rsquo;clock, before the sun had even thought of rising, we awoke. Our culinary companions miraculously had breakfast served by four thirty. Afterwards, we hurriedly stowed our packs and departed for the Lost City, carrying only water bottles. Leaving so early made us the first group out of camp and the first to arrive at Ciudad Perdida. The sun was rising by the time we reached the crossing of the Buritaca.\nThe stairs were rediscovered quite by accident. You could pass by them on the riverbank without knowing they were there. Even now the are shrouded in the deep green of the forest. We crossed the swiftly flowing river on the upstream side of a rope to prevent us from taking an unwanted ride downstream. After getting shoes back on wet feet, we started to climb. All the way up one thousand two hundred steps, made with narrow stones twelve centuries ago.\n   The entrance to Teyuna, main axis    When we reached the top, we made an offering of coca leaves to the ancestors guarding the city. Then, our guides told us all about the various features of the main axis, which is the central excavated portion. Foolishly I did not take notes or make journal entries, and my memories three quarters of a year later are wanting. One sharp thing in my memory is the map of all the waterways and trails in the Sierra, carved centuries ago. The fact that most of these sites and trails are so remote is intriguing. It seems likely to me that there is still much to be discovered about the precolonial civilization in la Sierra Nevada de Santa Marta.\n   A map of la Sierra Nevada de Santa Marta, made centuries ago    After climbing a few hundred more steps, those of us with the energy climbed another set, and then another. From the rarefied heights we had a birds eye view of a mostly empty main axis. The scale is not done justice by the pictures. What is pictured here is perhaps five percent of the entire settlement. That settlement further extends under the jungle not as of yet excavated in all directions.\n   The main axis of Teyuna, the lone soldier gives an idea of scale    This was where the nobility and priesthood lived for centuries, a city that would fill with peasantry from the countryside during rituals. Each generation building on the houses of the previous. All of the rings are foundations of these multi-generational houses and ceremonial buildings. The wood bodies of which are long since gone. Ever September the whole city still closes for these rituals. Our guide morbidly hinted that they used to involve human sacrifice.\n   Our group on the main axis of Ciudad Perdida    After the main axis we made wishes on a ceremonial stone that had been there for ten or more centuries. Our group had the unique chance to see a newly uncovered area. A part of the city where craftspeople would produce refined goods. Some of the stone tooling was still present. This was where all the metallurgy that had attracted the treasure hunters in the seventies took place.\n   We had an exclusive look at the newly excavated workshop area    We walked by the home of the Mamo, who is caretaker of the city. Passing through his gardens provided an image of what the houses on top of those foundations might have once looked like. Then it was time for the long climb down the steep and narrow stairway. Once down the stairway, after stopping at Paraiso to pick up our bags and eat, it was time for the long walk back to Wiwa Camp.\nOn the way back to Wiwa Camp there was a torrential downpour. The mountains turned into mudslides and the rivers swelled up like angry serpents. We were in a race against the weather. Sometimes it felt like the rivers would sweep you away during a crossing. Luckily we all made it in without any injury. Dinner and more stories about the locale and indigenous beliefs ensued. Then we went to sleep for what must have been a solid twelve hours.\nDay 4 - Wiwa Camp to Vista Hermosa We left Wiwa Camp after a hearty breakfast and retraced our path back along the way we had come in day prior. Although the day was balmy, it is amazing how much I, at least, had adapted to the heat. The vistas were as stunning as they were on the way out. I enjoyed the way back more as I had my trail legs and was able to expend that surplus energy in admiration of the scenery. The trail was shepherding us back towards the outside world.\nWe stopped for a fruit break at Adán Camp. There were artifacts there that had been looted from the Lost City when it was initially discovered. This was fascinating to see, but definitely highlighted the exploitation which had occurred there. After eating and catching our wind, some of us stopped to swim. The rest of us continued uphill to Ricardito Camp.\nOn the trip up the hill, we encountered the first motorcycles and telephone signal in days. We were coming back to the world, and I was not completely certain that was something I desired. When we arrived, we enjoyed another fantastic lunch. Then we enjoyed an afternoon of sitting around and relaxing. That was something I was certainly ready for.\n   The views from Ricardito Camp, known locally as Vista Hermosa    Once we had decompressed and bathed in the views, yet another extraordinary dinner came and went. Then, for the first time on the trip, we had a campfire. We sat around the fire and talked about the Wiwa creation myths. Later on we sang songs in native tongues, Spanish and English, reflecting on the journey so far. Slowly members of the group drifted off and then we turned in, ready for the final day of walking yet to come.\nDay 5 - Vista Hermosa to Gotsezhi Village    Rolling pasture on the way to Gotsezhi Village    From Ricardito Camp we had a unique opportunity to visit Gotsezhi Village, which in an indigenous Wiwa community. Walking through the pastures on our way there we were out in the open under the tropical sun. Luckily we had an early start and reached our destination before the heat of the day. When we arrived at Gotsezhi Village we had the opportunity to enjoy more traditional cuisine prepared by the Wiwa people and shop for goods produced locally by the Wiwa.\nOnce we took that in and took another swim in another gorgeous swimming hole, it was time to return to Santa Marta. We all boarded our 4x4 livery for a two hour ride. The roads were some of the roughest I have ever been on. Unexpectedly the air conditioning in the vehicles felt unbearably cold. It is amazing how the body changes after adjusting to sleeping under the stars in the tropical heat for a week.\nOn return to Santa Marta, we all had some time to settle in, then went out on the town for a final dinner. The dinner was at a local restaurant and we were treated to the famous Colombian hospitality. Afterwards, a subset of the group went out dancing to experience the Santa Marta nightlife, us included. This was our last night in Colombia, and so there was reason to resist the strong urge to sleep.\nDeparture The next morning, being just two again, we bid farewell to the hotel and went out for breakfast at a cafe. Then we hopped in a taxi, feeling nostalgic for our time in the jungle and headed back to our own concrete jungle.\n","summary":"The Trip On a Saturday evening in August of 2018 my other half and I and left the Upper East Side of Manhattan in a taxi bound for JFK. Excitement and apprehension were in the air as we were eager to leave, but about to commend our kids to my mother in law for a week. We had, and have not since, been away for so long from our children. After arriving, we were informed that we were extremely early.","tags":["ciudad perdida","lost city","teyuna","travel","santa marta","nature","history"],"title":"Hiking to Ciudad Perdida","url":"/blog/hiking-to-ciudad-perdida-the-lost-city/"},{"categories":null,"contents":"Ceph OSD performance characteristics are one of the most important considerations when deploying a RADOS (Replicated Asynchronous Distributed Object Storage) cluster. Ceph is an open source project for scale out storage based on the CRUSH algorithm. An OSD is an \u0026ldquo;Object Storage Daemon\u0026rdquo;, which represents a journaling partition and a data storage partition in the Filestore backend implementation. An OSD is, in a broader sense where Ceph stores objects which hash to a specific placement group (PG). A placement group is a hash bucket in a general computer science sense. This article explores the performance characteristics and features of various Ceph OSD backends and filesystems. The content of this article was prepared for and presented at the April 12th, 2016 Ceph ATL meetup.\nCeph OSD Performance Optimal Ceph OSD performance can reduce the capital expense and operational expense of meeting deployment requirements for a Ceph storage cluster. There are many considerations and best practices when deploying a Ceph/RADOS cluster which can enhance performance and stability. Many of those, such as kernel optimizations, network stack optimizations, choice of hardware and Ceph tuning parameters are outside the scope of this article. For those interested in other performance enhancement vectors for Ceph deployments, some were covered at the Ceph ATL Kick-Off Meetup, and many can be found in the Red Hat/Supermicro Ceph reference architecture document. However, perhaps obviously, the interface to backing storage block devices is integral in determining the performance of a RADOS cluster. The most widely used deployment is with the OSD filestore backend and the XFS filesystem. There are interesting developments in Ceph Jewel, namely the bluestore backend which may change that.\nCeph OSD Backends As of the Ceph Jewel release there will be multiple backends which can be used for Ceph OSDs. This article covers filestore, bluestore and memstore.\nFilestore At present filestore is the de-facto backend for production Ceph clusters. With the filestore backend, Ceph writes objects as files on top of a POSIX filesystem such as XFS, BTRFS or EXT4. With the filestore backend a OSD is composed of an un-formatted journal partition and an OSD data partition.\nOne of the largest drawbacks with the OSD filestore backend is the fact that all data is written twice, through the journal and then to the backing data partition. This essentially cuts the write performance of an OSD with co-located journal and data in twain. This has resulted in many deployments using dedicated solid state block devices split up into multiple partitions for journals. Many deployments use a 4:1 or 5:1 ratio for journals to SSD an disk. This requires the use of additional drive bays and increases the cost to performance ratio significantly.\nFilestore is the only backend bench-marked in this article.\nFilesystems The Ceph filestore OSD backend supports XFS, BTRFS and EXT4 filesystems. The documentation presently recommends XFS for use in production, and BTRFS for testing and development environments. Below is a comparison of Ceph OSD performance for these three filesystems. But before going into Ceph OSD performance, a feature comparison is useful.\nXFS XFS is used at present in many production Ceph deployments. XFS was developed for Silicon Graphics, and is a mature and stable filesystem. The filestore/XFS combination is well tested, stable and great for use today. There are some tuning parameters which can be used during filesystem creation and mounting as an OSD data partition which can be used to improve Ceph OSD performance. These parameters are included in the Ceph configuration on the github page for these tests.\nBTRFS BTRFS is a copy-on-write filesystem. It supports file creation timestamps and checksums that verify metadata integrity, so it can detect bad copies of data and fix them with good copies. BTRFS very interestingly supports transparent LZO and GZIP compression among and other features. While the performance of compression on BTRFS will not be covered in this article, the use in a large scale storage cluster is obvious. The BTRFS community also aims to provide fsck, deduplication, and data encryption support. BTRFS is not recommended at this time for production use with Ceph, however according to BTRFS developers, it is no longer experimental.\nEXT4 EXT4 is a solid, battle tested filesystem. However, with a maximum size of 16TB, it is not exactly future proof (considering that Samsung has already released a 16TB drive). It is production ready for use as a Ceph filestore filesystem.\nBluestore Bluestore is set to release for experimental use in Jewel. The benefits of Bluestore are a direct to block OSD, without filesystem overhead or the need for a \u0026ldquo;double-write\u0026rdquo; penalty (associated with the filestore journal). Bluestore utilizes RocksDB, which stores object metadata, a write ahead log, Ceph omap data and allocator metadata. Bluestore can have 2-3 partitions per, one for RocksDB, one for RocksDB WAL and one for OSD data (un-formatted - direct to block). Due to time constraints on the presentation, and the lack of a Ceph Jewel build for Fedora 23. I may follow this up with a comparison of the best performing filestore backend on RHEL7 and Ceph bluestore. RocksDB and RocksDB WAL can be placed on the same partition. For the tests below both RocksDB and OSD data were colocated on the same physical disk for an apples-to-apples comparison with a co-located filestore backend. A great in depth explanation of the OSD bluestore backend is available here.\nMEMSTORE Memstore is an available backend for Ceph. However, it should never be used in production due to the obvious volatility of memory. Steps for enabling memstore can be found here. Due to the fact that memstore is not a serious backend for production use, no performance tests were run with it.\nCeph OSD PERFORMANCE TEST METHOD This section covers how the performance tests in this article were executed, the environment and overall method. Since this article is strictly about OSD backends and filesystems, all tests were executed on a single machine to eliminate network related variance. Journal partitions (for the filestore backend) were co-located on the same physical disks as the data partitions.\nENVIRONMENT All performance tests in this article were performed using Ceph Hammer. The specifications for the test machine are as follows:\n CPU: Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz RAM: 32GB DDR4 2133 MHz OSDs: 5 x Seagate ST2000DM001 2TB OS: Fedora 23 OS Drive: 2 x Samsung 850 Pro SSD (BTRFS RAID1)  The OSD Performance Test Rig[/caption] The test environment was my personal desktop system. The OSD hard drives are consumer grade Seagate drives. Block device/drive type has a huge impact on the performance of a Ceph storage cluster, these 7200 RPM SATA III drives were used for all tests in this article. To more on how to test the raw performance characteristics of a physical drive for use as a OSD journal, data or co-located journal/data device see this github repo. This test cluster was a single monitor, single node \u0026ldquo;cluster\u0026rdquo;. The SATA controller was on-board ASM1062 and nothing fancy like an LSI-2308. There was a PCIe \u0026ldquo;cache tier\u0026rdquo; present with 2 M2 form factor SSDs as OSDs, although those were unused in these tests. The test machine was running kernel 4.3.3-300.\nCONFIGURATIONS The Ceph configurations filesystem tested are available on this github repository. All tests were performed with 3 replica storage pools.\nTools The build-in rados bench command was used for all performance metrics in this article.\nCeph OSD Performance Test Results Note: Take the random read speeds with a grain of salt. Even with running echo 3 \u0026gt; /proc/sys/vm/drop_caches in between benchmarks, randomly read objects may have already been stored in memory.\n* XFS: * 4 MB * Write: * IOPS: 19.17 * BW: 76.516 MB/s * Latency: 0.835348s * Read: * IOPS: 118.38 * BW: 473.466MB/s * Latency: 0.134731s * 4 KB * Write: * IOPS: 203.124 * BW: 0.790MB/s * Latency: 0.0789896s * Read: * IOPS: 209.33 * BW: 0.812MB/s * Latency: 0.076963s Conclusion The Filestore/XFS deployment scenario may be the stable way to go for production Ceph clusters at the present. BTRFS/Filestore may be the most feature rich. However, with the development of Bluestore this may change in the future.\n","summary":"Ceph OSD performance characteristics are one of the most important considerations when deploying a RADOS (Replicated Asynchronous Distributed Object Storage) cluster. Ceph is an open source project for scale out storage based on the CRUSH algorithm. An OSD is an \u0026ldquo;Object Storage Daemon\u0026rdquo;, which represents a journaling partition and a data storage partition in the Filestore backend implementation. An OSD is, in a broader sense where Ceph stores objects which hash to a specific placement group (PG).","tags":["atlanta","bluestore","btrfs","ceph","storage","xfs"],"title":"Ceph OSD Performance: Backends and Filesystems","url":"/blog/ceph-osd-performance-backends-and-filesystems/"},{"categories":null,"contents":"In August 2019 I became interested in reading the great books of Western Civilization. This interest started whilst considering, at length, the public school system in the United States. Both in consideration of my mixed experiences with it, and in planning for my children’s educations. I have always been an avid reader. However, I haven’t read many of, or studied in great depth, the foundational works of the West. In my research I stumbled upon Online Great Books via the Art of Manliness Podcast.\nThis started me on a journey of reading deeply into the western canon as outlined in How to Read a Book by Mortimer Adler. For the time being, I am using Online great books, for a community to discuss these works with. This series contains my musings on the works as I read them and my eventualy contain important resources and study methods.\nMy Musings on the Great Books\n","summary":"In August 2019 I became interested in reading the great books of Western Civilization. This interest started whilst considering, at length, the public school system in the United States. Both in consideration of my mixed experiences with it, and in planning for my children’s educations. I have always been an avid reader. However, I haven’t read many of, or studied in great depth, the foundational works of the West. In my research I stumbled upon Online Great Books via the Art of Manliness Podcast.","tags":["literature","great books","philosophy","duty"],"title":"A Study of the Great Books","url":"/blog/a-study-of-the-great-books/"},{"categories":null,"contents":"About eight years back, I wrote up a program for and designed a bipedal robot (I coined it the SimpleBiped) based on the Arduino Micro board. This article is an outline of what I did, mainly for reference if I revisit this later, but hopefully its useful to you as well. I designed a bipedal robot wh That code looks something like this:ich uses ultrasonic sensors for object detection, and some basic logic for navigation as well as some primitives for servo control. The robot had 5 servos as defined in the code below. Here is a SimpleBiped hardware spreadsheet on what I used for the build. The body parts were found on Thingiverse, modified in blender and printed with an old thing-o-matic.\nThat printer was awesome at the time and definitely served it’s purpose. It unfortunately no longer works (going to make some iteration of the Prusa eventually). I don’t have pictures of the robot and my last iteration is in storage in NY right now (I am in Georgia). I will post the blender parts up some time on my Thingiverse (all that is there now is the enclosure for the Arduino micro I integrated into the body) and link them here. Once (and if) I get started on this I’ll post circuit diagrams as well. At first the robot would walk, stop, scan for objects - then make a choice and continue walking.\nThe code can be found on github. I did try to add an RTOS. Unfortunately I never got this working smoothly and the robot would jitter too much while walking due to the interrupts. Mutexs and Semaphores are tough to get right, I’d love to revisit this project at a later time (once I have a working 3D printer for body components again). Feel free to use this code (but not the name SimpleBiped) in your projects under a GNU/GPL liscence, or (and I would appreciate it), tell me where I went wrong with my mutexs!\nIf and when I do revisit this project I will definitely post further updates here.\n","summary":"About eight years back, I wrote up a program for and designed a bipedal robot (I coined it the SimpleBiped) based on the Arduino Micro board. This article is an outline of what I did, mainly for reference if I revisit this later, but hopefully its useful to you as well. I designed a bipedal robot wh That code looks something like this:ich uses ultrasonic sensors for object detection, and some basic logic for navigation as well as some primitives for servo control.","tags":["arduino","robot","c","object detection","rtos"],"title":"A SimpleBiped with an RTOS on an Arduino Mini","url":"/blog/a-simplebiped-with-an-rtos-on-an-arduino-mini/"},{"categories":null,"contents":"The best way to setup OpenVPN on Ubuntu, like many other things, is to script it. This way it\u0026rsquo;s easier to create uniform deployment across larger networks. So, this is how you setup OpenVPN on Ubuntu the easy way - this neat little script makes installing OpenVPN on an Ubuntu VPS simple:\nGo to your home directory:\ncd ~ Then create a file by running this command:\ncat \u0026gt; openvpn.sh #!/usr/bin/env bash # # Functions ok() { echo -e \u0026#39;\\e[32m\u0026#39;$1\u0026#39;\\e[m\u0026#39;; } die() { echo -e \u0026#39;\\e[1;31m\u0026#39;$1\u0026#39;\\e[m\u0026#39;; exit 1; } # Sanity check if [[ $(id -g) != \u0026#34;0\u0026#34; ]] ; then die \u0026#34;❯❯❯ Script must be run as root.\u0026#34; fi if [[ ! -e /dev/net/tun ]] ; then die \u0026#34;❯❯❯ TUN/TAP device is not available.\u0026#34; fi dpkg -l openvpn \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 if [[ $? -eq 0 ]]; then die \u0026#34;❯❯❯ OpenVPN is already installed.\u0026#34; fi # Install openvpn ok \u0026#34;❯❯❯ apt-get update\u0026#34; apt-get update -q \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 ok \u0026#34;❯❯❯ apt-get install openvpn curl openssl\u0026#34; apt-get install -qy openvpn curl \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 # IP Address SERVER_IP=$(curl ipv4.icanhazip.com) if [[ -z \u0026#34;${SERVER_IP}\u0026#34; ]]; then SERVER_IP=$(ip a | awk -F\u0026#34;[ /]+\u0026#34; \u0026#39;/global/ \u0026amp;\u0026amp; !/127.0/ {print $3; exit}\u0026#39;) fi # Generate CA Config ok \u0026#34;❯❯❯ Generating CA Config\u0026#34; openssl dhparam -out /etc/openvpn/dh.pem 2048 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 openssl genrsa -out /etc/openvpn/ca-key.pem 2048 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 chmod 600 /etc/openvpn/ca-key.pem openssl req -new -key /etc/openvpn/ca-key.pem -out /etc/openvpn/ca-csr.pem -subj /CN=OpenVPN-CA/ \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 openssl x509 -req -in /etc/openvpn/ca-csr.pem -out /etc/openvpn/ca.pem -signkey /etc/openvpn/ca-key.pem -days 365 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo 01 \u0026gt; /etc/openvpn/ca.srl # Generate Server Config ok \u0026#34;❯❯❯ Generating Server Config\u0026#34; openssl genrsa -out /etc/openvpn/server-key.pem 2048 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 chmod 600 /etc/openvpn/server-key.pem openssl req -new -key /etc/openvpn/server-key.pem -out /etc/openvpn/server-csr.pem -subj /CN=OpenVPN/ \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 openssl x509 -req -in /etc/openvpn/server-csr.pem -out /etc/openvpn/server-cert.pem -CA /etc/openvpn/ca.pem -CAkey /etc/openvpn/ca-key.pem -days 365 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 cat \u0026gt; /etc/openvpn/udp1194.conf \u0026lt; /dev/null 2\u0026gt;\u0026amp;1 chmod 600 /etc/openvpn/client-key.pem openssl req -new -key /etc/openvpn/client-key.pem -out /etc/openvpn/client-csr.pem -subj /CN=OpenVPN-Client/ \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 openssl x509 -req -in /etc/openvpn/client-csr.pem -out /etc/openvpn/client-cert.pem -CA /etc/openvpn/ca.pem -CAkey /etc/openvpn/ca-key.pem -days 36525 \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 cat \u0026gt; /etc/openvpn/client.ovpn \u0026lt; $(cat /etc/openvpn/client-key.pem) $(cat /etc/openvpn/client-cert.pem) $(cat /etc/openvpn/ca.pem) EOF # Iptables if [[ ! -f /proc/user_beancounters ]]; then N_INT = $(ip a |awk -v sip=\u0026#34;$SERVER_IP\u0026#34; \u0026#39;$0 ~ sip { print $7}\u0026#39;) iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o $N_INT -j MASQUERADE else iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -j SNAT --to-source $SERVER_IP fi iptables-save \u0026gt; /etc/iptables.conf cat \u0026gt; /etc/network/if-up.d/iptables \u0026lt; /proc/sys/net/ipv4/ip_forward # Restart Service ok \u0026#34;❯❯❯ service openvpn restart\u0026#34; service openvpn restart \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 ok \u0026#34;❯❯❯ Your client config is available at /etc/openvpn/client.ovpn\u0026#34; ok \u0026#34;❯❯❯ All done!\u0026#34;\u0026lt;/pre\u0026gt; Press CTRL+D to save. Then:\nchmod 755 openvpn.sh This simple script will got OpenVPN installed and working on your VM or box easily. OpenVPN is a great way to connect to a work network, remain private, and encrypt your endpoint.\nIn just a few seconds you are all set, the script will automatically install OpenVPN and all the necessary dependencies, configure, and add a new user. Then just connect via SFTP and download the files to connect. Place them in the OpenVPN config directory on Windows or setup the values to match on a linux desktop.\nOpenVPN is a very secure tunnel and I highly recommend it. I get near native speed running OpenVPN on a 512MB RAM Ubuntu 14.04 VM.\n","summary":"The best way to setup OpenVPN on Ubuntu, like many other things, is to script it. This way it\u0026rsquo;s easier to create uniform deployment across larger networks. So, this is how you setup OpenVPN on Ubuntu the easy way - this neat little script makes installing OpenVPN on an Ubuntu VPS simple:\nGo to your home directory:\ncd ~ Then create a file by running this command:\ncat \u0026gt; openvpn.sh #!","tags":["linux","openvpn","security","ssl","tunnel","ubuntu"],"title":"Setup OpenVPN on Ubuntu the Easy Way","url":"/blog/setup-openvpn-on-ubuntu-the-easy-way/"},{"categories":null,"contents":"A Ceph cluster on Raspberry Pi is an awesome way to create a RADOS home storage solution (NAS) that is highly redundant and low power usage. It\u0026rsquo;s also a low cost way to get into Ceph, which may or may not be the future of storage (software defined storage definitely is as a whole). Ceph on ARM is an interesting idea in and of itself. I built one of these as a development environment (playground) for home. It can be done on a relatively small budget. Since this was a spur of the moment idea, I purchased everything locally. I opted for the Raspberry Pi 2 B (for the 4 cores and 1GB of RAM). I\u0026rsquo;d really recommend going with the Pi 2 B, so you have one core and 256MB RAM for each USB port (potential OSD). In this guide I will outline the parts, software I used and some options that you can use for achieving better performance. This guide assumes you have access to a Linux PC with an SD card reader. It also assumes you have a working knowledge of Linux in general and a passing familiarity with Ceph.\nParts Although I will explain many options in this guide, this is the minimum you will need to get a cluster up and running, this list assumes 3 Pi nodes.\n 3 x 3ft Cat6 Cables 3 x Raspberry Pi 2 B 3 x Raspberry Pi 2 B Case 3 x 2 Amp Micro USB Power Supply 3 empty ports on a gigabit router 3 x Class 10 MicroSD (16GB or more) for OS drive 3-12 x USB 2.0 Flash Drives (at least 32GB, better drive for better performance)  I used 3 x 64GB flash drives, 3 x 32GB MicroSD and existing ports on my router. My cost came in at about $250. You can add to this list based on what you add to your setup throughout the guide, but this is pretty much the minimum for a fully functional Ceph cluster.\nOperating System Raspbian. The testing repository for Raspbian has the many packages of Ceph 0.80.9 and dependencies pre-compiled. Everything you\u0026rsquo;ll need for this tutorial and is the \u0026ldquo;de facto\u0026rdquo; OS of choice for flexibility on Raspberry Pi. You can download the Raspbian image here: Raspbian Download. Once you have the image, you\u0026rsquo;ll want to put it on an SD card. For this application I recommend using at least a 16GB MicroSD card (Class 10 preferably - OS drive speed matters for Ceph monitor processes). To transfer the image on Linux, you can use DD. run the lsblk command to display your devices once you\u0026rsquo;ve inserted the card into your card reader. Then you can use dd to transfer the image to your SD. The command below assumes the image name is raspbian-wheezy.img and that it lives in your present working directory. The above command also assumes that your SD card is located at /dev/mmcblk0 adjust these accordingly and make sure that your SD card doesn\u0026rsquo;t contain anything important and is empty.\nsudo dd bs=4M if=raspbian-wheezy.img of=/dev/mmcblk0\nThis command will take a few minutes to complete. Once it does run sync to flush all cache to disk and make sure it is safe to remove the device. You\u0026rsquo;ll then boot up into Raspbian, re-size the image to the full size of your MicroSD, set a memorable password, overclock if you want. Once this is done there are a few modifications to make. We\u0026rsquo;ll get into this in the installation section below. I don\u0026rsquo;t recommend using too large of a MicroSD as later in this tutorial we will image the whole OS from our first MicroSD for deployment to our other Pi nodes.\nHardware Limitations The first limitation to consider is overall storage space. Ceph OSD processes require roughly 1MB of RAM per GB of storage. Since we are co-locating monitor processes the effective storage limitation is 512GB per Pi 2 B (4 x 128GB sticks) RAW (before Ceph replication or erasure coding overhead). Network speed is also a factor as discussed later in document. You will hit network speed limitations before you hit the speed limitations of the Pi 2 B\u0026rsquo;s single USB 2.0 bus (480Mbit).\nNetwork In this setup I used empty ports on my router. I run a local DNS server on my home router and use static assignments for local DNS. You may want to consider just using a flat 5 or 8 port (depending on number of nodes you plan to have) gigabit switch for the cluster network and WiPi modules for the public (connected to your router via WiFi). The nice thing about using a flat layer 2 switch is that if all the Pi nodes are in the same subnet, you don\u0026rsquo;t have to worry about a gateway and it also keeps the cost down (compared to using router ports) while reducing the network overhead (for Ceph replication) on your home network. Using a dedicated switch for the cluster network will also increase your cluster performance, especially considering the 100Mbit limitations of the Pi 2 B\u0026rsquo;s network port. By using a BGN Dongle for Pi and a dedicated switch for the cluster network, you will get a speedier cluster. This will use one of your 4 USB ports and thus, you will get one less OSD per Pi. Keep in mind, depending on if you use replication or erasure coding private traffic can be 1-X times greater then client IO (X being 3 in a standard replication profile) if that matters for your application. Of course this is all optional and for additional \u0026ldquo;clustery goodness\u0026rdquo;. It really depends on budget, usage - etcetera.\nObject Storage Daemons In this guide, I co-located OSD journals on the OSD drives. For better performance, you can use a faster USB like the SanDisk Extreme 3.0 (keep in mind that you\u0026rsquo;ll be limited by the 60MB/s speed of USB 2.0). Using a dedicated (faster) journal drive will yield much better performance. But you don\u0026rsquo;t really need to worry about it unless you are using multiple networks as outlined above. If you are not, 4 decent USB sticks will saturate your 100Mbit NIC per node. There is a lot more to learn about Ceph architecture that I cover in this article and I highly recommend you do so here.\nOSD Filesystem XFS is the default in Ceph Firefly. I prefer BTRFS as an OSD filesystem for multi-fold reasons and I use it in this tutorial.\nInstallation Assuming you have setup your network and operating system - have 3 nodes and the hardware you want to use - we can begin. The first thing to do is wire up power and network as you see fit. After that, you\u0026rsquo;ll want to run through the initial raspi-config on what will become your admin node. Then it\u0026rsquo;s time to make some changes. Once your admin node is booted and configured, you have to edit /etc/apt/sources.list . Raspbian Wheezy has archaic versions of Ceph in the main repository, but the latest firefly version in the testing repository. Before we delve into this, I find it useful to install some basic tools and requirements. Connect via SSH or directly to terminal and issue this command from the Pi:\nsudo apt-get install vim screen htop iotop btrfs-tools lsb-release gdisk\nFrom this point forward we will assume you are connecting to your Pi nodes via SSH. You\u0026rsquo;ve just installed BTRFS-tools, vim (better then vi) and some performance diagnostics tools I like. Now that we have vim it\u0026rsquo;s time to edit our sources:\nvi /etc/apt/sources.list\nYou\u0026rsquo;ll see the contents of your sources file. Which will look like this:\ndeb http://mirrordirector.raspbian.org/raspbian/ wheezy main contrib non-free rpi\nUncomment line below then \u0026lsquo;apt-get update\u0026rsquo; to enable \u0026lsquo;apt-get source\u0026rsquo; #deb-src http://archive.raspbian.org/raspbian/ wheezy main contrib non-free rpi\nModify it to look like this:\ndeb http://mirrordirector.raspbian.org/raspbian/ testing main contrib non-free rpi\nUncomment line below then \u0026lsquo;apt-get update\u0026rsquo; to enable \u0026lsquo;apt-get source\u0026rsquo; #deb-src http://archive.raspbian.org/raspbian/ testing main contrib non-free rpi\nWe\u0026rsquo;ve replaced wheezy with testing .Once this is done, then issue this command:\nsudo apt-get update\nOnce this process has completed is time to start getting the OS ready for Ceph. Everything we do in this section up to the point of imaging the OS is needed for nodes that will run Ceph. First we will create a ceph user and give it password-less sudo access. To do so issue these commands:\nssh user@ceph-server sudo useradd -d /home/ceph -m ceph sudo passwd ceph Set the password to a memorable one as it will be used on all of your nodes in this guide. Now we need to give the ceph user sudo access\necho \u0026#34;ceph ALL = (root) NOPASSWD:ALL\u0026#34; | sudo tee /etc/sudoers.d/ceph sudo chmod 0440 /etc/sudoers.d/ceph We\u0026rsquo;ll be using ceph-deploy later and it\u0026rsquo;s best to have a defult user to login as all the time. Issue this command:\nmkdir -p ~/.ssh/ Then create this file using vi:\nvi ~/.ssh/config I assume 3 nodes in this tutorial and a naming convention of piY, where Y is the node number starting from 1.\nHost pi1 Hostname pi1 User ceph Host pi2 Hostname pi2 User ceph Host pi3 Hostname pi3 User ceph Save the file and exit. As far as hostnames, you can use whatever you want of course. As I mentioned, I run local DNS and DHCP with static assignments. If you do not, you\u0026rsquo;ll need to edit /etc/hosts so that your nodes can resolve each-other. You can do this after the OS image, as each node will have a different IP. Now it\u0026rsquo;s time to install the ceph-deploy tool. Raspbian wget can be strange with HTTPS so we will ignore the certificate (do so at your own peril):\nwget --no-check-certificate -q -O- \u0026#39;https://ceph.com/git/?p=ceph.git;a=blob\\_plain;f=keys/release.asc\u0026#39; | sudo apt-key add - echo deb http://ceph.com/debian-firefly/ wheezy main | sudo tee /etc/apt/sources.list.d/ceph.list Now that we\u0026rsquo;ve added the Ceph repository, we can install ceph-deploy:\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install ceph-deploy ceph ceph-common\nSince we are installing ceph from the Raspbian repositories, we need to change the default behavior of ceph-deploy:\nsudo vi /usr/share/pyshared/ceph_deploy/hosts/debian/install.py\nChange\ndef install(distro, version_kind, version, adjust_repos): codename = distro.codename machine = distro.machine_type To\ndef install(distro, version_kind, version, adjust_repos): adjust_repos = False codename = distro.codename machine = distro.machine_type This will prevent ceph-deploy from altering repos as the Ceph armhf (Rasberry Pi\u0026rsquo;s processor type) repos are mostly empty. Finally, we should revert the contents of /etc/apt/sources.list :\nsudo vi /etc/apt/sources.list\nYou\u0026rsquo;ll see the contents of your sources file. Which will look like this:\ndeb http://mirrordirector.raspbian.org/raspbian/ testing main contrib non-free rpi # Uncomment line below then 'apt-get update' to enable 'apt-get source' #deb-src http://archive.raspbian.org/raspbian/ testing main contrib non-free rpi Modify it to look like this:\ndeb http://mirrordirector.raspbian.org/raspbian/ wheezy main contrib non-free rpi # Uncomment line below then 'apt-get update' to enable 'apt-get source' #deb-src http://archive.raspbian.org/raspbian/ wheezy main contrib non-free rpi We\u0026rsquo;ve replaced testing with wheezy .Once this is done, then issue this command:\nsudo apt-get update Kernel Tweaks We are also going to tweak some kernel parameters for better stability. To do so we will edit /etc/sysctl.conf .\nvi /etc/sysctl.conf\nAt the bottom of the file, change add the following lines:\nvm.swappiness=1 vm.min_free_kbytes = 32768 kernel.pid_max = 32768 Imaging the OS Now we have a good baseline for deploying ceph to our other Pi nodes. It\u0026rsquo;s time to stop our admin node and image the drive (MicroSD). Issue:\nsudo halt\nThen unplug power to your Pi node and remove the MicroSD. Insert the microSD in your SD adapter, then the SD adapter into your Linux PC. You\u0026rsquo;ll need at least as much free drive space on your PC as the size of the MicroSD card.Where /dev/mmcblk0 is your SD card and pi-ceph.img is your image destination, run:\nsudo dd if=/dev/mmcblk0 of=ceph-pi.img bs=4M\nThis can take a vary long time depending on the size of your SD and you can compress it with gzip or xz for long term storage (empty space compresses really well it turns out). Once the command returns, run sync to flush the cache to disk and make sure you can remove the MicroSD\nImaging Your Nodes OS Drives Now that you have a good baseline image on your PC, you are ready to crank out \u0026ldquo;Ceph-Pi\u0026rdquo; nodes - without redoing all of the above. To do so, insert a fresh MicroSD into your adapter and then PC. Then assuming ceph-pi.img is your OS image and /dev/mmcblk0 is your MicroSD card run:\nsudo dd if=ceph-pi.img of=/dev/mmcblk0 bs=4M\nRepeat this for a many nodes as you intend to deploy.\nCreate a Ceph Cluster on Raspberry Pi Insert your ceph-pi MicroSD cards into your Pi nodes and power them all on. You\u0026rsquo;ve made it this far, now it\u0026rsquo;s time to get \u0026ldquo;cephy\u0026rdquo;. Deploying with ceph-deploy is a breeze. First we need to SSH to our admin node, make sure you have setup IPs, network and /etc/hosts on all Pi nodes if you are not using local DNS and DHCP with static assignments. We need to generate and distribute an SSH key for password-less authentication between nodes. To do so run (leave the password blank):\nssh-keygen Generating public/private key pair. Enter file in which to save the key (/ceph-client/.ssh/id\\_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /ceph-client/.ssh/id\\_rsa. Your public key has been saved in /ceph-client/.ssh/id\\_rsa.pub. Now copy the key to all nodes (assuming 3 with the naming convention from above): ssh-copy-id ceph@pi1 ssh-copy-id ceph@pi2 ssh-copy-id ceph@pi3 You will be prompted for the password you created for the ceph user each time to establish initial authentication. Once that is done and you are connected to your admin node (1st node in the cluster) as the pi user you\u0026rsquo;ll want to create an admin node directory:\nmkdir -p ~/ceph-pi-cluster cd ~/ceph-pi-cluster Creating an initial Ceph Configuration We are going to create an initial Ceph configuration assuming all 3 pi nodes as monitors. If you have more, keep in mind - you always want an odd number of monitors to avoid a split-brain scenario. To to this run:\nceph-deploy new pi1 pi2 pi3\nNow there are some special tweaks that should be made for best stability and performance within the hardware limitations of the Raspberry Pi 2 B. To apply these changes we\u0026rsquo;ll need to edit the ceph.conf here on the admin node before it is distributed. To do so:\nvi ~/ceph-pi-cluster/ceph.conf\nAfter the existing lines add:\n# Disable in-memory logs debug_lockdep = 0/0 debug_context = 0/0 debug_crush = 0/0 debug_buffer = 0/0 debug_timer = 0/0 debug_filer = 0/0 debug_objecter = 0/0 debug_rados = 0/0 debug_rbd = 0/0 debug_journaler = 0/0 debug_objectcatcher = 0/0 debug_client = 0/0 debug_osd = 0/0 debug_optracker = 0/0 debug_objclass = 0/0 debug_filestore = 0/0 debug_journal = 0/0 debug_ms = 0/0 debug_monc = 0/0 debug_tp = 0/0 debug_auth = 0/0 debug_finisher = 0/0 debug_heartbeatmap = 0/0 debug_perfcounter = 0/0 debug_asok = 0/0 debug_throttle = 0/0 debug_mon = 0/0 debug_paxos = 0/0 debug_rgw = 0/0 osd heartbeat grace = 8 [mon] mon compact on start = true mon osd down out subtree_limit = host [osd] # Filesystem Optimizations osd journal size = 1024 # Performance tuning max open files = 327680 osd op threads = 2 filestore op threads = 2 #Capacity Tuning osd backfill full ratio = 0.95 mon osd nearfull ratio = 0.90 mon osd full ratio = 0.95 # Recovery tuning osd recovery max active = 1 osd recovery max single start = 1 osd max backfills = 1 osd recovery op priority = 1 # Optimize Filestore Merge and Split filestore merge threshold = 40 filestore split multiple = 8 Creating Initial Monitors Now we can deploy our spiffy ceph.conf, create our initial monitor daemons, deploy our authentication keyring and chmod it as needed. We will be deploying to all 3 nodes for the purposes of this guide:\nceph-deploy mon create-initial ceph-deploy admin pi1 pi2 pi3 for i in pi1 pi2 pi3;do ssh $i chmod 644 /etc/ceph/ceph.client.admin.keyring;done Creating OSDs (Object Storage Daemons) Ready to create some storage? I know I am. Insert your USB keys of choice into your Pi USB ports. For the purposes of this guide I will be deploying 1 OSD (USB key) per Pi node. I will also be using the BTRFS filesystem and co-locating the journals on the OSDs with a default journal size of 1GB (assuming 2 * 40MB/s throughput max and a default filestor max sync interval of 5). This value is hard coded into our ceph-pi config above. The formula is:\nosd journal size = {2 * (expected throughput * filestore max sync interval)}\nSo let\u0026rsquo;s deploy our OSDs. Once our USBs are plugged in, use lsblk to display the device locations. To make sure our drives are clean and have a GPT partition table, use the gdisk command for each OSD on each node. Assuming /dev/sda as our OSD: gdisk /dev/sda Create a new partition table, write it to disk and exit. Do this for each OSD on each node. You can craft a bash for loop if you are feeling \u0026ldquo;bashy\u0026rdquo; or programmatic. Once all OSD drives have a fresh partition table you can use ceph-deploy to create your OSDs (using BTRFS for this guide) where pi1 is our present node and /dev/sda is the OSD we are creating:\nceph-deploy osd create --fs-type btrfs pi1:/dev/sda\nRepeat this for all OSD drives on all nodes (or write a for loop). Once you\u0026rsquo;ve created at least 3 you are ready to move on.\nChecking Cluster Health Congratulations! You should have a working Ceph-Pi cluster. Trust, but verify. Get the health status of your cluster using this command:\nceph -s\nand for a less verbose output\nceph health\nWhat to do now? Use your storage cluster! Create an RBD, mount it - export NFS or CIFS. There is a lot of reading out there. Now you know how to deploy a Ceph cluster on Raspberry Pi.\n","summary":"A Ceph cluster on Raspberry Pi is an awesome way to create a RADOS home storage solution (NAS) that is highly redundant and low power usage. It\u0026rsquo;s also a low cost way to get into Ceph, which may or may not be the future of storage (software defined storage definitely is as a whole). Ceph on ARM is an interesting idea in and of itself. I built one of these as a development environment (playground) for home.","tags":["arm","ceph","rados","raspberry-pi","storage"],"title":"The Definitive Guide: Ceph Cluster on Raspberry Pi","url":"/blog/the-definitive-guide-ceph-cluster-on-raspberry-pi/"},{"categories":null,"contents":"The next step in the tutorial \u0026ldquo;Making WordPress Fly\u0026rdquo; is to configure Nginx/HHVM and install WordPress. This step has two options, configuring for single site (this article) or configuring for multisite. This tutorial will assume that you have completed the prerequisites and read the introduction (part one). It will also that you have completed both parts two and three. We will also assume that you have an Ubuntu VPS. If you don\u0026rsquo;t, you can get one at Vultr. At this point you should have your VPS secured. You should also have MariaDB, Nginx, and HHVM installed. The first step in this section is to reconnect to your VM via SSH.\nssh -p port user@you.rip.add.res\nAfter connecting to your instance, we are going to create a location to install WordPress. We are going to use /var/www/html however you can use a different directory like /var/www/html/domain-com/ if you choose to. Be aware that you will have to update the configurations to match if you choose a different directory structure. Creating the new directory is straightforward.\nsudo mkdir /var/www/html/\nWe need to install WordPress to /var/www/html . The guide for that can be found here. Once you have completed that, you are ready to move on to the next step. Configuring Nginx for Multisite WordPress with W3 Total cache is the most involved part of the \u0026ldquo;Making WordPress Fly\u0026rdquo; series. At this point we are going to assume that you have been following the series since the beginning. We are also going to assume you already have an Ubuntu VPS. So far we have secured an Ubuntu VPS, setup MariaDB, installed HHVM and installed Nginx. Now we are ready for part 4a, to configure Nginx to work with WordPress Multisite and W3 Total Cache. Part 4b is the alternative if you only need a single site WordPress install on your HHVM enhanced, WordPress optimized VPS. Choosing a multisite install definitely makes sense as you can use this VPS to host more then a few sites, even if the get significant traffic.\nThat\u0026rsquo;s all for now.\n","summary":"The next step in the tutorial \u0026ldquo;Making WordPress Fly\u0026rdquo; is to configure Nginx/HHVM and install WordPress. This step has two options, configuring for single site (this article) or configuring for multisite. This tutorial will assume that you have completed the prerequisites and read the introduction (part one). It will also that you have completed both parts two and three. We will also assume that you have an Ubuntu VPS. If you don\u0026rsquo;t, you can get one at Vultr.","tags":["nginx","wordpress","hhvm"],"title":"Configure Nginx/HHVM for WP - Making WordPress Fly","url":"/blog/configure-nginx-hhvm-for-wp-making-wordpress-fly/"},{"categories":null,"contents":"In this tutorial we will cover optimal MariaDB 10.1 setup for Ubuntu 14.04 on a VM with 2-4GB of RAM. This is part 2 of the \u0026ldquo;Make WordPress Fly\u0026rdquo; tutorial. You can find part 1 here. Part 1 covered the benefits of using HHVM, MariaDB, Nginx and Ubuntu 14.04 to run a WordPress website. In this section we\u0026rsquo;ll be digging in to MariaDB and the optimal configurations for it. This tutorial assumes you have a VM with at least 512MB of RAM, 1 Xeon Core, 10 GB HDD and Vanilla Ubuntu 14.04 installed and ideally secured. So, assuming you have your Ubuntu VPS all setup, we will proceed with the fairly straightforward process of installing MariaDB on Ubuntu 14.04. We are specifically going to be deploying MariaDB 10.1 which as discussed in part 1 has significant performance benefits over even the newest versions on MySQL. First, connect to your VM via SSH.\nssh -p port user@you.rip.add.res\nThen we\u0026rsquo;ll add the MariaDB 10.1 repository and install the prerequisites.\nsudo apt-get install software-properties-common sudo apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xcbcb082a1bb943db sudo add-apt-repository 'deb http://ftp.osuosl.org/pub/mariadb/repo/10.1/ubuntu trusty main' Once the key is imported and the repository added we will install MariaDB.\nsudo apt-get update sudo apt-get install mariadb-server During that process you will be prompted to create a root password for MariaDB. Make sure that you store it in a safe place. Consider using KeePass (or a similar utility) for test passwords, it creates strong passwords you can review later and encrypts them with a master key. Now that MariaDB is installed we need to make sure it runs on startup.\nsudo update-rc.d mysql defaults Then, run the sudo mysql_secure_installation. This will guide you through some procedures that will remove some defaults which are dangerous to use in a production environment.\nNext we will want to check that everything looks good in the my.cnf file.\nnano /etc/mysql/my.cnf It looks like this, yours should be similar, it may be a bit different as MariaDB does some system based configuration on installation.\nsocket\t= /var/run/mysqld/mysqld.sock nice\t= 0 [mysqld] # # * Basic Settings # user\t= mysql pid-file\t= /var/run/mysqld/mysqld.pid socket\t= /var/run/mysqld/mysqld.sock port\t= 3306 basedir\t= /usr datadir\t= /var/lib/mysql tmpdir\t= /tmp lc_messages_dir\t= /usr/share/mysql lc_messages\t= en_US skip-external-locking # # Instead of skip-networking the default is now to listen only on # localhost which is more compatible and is not less secure. bind-address\t= 127.0.0.1 # # * Fine Tuning # max_connections\t= 100 connect_timeout\t= 5 wait_timeout\t= 600 max_allowed_packet\t= 16M thread_cache_size = 128 sort_buffer_size\t= 4M bulk_insert_buffer_size\t= 16M tmp_table_size\t= 32M max_heap_table_size\t= 32M # # * MyISAM # # This replaces the startup script and checks MyISAM tables if needed # the first time they are touched. On error, make copy and try a repair. myisam_recover = BACKUP key_buffer_size\t= 128M #open-files-limit\t= 2000 table_open_cache\t= 400 myisam_sort_buffer_size\t= 512M concurrent_insert\t= 2 read_buffer_size\t= 2M read_rnd_buffer_size\t= 1M # # * Query Cache Configuration # # Cache only tiny result sets, so we can fit more in the query cache. query_cache_limit\t= 128K query_cache_size\t= 64M # for more write intensive setups, set to DEMAND or OFF #query_cache_type\t= DEMAND # # * Logging and Replication # # Both location gets rotated by the cronjob. # Be aware that this log type is a performance killer. # As of 5.1 you can enable the log at runtime! #general_log_file = /var/log/mysql/mysql.log #general_log = 1 # # Error logging goes to syslog due to /etc/mysql/conf.d/mysqld_safe_syslog.cnf. # # we do want to know about network errors and such log_warnings\t= 2 # # Enable the slow query log to see queries with especially long duration #slow_query_log[={0|1}] slow_query_log_file\t= /var/log/mysql/mariadb-slow.log long_query_time = 10 #log_slow_rate_limit\t= 1000 log_slow_verbosity\t= query_plan #log-queries-not-using-indexes #log_slow_admin_statements # # The following can be used as easy to replay backup logs or for replication. # note: if you are setting up a replication slave, see README.Debian about # other settings you may need to change. #server-id\t= 1 #report_host\t= master1 #auto_increment_increment = 2 #auto_increment_offset\t= 1 log_bin\t= /var/log/mysql/mariadb-bin log_bin_index\t= /var/log/mysql/mariadb-bin.index # not fab for performance, but safer #sync_binlog\t= 1 expire_logs_days\t= 10 max_binlog_size = 100M # slaves #relay_log\t= /var/log/mysql/relay-bin #relay_log_index\t= /var/log/mysql/relay-bin.index #relay_log_info_file\t= /var/log/mysql/relay-bin.info #log_slave_updates #read_only # # If applications support it, this stricter sql_mode prevents some # mistakes like inserting invalid dates etc. #sql_mode\t= NO_ENGINE_SUBSTITUTION,TRADITIONAL # # * InnoDB # # InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/. # Read the manual for more InnoDB related options. There are many! default_storage_engine\t= InnoDB # you can\u0026#39;t just change log file size, requires special procedure #innodb_log_file_size\t= 50M innodb_buffer_pool_size\t= 256M innodb_log_buffer_size\t= 8M innodb_file_per_table\t= 1 innodb_open_files\t= 400 innodb_io_capacity\t= 400 innodb_flush_method\t= O_DIRECT # # * Security Features # # Read the manual, too, if you want chroot! # chroot = /var/lib/mysql/ # # For generating SSL certificates I recommend the OpenSSL GUI \u0026#34;tinyca\u0026#34;. # # ssl-ca=/etc/mysql/cacert.pem # ssl-cert=/etc/mysql/server-cert.pem # ssl-key=/etc/mysql/server-key.pem [mysqldump] quick quote-names max_allowed_packet\t= 16M [mysql] #no-auto-rehash\t# faster start of mysql but no tab completition [isamchk] key_buffer\t= 16M # # * IMPORTANT: Additional settings that can override those from this file! # The files must end with \u0026#39;.cnf\u0026#39;, otherwise they\u0026#39;ll be ignored. # !includedir /etc/mysql/conf.d/\u0026lt;/pre\u0026gt; Performance can be tweaked a bit once we\u0026rsquo;ve had the WordPress site up and running for 24-48 hours by using mysqltuner.pl. For good measure restart the service.\nsudo service mysql restart\nThis concludes part 2 of the guide \u0026ldquo;MariaDB Setup for Ubuntu 14.04 - Make WordPress Fly\u0026rdquo;. As the rest of the guide is released links will be posted here and on all of the articles in the tutorial.\n","summary":"In this tutorial we will cover optimal MariaDB 10.1 setup for Ubuntu 14.04 on a VM with 2-4GB of RAM. This is part 2 of the \u0026ldquo;Make WordPress Fly\u0026rdquo; tutorial. You can find part 1 here. Part 1 covered the benefits of using HHVM, MariaDB, Nginx and Ubuntu 14.04 to run a WordPress website. In this section we\u0026rsquo;ll be digging in to MariaDB and the optimal configurations for it. This tutorial assumes you have a VM with at least 512MB of RAM, 1 Xeon Core, 10 GB HDD and Vanilla Ubuntu 14.","tags":["mariadb","ubuntu","wordpress"],"title":"MariaDB 10.1 Setup for Ubuntu 14.04 - Make WordPress Fly","url":"/blog/mariadb-10-1-setup-for-ubuntu-14-04-make-wordpress-fly/"},{"categories":null,"contents":"HHVM, MariaDB and Nginx Make WordPress fly (seriously). This site is running on what may the fastest possible software stack for WordPress. That stack is HHVM, MariaDB 10.1, Nginx and Ubuntu 14.04. As you are browsing this site you may notice that it is graphically intensive. It also leverages many CPU hungry plugins that would make it take 6-10 seconds to load on even good shared hosting. With this aforementioned software stack pages up to 5MB on this site still load in under a second, end-user pipe permitting. This is all happening on a VPS with 2 x 2.26Ghz cores and 2GB of RAM. Not only that but this stack can serve over 1000 2MB WordPress pages per second without losing stability:\nAB BenchMark [user@host ~]# ab -c 50 -n 5000 http://bryanapperson.com/ This is ApacheBench, Version 2.3 \u0026lt;$Revision: 655654 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking bryanapperson.com (be patient) Completed 500 requests Completed 1000 requests Completed 1500 requests Completed 2000 requests Completed 2500 requests Completed 3000 requests Completed 3500 requests Completed 4000 requests Completed 4500 requests Completed 5000 requests Finished 5000 requests Server Software: nginx Server Hostname: bryanapperson.com Server Port: 80 Document Path: / Document Length: 16138 bytes Concurrency Level: 50 Time taken for tests: 3.916 seconds Complete requests: 5000 Failed requests: 0 Write errors: 0 Total transferred: 83046606 bytes HTML transferred: 80706138 bytes Requests per second: 1276.68 \\[#/sec\\](mean) Time per request: 39.164 \\[ms\\](mean) Time per request: 0.783 \\[ms\\] (mean, across all concurrent requests) Transfer rate: 20707.77 \\[Kbytes/sec\\] received Connection Times (ms) min mean\\[+/-sd\\] median max Connect: 1 1 0.2 1 5 Processing: 12 38 8.1 37 88 Waiting: 11 37 8.1 36 87 Total: 14 39 8.1 38 89 Percentage of the requests served within a certain time (ms) 50% 38 66% 41 75% 43 80% 44 90% 49 95% 54 98% 60 99% 66 100% 89 (longest request) Why HHVM for WordPress? So you may be asking yourself, is that really possible? Yes, HHVM and WordPress work very well together. If you asked me a few days ago I might have said no. But after playing around with HHVM, also known as \u0026ldquo;Hip Hop for PHP\u0026rdquo;, it is. HHVM is Facebook\u0026rsquo;s production PHP server which has now gone open source. At this point it still has a few compatibility issues. Especially with the usual culprits like Ioncube. However it works very well with WordPress 3.9+. When combined with Nginx, MariaDB and Ubuntu \u0026ldquo;Trust Tahr\u0026rdquo; you get a pretty unbeatable platform for WordPress. Serving 200 request per second even on un-cached and heavy pages where PHP-FPM can only achieve 18 requests per second on a VM with the same resources (rendering the same un-cached pages).\nMariaDB 10.1 MariaDB provides a solid database back-end and can easily be scaled out into a Galera Cluster for larger deployments. MariaDB 10.1 outperforms MySQL 5.7.4 by a significant margin, that is why it was chosen for this stack and it proved itself in implementation. MariaDB would perform better on SSD if available, but the above results were achieved on RAID10 7200RPM SATAIII with an LSI Megaraid BBU controller (512MB Cache).\nNginx Nginx can be somewhat less intuitive to configure then Apache. However it is a beast for serving static files especially per resource usage when configured correctly. Which is mostly what it does in this stack as all PHP processing is done by HHVM. Nginx really shines in serving static files to many user concurrently with the configuration we\u0026rsquo;ll outline in the coming articles.\nUbuntu 14.04 \u0026ldquo;Trusty Tahr\u0026rdquo; Choosing Ubuntu 14.04 for this deployment made sense, because it is LTS (5 years of support) and apt-get makes it almost trivial to get all of this setup. Not to mention that Ubuntu is a stable OS (although I usually prefer CentOS/RHEL). Nginx is built into the native repos for Ubuntu 14.04 and having maintained repos for both HHVM and MariaDB with Ubuntu 14.04 makes this stack easy to update later on. If you need an Ubuntu VPS you can get one for here. Before you get started with this you will probably want to secure your Ubuntu VPS.\nWordPress 3.9.2 This series of articles will show you how to set all of this up and make it work with both a WordPress multi-site network, and a single WordPress site. I used the multi-site network with WP MU Domain Mapper and Nginx helper for ease of moving my multiple blogs and family/friends WordPress sites on to one platform. We will also be leveraging W3 Total Cache and APC (which is built in to HHVM) for Opcode caching.\nConcluding the Introduction This setup is so efficient you wouldn\u0026rsquo;t need to scale out past a single VM instance unless you were in the Alexa top 10000, so we won\u0026rsquo;t handle that in this series. In articles to follow I will layout how to build this stack and use it for Lightning fast WordPress hosting on a shoestring budget. You\u0026rsquo;ll be able to handle 50,000 page loads an hour or more on a 2GB RAM Xen VM. I will update this article with links to the upcoming tutorials.\n","summary":"HHVM, MariaDB and Nginx Make WordPress fly (seriously). This site is running on what may the fastest possible software stack for WordPress. That stack is HHVM, MariaDB 10.1, Nginx and Ubuntu 14.04. As you are browsing this site you may notice that it is graphically intensive. It also leverages many CPU hungry plugins that would make it take 6-10 seconds to load on even good shared hosting. With this aforementioned software stack pages up to 5MB on this site still load in under a second, end-user pipe permitting.","tags":["development","hhvm","mariadb","nginx","ubuntu","wordpress"],"title":"HHVM, MariaDB and Nginx Make WordPress Fly - Intro","url":"/blog/hhvm-mariadb-and-nginx-make-wordpress-fly-intro/"},{"categories":null,"contents":"When you get a new Ubuntu VPS or server there are a few things you want to make sure are taken care of right off the bat. This will optimize the security and usability of your server, providing a reliable foundation for subsequent alterations. If you need an Ubuntu VPS you can get one for .\nStep One Logging into your new VPS is the first step. You\u0026rsquo;ll need to know the public IP address of the server to begin. You\u0026rsquo;ll also need to know the password for the \u0026ldquo;root\u0026rdquo; user. Once you know those two things you are ready to login. The public IP address is e-mailed to you and you can view the credentials you chose in your client area if the VPS is hosted with Bitronic Technologies (my company). In Linux the root user an administrator with sweeping privileges. Because the root user has the power to irreversibly damage a system you shouldn\u0026rsquo;t use root for daily use. It is best to login as a user with limited privileges and then to \u0026ldquo;sudo\u0026rdquo; root when you need to in the case of Ubuntu 14.04. In this tutorial we will cover how to login as root, create an alternate user for day-to-day use, disable root login and how to login as an alternate (non-root) user, then escalate to root. As mentioned, logging in is the first step to getting started with your Ubuntu 14.04 VPS. Initially the only account is the root account. If you are using Linux connect to the server using the \u0026ldquo;ssh\u0026rdquo; command at terminal. If you are using windows consider using an SSH program like putty. For purposes of this article we will assume you are running Linux on your PC, however all of the same steps will apply either way except for initial login. If you are using Windows you will need to open putty, enter the root@pub.lic.ipa.ddr (the dotted quad public IP of your VPS) then login and enter the password. On linux just run this command in terminal:\nssh root@pub.lic.ipa.ddr\nOf course entering your public IP address in place of those letters. You will then most likely see a warning in your terminal that looks like this:\nestablished. ECDSA key fingerprint is 79:95:46:1a:ab:37:11:8e:86:54:36:38:bb:3c:fa:c0. Are you sure you want to continue connecting (yes/no)? Your computer is letting you know that it does not recognize your remote server. This is expected behaviour because it is the first time this connection is being made. However if you see this message in the future (on the same computer) it can be a sign that your server has been compromised (assuming you haven\u0026rsquo;t reinstalled the operating system). It is safe to type \u0026ldquo;yes\u0026rdquo; and accept the connection. Then you will need to enter the password for the root account.\nStep Two You may wish to change the root password to something more memorable. Make sure to use a strong password as root login is the \u0026ldquo;key to the kingdom\u0026rdquo; so to speak. A good password can be a sentence like \u0026ldquo;IlikedtheFordBroncoin1996!\u0026rdquo; or a random string. But it should ideally contain uppercase/lowercase letters, numbers and at least 1 special character. While logged in as root it is easy to change the password by typing:\npasswd\nYou will be prompted to enter and then confirm your new root password. You will not see any characters display on the screen while you are typing (this is an intentional security measure).\nStep Three The third step in preparing and securing your new Ubuntu 14.04 VPS is to add a new user. This is going to be the account you\u0026rsquo;ll use to login after the end of the tutorial, so make sure the username is memorable. For the purposes of this tutorial we are going to make the username \u0026ldquo;demo\u0026rdquo;, but you should select a more meaningful username. So to add the user just run this command:\nadduser demo\nYou will then be prompted with a few questions, starting with the password you want to use for the account. It is recommended that you choose a different password then the one you used for the root account as an added security measure. So go ahead and fill out the password, then optionally provide the other details requested. If you don\u0026rsquo;t want to fill those out you can just hit enter and Ubuntu will skip those fields.\nStep Four In step four we will provide the new user we just created in step three with the ability to escalate to root. This allows you to gain root access without having to allow remote root login, or logout and log back in as root if you do choose to leave root login over SSH enable (not recommended). In Ubuntu we use the command \u0026ldquo;sudo\u0026rdquo; to escalate to root. This allows a normal user to run a command that requires root privileges. You can also use the \u0026ldquo;sudo -i\u0026rdquo; command to assume the identity of root until you use the command \u0026ldquo;exit\u0026rdquo;. To add the new user we just created to the list of users who can use \u0026ldquo;sudo\u0026rdquo; to attain root, we need to use a command called \u0026ldquo;visudo\u0026rdquo;. This command will open a configuration file. Go ahead and type:\nvisudo\nThen scroll down until you find a section that looks similar to this:\n# User privilege specification root ALL=(ALL:ALL) ALL This may look complex to you but that line is non-consequential and we don\u0026rsquo;t need to alter it. Just add another line below it that follows the format, replacing \u0026ldquo;demo\u0026rdquo; with the username you created in step three. The result should look like this:\n# User privilege specification root ALL=(ALL:ALL) ALL demo ALL=(ALL:ALL) ALL After you have made the changes, press CTRL-X to exit visudo. You will have to type \u0026ldquo;Y\u0026rdquo; to save the file and \u0026ldquo;ENTER\u0026rdquo; to confirm the location (you can use the provided value).\nStep Five (Optional) This step is optional but is highly recommended for optimal security going forward. You can secure your server a bit more by disabling root login over SSH entirely and changing SSH to a non default port. The process is fairly straightforward. Get started by opening the sshd configuration file with a text editor as root. You can use any text editor installed on your VPS but for this tutorial we will assume you use \u0026ldquo;nano\u0026rdquo;. Nano is a fairly easy to use command line text editor for Linux. So go ahead and type this command:\nnano /etc/ssh/sshd_config\nStep Five-A (Optional) The first thing to do is to change the port that SSHD listens on. Find the line that looks like this:\nPort 22\nChange this number to something between 1025 and 65536, make sure it does not conflict with the port of another program you plan on running. This is helpful in preventing unauthorized users from trying to break into the system over SSH. If you change from the default port it adds an extra step of sniffing for them to find it and you can even add a firewall rule to block IPs for port scanning of use a tool like ConfigServer Firewall to do so. If you do change this value make sure you remember it or write it down. You will need to know this port to reconnect to your server. For the purposes of this guide we will change the port to 3333. Remember you will need to tell your ssh client to connect to port 3333 (or the port you choose) in the future. So change the line to look like this (or the port you choose):\nPort 3333\nSave the file using \u0026ldquo;CTRL-O\u0026rdquo;, \u0026ldquo;Y\u0026rdquo; then exit \u0026ldquo;CTRL-X\u0026rdquo;. Now we are going to enable Ubuntu\u0026rsquo;s UFW (Uncomplicated Firewall). Enter this command:\nsudo ufw enable\nThen we are going to make sure that port 3333 or whatever port you chose is open. Run the command:\nsudo ufw allow 3333\nReplace 3333 with whatever port you decided to use then run:\nsudo service ufw restart sudo service ssh restart Now you can reconnect via the port you chose in the future.\nStep Five-B (Optional) The next thing we can do for added security is to restrict remote root login over SSH. Run the command:\nnano /etc/ssh/sshd_config\nThen find the line that looks like this:\nPermitRootLogin yes\nChange this to:\nPermitRootLogin no\nThis is a much more secure setting as it discourages the direct brute force or the root password. This is especially effective if you use a different password for your \u0026ldquo;day-to-day\u0026rdquo; user then you do for root. If you run:\nsudo service ssh restart\nYou will no longer be able to login as root, only as the alternate user you created.\nStep Five-C (Optional) If you wanted to go one step further you could explicitly specify which users can connect to your server via SSH. This might not be needed now, but can become useful as you add additional users. Any user not on the list you are about to configure will not be able to login over SSH at all. You should use caution when configuring this as you can lock your self out of your server entirely. We are going to need to open the SSHD configuration again:\nnano /etc/ssh/sshd_config\nYou\u0026rsquo;ll have to add this line yourself as it does not exist by default. Make sure to replace \u0026ldquo;demo\u0026rdquo; with the username you created in step 3.\nAllowUsers demo\nWhen you have added the line and triple checked that the syntax and username are correct, save and close the file using \u0026ldquo;CTRL-O\u0026rdquo;, \u0026ldquo;Y\u0026rdquo; and then \u0026ldquo;CTRL-X\u0026rdquo; as we covered earlier. Then go ahead and restart SSHD again:\nsudo service ssh restart\nBefore you log out of the server, test your configuration by opening another connection using the user, password and port that you have created in this guide. If you can\u0026rsquo;t connect go back and correct your errors using the original connection (which you should not close until you are sure that your settings work). If you followed this guide the command you would use is:\nssh -p 3333 demo@ser.ver.ipa.ddr\nReplacing the port and user with the ones you chose. Then entering the password for the user you created in step 3. Remember from now on you will have to use the sudo command to run commands that need root:\nsudo your_command\nYou can now close both connections by typing exit in each window.\nWhat\u0026rsquo;s Next? You now have a fairly secure server, however you can continue further securing you server by installing fail2ban or a similar utility to help prevent brute force and port scanning attacks. Outside of that you are ready to install LAMP, LEMP or whatever other programs you want to run on your VPS! Leave me your thoughts in the comments below and check out my other Ubuntu VPS articles.\n","summary":"When you get a new Ubuntu VPS or server there are a few things you want to make sure are taken care of right off the bat. This will optimize the security and usability of your server, providing a reliable foundation for subsequent alterations. If you need an Ubuntu VPS you can get one for .\nStep One Logging into your new VPS is the first step. You\u0026rsquo;ll need to know the public IP address of the server to begin.","tags":["linux","tutorial"],"title":"Getting Started with an Ubuntu VPS Running 14.04","url":"/blog/getting-started-with-an-ubuntu-vps-running-14-04/"},{"categories":null,"contents":"    Hugo static blogs are awesome    Migrating to Hugo It is 2019, enter the brave new world of serverless computing and static blogging. After years of using WordPress, and a few years without any updates this blog is in need of a refresh. When I originally created this blog I had servers (a whole web hosting company in fact). Now I just want to be able to create content and have a lightning fast website that is easy to maintain.\nEarlier today I was doing some research, came across Hugo and decided to rebuild this blog using it and host it using github pages, at least initially. This site will likely be a work in progress for some time, however at least the text of the old content is migrated now. As I do this migration I will put out a series of articles on the how and the why of the migration.\nWhy Hugo and the Coder theme? Hugo is lightning fast at rendering pages and simple to use, plus the workflow with github pages is straightforward. The coder theme was chosed for now because I like the aesthetics.\nMigrating from Wordpress I wanted a pretty fresh start, so I just used WordPress to export XML and then found a script here. Then I modified it for python 3 as follows:\n#!/usr/bin/env python3 \u0026#34;\u0026#34;\u0026#34; This script is used to convert a WordPress XML dump to Hugo-formatted posts. NOTE: The WP post data is kept as-is (probably HTML). It is not converted to Markdown. This is to reduce the amount of \u0026#34;fixing\u0026#34; one has to do after the data is converted (e.g. line endings, links, etc). This is generally not an issue since Markdown allows HTML. The post Metadata is converted to TOML. The posts are written as: \u0026lt;year\u0026gt;/\u0026lt;title\u0026gt;.md where \u0026lt;year\u0026gt; is the year the post was written, and \u0026lt;title\u0026gt; is the WP title with all non-word characters replaced with \u0026#34;-\u0026#34;, and converted to lower case. \u0026#34;\u0026#34;\u0026#34; # Imports ###################################################################### import os import re import maya import time import calendar import xml.etree.ElementTree as ET from distutils.version import LooseVersion # Metadata ##################################################################### __author__ = \u0026#34;Timothy McFadden\u0026#34; __creationDate__ = \u0026#34;07/24/2015\u0026#34; __license__ = \u0026#34;MIT\u0026#34; __version__ = \u0026#34;1.0.0dev\u0026#34; # Globals ###################################################################### DEBUG = False KNOWN_WP_VERSION = LooseVersion(\u0026#34;4.2\u0026#34;) def hugo_format(data): result = [\u0026#34;+++\u0026#34;] for heading in [\u0026#34;title\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;type\u0026#34;]: result.append(\u0026#39;%s= \u0026#34;%s\u0026#34;\u0026#39; % (heading, data[heading])) result.append(\u0026#34;tags = %s\u0026#34; % str(data[\u0026#34;tags\u0026#34;])) result.append(\u0026#34;+++\u0026#34;) result.append(\u0026#34;\u0026#34;) result.append(data[\u0026#34;body\u0026#34;]) return \u0026#34;\\n\u0026#34;.join(result) def wp_version_check(channel): match = re.search(\u0026#34;\\?v=([\\d\\.]+)\u0026#34;, channel.find(\u0026#34;generator\u0026#34;).text) if not match: print(\u0026#34;WARNING: Could not find WP version in your XML.\u0026#34;) print(\u0026#34;...This script may not work\u0026#34;) raw_input(\u0026#34;...press Enter to continue: \u0026#34;) else: wp_version = LooseVersion(match.group(1)) if wp_version \u0026lt; KNOWN_WP_VERSION: print(\u0026#34;WARNING: WP version in your XML (%s) is less than known good version (%s)!\u0026#34; % (wp_version, KNOWN_WP_VERSION)) print(\u0026#34;...This script may not work\u0026#34;) raw_input(\u0026#34;...press Enter to continue: \u0026#34;) def convert_wp_xml(xml_path): tree = ET.parse(xml_path) # FYI: xml.etree doesn\u0026#39;t support reading the namespaces, and I don\u0026#39;t feel # like requiring lxml. nsmap = { \u0026#34;excerpt\u0026#34;: \u0026#34;http://wordpress.org/export/1.2/excerpt/\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;http://purl.org/rss/1.0/modules/content/\u0026#34;, \u0026#34;wfw\u0026#34;: \u0026#34;http://wellformedweb.org/CommentAPI/\u0026#34;, \u0026#34;dc\u0026#34;: \u0026#34;http://purl.org/dc/elements/1.1/\u0026#34;, \u0026#34;wp\u0026#34;: \u0026#34;http://wordpress.org/export/1.2/\u0026#34;, } channel = tree.find(\u0026#34;channel\u0026#34;) wp_version_check(channel) for item in channel.findall(\u0026#34;item\u0026#34;): data = { \u0026#34;tags\u0026#34;: [], \u0026#34;title\u0026#34;: (item.find(\u0026#34;title\u0026#34;).text).strip(\u0026#39;\\n\u0026#39;), \u0026#34;date\u0026#34;: None, \u0026#34;body\u0026#34;: None, \u0026#34;fpath\u0026#34;: None, \u0026#34;type\u0026#34;: \u0026#34;post\u0026#34; } scraped_time = item.find(\u0026#34;pubDate\u0026#34;).text datetime = maya.parse(scraped_time).datetime(to_timezone=\u0026#39;US/Eastern\u0026#39;, naive=True) data[\u0026#34;date\u0026#34;] = datetime data[\u0026#34;tags\u0026#34;] = [x.attrib[\u0026#34;nicename\u0026#34;] for x in item.findall(\u0026#34;category\u0026#34;)] data[\u0026#34;body\u0026#34;] = item.find(\u0026#34;content:encoded\u0026#34;, nsmap).text fname = re.sub(\u0026#34;\\W+\u0026#34;, \u0026#34;-\u0026#34;, data[\u0026#34;title\u0026#34;]) fname = re.sub(\u0026#34;(-+)$\u0026#34;, \u0026#34;\u0026#34;, fname) fname = fname[1:] data[\u0026#34;fname\u0026#34;] = \u0026#34;{}.md\u0026#34;.format(fname.lower()) data[\u0026#34;fdir\u0026#34;] = os.path.abspath(os.path.join(\u0026#34;.\u0026#34;, str(datetime.year))) data[\u0026#34;fpath\u0026#34;] = os.path.join(data[\u0026#34;fdir\u0026#34;], data[\u0026#34;fname\u0026#34;]) hugo_text = hugo_format(data) if not os.path.isdir(data[\u0026#34;fdir\u0026#34;]): os.makedirs(data[\u0026#34;fdir\u0026#34;]) with open(data[\u0026#34;fpath\u0026#34;], \u0026#34;wb\u0026#34;) as fh: fh.write(hugo_text.encode(\u0026#39;UTF-8\u0026#39;)) print(\u0026#34;Created: {}/{}\u0026#34;.format(datetime.year, data[\u0026#34;fname\u0026#34;])) if __name__ == \u0026#39;__main__\u0026#39;: import sys if len(sys.argv) == 1: print(\u0026#34;Usage: python wp_to_hugo.py \u0026lt;wordpress XML file\u0026gt;\u0026#34;) sys.exit(1) convert_wp_xml(sys.argv[1]) I still need to manually sift through and fix images and curate the older posts have now updated all of the older posts.\nGithub Pages I chose github pages because I already have a github account and the workflow seemed good. Plus there is an easy way I plan to automate publishing master using travis-ci in the near future have automated publishing. I followed the Hugo docs, using the gh-pages branch based workflow. The repo for this blog now lives here.\nTravis CI I adapted this guide on Travis CI and Hugo to automate the build and publish of my blog to github pages. It is pretty straightforward and I highly recommend it.\nHere is the .travis.yml that I ended up with after optimizing a bit for faster builds of Hugo:\n# Credit to:#https://axdlog.com/2018/using-hugo-and-travis-ci-to-deploy-blog-to-github-pages-automatically/# https://docs.travis-ci.com/user/deployment/pages/# https://docs.travis-ci.com/user/reference/xenial/# https://docs.travis-ci.com/user/languages/go/# https://docs.travis-ci.com/user/customizing-the-build/cache:directories:- $HOME/.cache/go-build- $HOME/gopath/pkg/moddist:xeniallanguage:gogo:- 1.12.x# Only clone the most recent commit.git:depth:1# before_install# install - install any dependencies requiredinstall:- go get github.com/gohugoio/hugobefore_script:- rm -rf public 2\u0026gt; /dev/null# script - run the build scriptscript:- hugodeploy:provider:pagesskip-cleanup:truegithub-token:$GITHUB_TOKEN# Set in travis-ci.org dashboard, marked secureemail:$GITHUB_EMAILname:$GITHUB_USERNAMEverbose:truelocal-dir:publicfqdn:bryanapperson.comon:branch:master# branch contains Hugo generator codeConclusion Overall the initial learning curve, setup, customization and migration took about 8 hours. I would say that is pretty good. There will definitely be more posts to follow on my journey with blogging using Hugo.\n","summary":"Hugo static blogs are awesome    Migrating to Hugo It is 2019, enter the brave new world of serverless computing and static blogging. After years of using WordPress, and a few years without any updates this blog is in need of a refresh. When I originally created this blog I had servers (a whole web hosting company in fact). Now I just want to be able to create content and have a lightning fast website that is easy to maintain.","tags":["hugo","blog","go","wordpress"],"title":"From WordPress to Hugo","url":"/blog/from-wordpress-to-hugo/"},{"categories":null,"contents":"Alfred North Whitehead\u0026rsquo;s An Introduction to Mathematics takes us on a journey of broad strokes, one that traverses the history, notation, and application of mathematical fields. An Introduction to Mathematics is a practical book on theory, great for enticing the study of math at the university level. While covering introductory material, it introduces elegant mathematical insights that will intrigue the reader. It illustrates the principles of generality, form, variable, and abstraction as the elements of mathematics.\nAfter touching on the abstract nature of math, we take a look at variables. The work explores essential concepts of any and some. They are the ideas introduced in algebra which allow for concepts like infinity. Applications of variables, such as Boyle\u0026rsquo;s law, are brought to light. Illuminations of how variables work in diverse fields present their usefulness. From Archimedes to Newton, this concept proves itself intrinsic.\nFrom there, we approach vectors and their use in mechanics. All along, talking about the originators of these ideas. The pattern of history, application, and theory continues. The history of mechanics, under the title of dynamics tours from the greeks to Kopernicus and beyond. We learn the concept of vector operations and thereby adding velocities together using the parallelogram law. The explanations are simple yet powerful.\nAfter this digression into applications, the author conveys us back to the realm of pure mathematics. The evolution, and importance, of mathematical notation as a function of abstraction becomes apparent. The discussion of notion naturally leads back to generalizations about real numbers. In contrast to real numbers, complex or imaginary numbers are next. To illustrate complex numbers, we step into the realm of cartesian geometry. An eminent explanation of co-ordinate geometry shows that it serves as a bridge between the quantification of algebra and the pure spacial reasoning of euclidean geometry. As an aside, it seems non-euclidean geometries of curved space were unknown in 1911. Thus, neither were the theories of general and special relativity. Hence some outdated ideas are found, such as molecules in ether.\nTraveling on the continuum, we come to conic sections. We explore the properties of various curvilinear shapes. The importance of these shapes is made apparent by touching, once again, upon orbital mechanics. Ironically the Greeks, who originally studied conics, had no application for them. The ancient Greek love of pure theory came to fruition in concrete forms many millennia later; this is a demonstration of essential wonder.\nWe explore the nature of functions in some depth. The ideas of discrete and continuous functions lead to other concepts. Interval, the approximation to a number within a standard, and the neighborhood of a number approach limits. The book does not visit upon the use of multivariate functions. It also does not touch upon the concept of functions in computing. Although the terminology has changed, the elegance of the elements proves itself in future applicability.\nPeriodicity in nature, the enormous importance of these periods in how we develop discrete measurements of time, and how they exist in mathematics is the next topic. Mathematical methods used to analyze physical phenomena bring us to the discussion of trigonometry. As discussed in Plato\u0026rsquo;s Timaeus, the whole universe is composed of triangles. We thus take a look at the relationship of these unique rectilinear figures and circles, a special curvilinear conic section, through the lens of the unit circle. The natural occurrence and application of the sine and cosine functions are apparent here.\nEarlier on, the treatment of discrete and continuous function approached the idea of limits. Now series are introduced, and limits formally defined in the context of series. Limits and series lead to a discussion on the history and theory of differential calculus. This discussion is complete with a demonstration of how to take the first derivative of a function. Saving the best for last, Euclidean geometry and its nature as the science of dimensional order is next. Lastly, the world of measurement visits the continuum in a discussion of quantity.\nAt shy of 200 pages, this book covers an immense breadth of ideas. For the most part, it elucidates them readily; I had multiple \u0026ldquo;Aha\u0026rdquo; moments during reading. All considered this was $4, I picked up a used copy on AbeBooks.com (though you can also find it on new on Amazon), and about 20 hours well spent. Rereading this book, after a more in-depth study of the field, would lead to making new connections with what is said. In that line of thought; the bibliography is unique, in that it lays out a plan for self-study through other works. This book may inspire a hunger for mathematical knowledge, read with caution.\n","summary":"Alfred North Whitehead\u0026rsquo;s An Introduction to Mathematics takes us on a journey of broad strokes, one that traverses the history, notation, and application of mathematical fields. An Introduction to Mathematics is a practical book on theory, great for enticing the study of math at the university level. While covering introductory material, it introduces elegant mathematical insights that will intrigue the reader. It illustrates the principles of generality, form, variable, and abstraction as the elements of mathematics.","tags":["mathematics","quantity","space","time","mechanics"],"title":"An Introduction to Mathematics","url":"/musings/an-introduction-to-mathematics/"},{"categories":null,"contents":"Website: https://www.bryanapperson.com\nGithub: bryanapperson\nLocation: New York, NY USA\nWhy I Want to Join Your Team  Being an asset to my team and providing value are my primary motivators. Automation, continuous delivery and cloud native design are at the forefront of my skill set. My interests are in distributed systems reliability, storage and compute technologies. I am a self educated engineer, a self-starter and a go-getter. I enjoy being part of a vibrant team of ruckus makers where I can learn from, as well as mentor others, while building reliable infrastructure.\n A Tour of My Career to Date  Senior Site Reliability Engineer Bloomberg LP, New York, NY December 2017 – Present\n Building integration around open source Ansible automation for Ceph deployment, configuration and management. Implementing agile software development methodologies for the team and acting as scrum master. Architecture and implementation of a highly available REST based dynamic inventory provider to provide hardware information and injectable variables for Ansible automation. Spearheading the migration to our internal fully managed linux solution to reduce operational load on the storage team. A key contributor in operating one of the largest all flash ceph S3 clusters in the world.  Storage Integration Engineer Concurrent Technology Inc, Atlanta, GA Mar 2015-Dec 2017\n Building enterprise management APIs and UI around ceph storage. Writing ansible automation to deploy ceph clusters. Second engineer on POC that resulted in sucessful sale of business unit. On site and offsite level 3 support and project engineering.  Founder and Public Cloud Architect Bitronic Technologies Southold, NY Oct 2007-Oct 2015\n Founding a web hosting company and building it from the ground up. Architecting public cloud, shared hosting and bare metal server product offerings. Providing support to 2000+ customers. Sucessful exit and sale of the company in October of 2015. Working with frontend development, full stack development, network architecture, cloud architecture, ceph, kvm, QEMU, libvirt, xen, cpanel, openstack, marketing, sales, billing and more.  Scripting and Programming Languages   Python, Bash, C++, Javascript, HTML, CSS   Tools/Platforms   Linux, Ansible, Ceph, Apache, Nginx, MySQL, Docker, Vagrant, MacOS   Education  Seth Godin\u0026rsquo;s AltMBA July 2019\n Shipping 14 projects in 4 weeks with 4 teams in the rapid fire leadership, business and marketing workshop.   ","summary":"Website: https://www.bryanapperson.com\nGithub: bryanapperson\nLocation: New York, NY USA\nWhy I Want to Join Your Team  Being an asset to my team and providing value are my primary motivators. Automation, continuous delivery and cloud native design are at the forefront of my skill set. My interests are in distributed systems reliability, storage and compute technologies. I am a self educated engineer, a self-starter and a go-getter. I enjoy being part of a vibrant team of ruckus makers where I can learn from, as well as mentor others, while building reliable infrastructure.","tags":["resume"],"title":"Bryan Apperson","url":"/resume/"},{"categories":null,"contents":"This blog post is about making an astable multivibrator from discrete transistors. This weekend I embarked on making a home-brew computer from discrete transistors. To test circuits like a JK flip-flop or SRAM, a clock is needed. In the spirit of keeping with an all discrete transistor computer, I used an astable multivibrator composed of two RTL NAND gates. I wanted to start with a very low frequency, so I targeted 10 seconds high, 10 low for the clock. Changing the size of the capacitor and resistor outside the NAND gates will adjust the frequency. The formula for the frequency of the clock works out to be something like this: $:f=\\frac{1}{t}=\\frac{1}{2RC}=\\frac{1}{2 \\times 4700\\Omega \\times 2200uF}=20.68s$.\nDesigning the Astable Multivibrator I am new to low-level electronics like this, but I had used Arduino in the past and the designers of that micro-controller advocate Fritzing. I used Fritzing to design the schematic, breadboard and PCB. The first step was to design the schematic. I used and adaptation of a circuit I found here.\nThe next step was to breadboard the circuit. The cover image of this blog post shows the real life version, however I was able to breadboard it in Fritzing to validate the circuit.\nAfter that I went ahead and breadboarded/tested the circuit. Everything worked as expected after making blue smoke out of an LED that was shorted.\nThe final step for having a drop in circuit for a test clock was to design a PCB. I made considerations for being able to change R8 and C1 to change the frequency to something like .5hz for use in testing other components as I go down the homebrew computer road. That was convenient because I was able to fabricate the PCB from the program directly. I ordered a fabrication of the PCB from Fritzing Fab.\nIn about two weeks the PCB will be here from Germany and the parts will go on the board. The Fritzing file for this circuit can be downloaded here. Hopefully this article is helpful to anybody looking to make a clock circuit from discrete transistors using resistor-transistor logic NAND gates. Let me know your thoughts in the comments.\n","summary":"This blog post is about making an astable multivibrator from discrete transistors. This weekend I embarked on making a home-brew computer from discrete transistors. To test circuits like a JK flip-flop or SRAM, a clock is needed. In the spirit of keeping with an all discrete transistor computer, I used an astable multivibrator composed of two RTL NAND gates. I wanted to start with a very low frequency, so I targeted 10 seconds high, 10 low for the clock.","tags":["astable","multivibrator","electronics","nand","transistor"],"title":"Astable Multivibrator from Discrete Transistors","url":"/blog/astable-multivibrator-from-discrete-transistors/"},{"categories":null,"contents":"Sometimes it is necessary to copy Ceph pool objects from one Ceph pool to another - such as when changing CRUSH/erasure rule sets on an expanding cluster. There is a built-in command in RADOS for doing this. However the command in question, rados cppool , has some limitations. It only seems to work with replicated target pools. Thus it cannot copy Ceph pool objects from a erasure pool to a replicated pool, or between erasure pools. So to offer a utility for copying the contents of an erasure coded pool to another erasure pool, this evening I wrote up a function in my python-rados-utils repository. To use the python-rados-utils package, you first have to build and install it. At this point the repository only works with RHEL/CentOS/Fedora due to the RPM based build system. You can however look through the code for usage on other platforms. It\u0026rsquo;s pretty easy to get python-rados-utils up and running. Building from python-rados-utils from source:\ngit clone git@github.com:bryanapperson/python-rados-utils.git cd python-rados-utils rpmbuild -ba python-rados-utils.spec Installing python-rados-utils: The rpm from the build we just did will be output in ~/rpmbuild/RPMS/noarch/. Install the rpm using: rpm -Uvh \u0026lt;path-to-rpm\u0026gt; Once the python-rados-utils package is installed, using it to copy all objects from one Ceph pool to another is very straight-forward. In your favorite text editor, open up a file called copy_objects.py . In this file place:\n# Optionally you can pass in the keyring and ceph.conf # locations as strings. thiscluster = common_utils.Cluster() # Replace these empty stings with your source and target pool names source = \u0026#39;\u0026#39; target = \u0026#39;\u0026#39; thiscluster.copy_pool(source, target) NOTE: Updated versions of the above snippet can be found here. This script is single threaded at the moment and synchronous. I may add asynchronous and multi-threading functionalities to speed up Ceph pool copy in the near future. This code comes with no warranty of any kind and the code is licensed under GPLv2. Test in your own environment, but for me this worked well to copy all objects in one pool to another. Please leave your thoughts in the comments below and commit back any cool stuff to the python-rados-utils repository.\n","summary":"Sometimes it is necessary to copy Ceph pool objects from one Ceph pool to another - such as when changing CRUSH/erasure rule sets on an expanding cluster. There is a built-in command in RADOS for doing this. However the command in question, rados cppool , has some limitations. It only seems to work with replicated target pools. Thus it cannot copy Ceph pool objects from a erasure pool to a replicated pool, or between erasure pools.","tags":["ceph","development","linux-tutorials","python","rados"],"title":"Copy Ceph Pool Objects to Another Pool","url":"/blog/copy-ceph-pool-objects-to-another-pool/"}]